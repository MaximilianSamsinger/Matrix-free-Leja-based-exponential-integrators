% !TeX spellcheck = en_GB
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman,english]{babel}
\usepackage{blindtext}
\usepackage{color}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{subcaption}
\usepackage{upgreek}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{mathdots}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}

%\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\}
%\setlength{\parindent}{0em}

\newcount\colveccount
\newcommand*\colvec[1]{
	\global\colveccount#1
	\begin{bmatrix}
		\colvecnext
	}
	\def\colvecnext#1{
		#1
		\global\advance\colveccount-1
		\ifnum\colveccount>0
		\\
		\expandafter\colvecnext
		\else
	\end{bmatrix}
	\fi
}

\begin{document}
	
	
	\theoremstyle{definition}
	\newtheorem{defn}{Definition}[section]
	\newtheorem{bsp}{Beispiel}[section]
	\newtheorem{satz}{Satz}[section]
	\newtheorem{prop}{Proposition}
	\newtheorem{lem}[satz]{Lemma}
	\newtheorem*{bem}{Bemerkung}
	\newtheorem*{rem}{Remark}
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
	
	%\title{The Leja method in Python}
	%\subtitle{supervised by Peter Kandolf, Alexander Ostermann}
	%\maketitle
	%\author{Maximilian Samsinger}
	\begin{titlepage}
		\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
		\vspace{3cm}
		\begin{center}
			{\Large Master Thesis}
			\vspace{50pt}\\
			\textbf{\Huge Matrix-free Leja based exponential integrators in Python}
			\vspace{40pt}\\
			\textbf{\Large Maximilian Samsinger}\vspace{20pt}\\
			{\large\today}
			\vspace{120pt}\\
			{\Large Supervised by Lukas Einkemmer and\\
				Alexander Ostermann\vspace{10pt}}
		\end{center}
	\end{titlepage}
	\textsf{
		{\hspace{-16pt}\large\textbf{Leopold-Franzens-Universität Innsbruck}}
		\begin{figure}[!htp]
			\begin{flushright}
				\includegraphics[scale=0.1]{./uni_logo}
			\end{flushright}
		\end{figure}\\\\
		{\Large\textbf{Eidesstattliche Erklärung}}\\\\
		Ich erkläre hiermit an Eides statt durch meine eigenhändige Unterschrift, dass ich die\\ vorliegende Arbeit selbständig verfasst und keine anderen als die angegebenen Quellen und\\ Hilfsmittel verwendet habe. Alle Stellen, die wörtlich oder inhaltlich den angegebenen Quellen \\entnommen wurden, sind als solche kenntlich gemacht.\\\\
		Ich erkläre mich mit der Archivierung der vorliegenden Masterarbeit einverstanden.
		\vspace{40pt}\\
		\begin{center}
			\ensuremath{\overline{\mbox{Datum}\hspace{8em}}
				\hspace{10em}
				\overline{\mbox{Unterschrift}\hspace{10em}}
			}
			\thispagestyle{empty}
	\end{center}}
	\pagebreak
	
	
	
	\begin{center}\textbf{\Huge Matrix-free Leja based exponential integrators in Python}\end{center}
	\begin{center}\textbf{Abstract}\end{center}
	\begin{abstract}
		In this master thesis we develop an algorithm to approximate the action of a matrix exponential function for matrix-free linear operators. This is achieved by using a modified version of the real Leja method. We choose optimal interpolation parameters based on a spectral radius estimate computed by the power method. With this procedure we construct exponential Rosenbrock-type integrators to solve stiff advection-diffusion-reaction equations. We compare the performance of these integrators with other matrix-free differential equation solvers. 
 		As part of this thesis we publish the code for the matrix-free Leja method for the action of the matrix exponential function on GitHub \footnote{INSERT LINK HERE}.
	\end{abstract}
	
	\setcounter{page}{1}
	
	\section{Introduction}
	%%%%%%%%
	%Examples needed
	%%%%%%%%%%
	Consider the action of the matrix exponential function 
	\[e^Av ~~\text{where}~~ A\in\mathbb{C}^{N\times N} ~~\text{and}~~ v\in\mathbb{C}^N.\] 
	Due to computational constraints it can be difficult or impossible to compute $e^A$ in a first step and then the action $e^Av$ in a separate step. This is especially true in applications where $N>10000$ is common. Furthermore the matrix exponential of a sparse matrix is in general no longer sparse. Therefore it is more feasible to compute the action of the matrix exponential in a single step. This can be done by approximating the matrix exponential with a matrix polynomial $p_m$ of degree $m$ in $A$
	\[e^Av \approx p_m(A)v.\]
	This approach has many benefits. The cost of the computation of $p_m(A)v$ mainly depends on the calculation of $m$ matrix-vector multiplications with $A$, which is inexpensive for sparse matrices. Furthermore the explicit knowledge of $A$ itself is no longer required. The matrix $A$ can be replaced by a linear operator, which can be more convenient to work with and save memory. 
	
	We introduce an polynomial interpolation procedure, the Leja method, in Section \ref{sec:LejaMethod}. The Leja method has many computational advantages. All interpolation nodes can be precomputed and stored for later use. The interpolation itself is done iteratively and can be interrupted if the interpolation error is small enough. For entire functions the Leja interpolation has beneficial convergence properties which translate well to the corresponding matrix functions. After this brief introduction to the Leja method we discuss its application to the action of the matrix exponential function. We consider an approach which bounds the backward error of the interpolation. This will lead to an algorithm which provides cost-minimizing parameters for the Leja interpolation based on the operator norm $\norm{A}$ of $A$. \\
	In Section \ref{sec:matrixfreeimplementation} we adapt the algorithm in order to obtain a matrix-free version. The implementation of the Newton interpolation is straight-forward for linear operators. However, without the matrix representation it can be computationally infeasible to calculate the operator norm. Instead we replace all instances of $\norm{A}$ with the spectral radius, which can be cheaply estimated using the power method. This modification introduces new challenges. On the one hand it is unclear how many power iterations are necessary for a sufficient estimate of the spectral radius. On the other hand it is no longer possible to specify the norm for the backward error bound. \\
	In Section \ref{sec:LinearADe} we study the matrix-free Leja method for the discretized linear, one-dimensional advection-diffusion equation. For this specific problem we experimentally and numerically investigate the behavior of the power method and identify a suitable choice for the number of power iterations. Furthermore, this choice is reasonable in the nonlinear case as well. In order to verify that claim we introduce exponential Rosenbrock-type integrators in Section \ref{sec:expint}. Finally, we compare the performance of three matrix-free Leja based exponential integrators of different order against other matrix-free differential equation solvers in Section \ref{sec:NE}.
	

	\section{The Leja method} \label{sec:LejaMethod}
	This section serves as an introduction to the Leja method for approximating the action of the exponential function. We briefly cover the key concepts and definitions in Section \ref{sec:LejaInterpolation} and \ref{sec:ApproxMatrixExponential}. For more details and all proofs we refer to \cite{advdif} and \cite{lejarev}. 
	
	\subsection{Leja interpolation} \label{sec:LejaInterpolation}
	Let $K\in\mathbb{C}$ be a compact set in the complex plane and $\xi_0\in K$ be arbitrary. The sequence $(\xi_k)_{k=0}^{\infty}$ recursively defined as
	\[\xi_k = \underset{{\xi\in K}}{\operatorname{arg\,max}}\displaystyle\prod_{j=0}^{k-1}\abs{\xi-\xi_j}\]
	is called a Leja sequence. Due to the maximum principle all elements in the sequence realize their maximum on the border $\partial K$. Typically $\xi_0$ is also chosen on $\partial K$.\\
	For analytical functions $f\!:K\to\mathbb{C}$ the Newton interpolation polynomial $p_m$ with nodes $(\xi_k)_{k=0}^{m}$ has the following beneficial properties. 
	\paragraph{Convergence properties:}
	The sequence $(p_m)_{m=0}^\infty$ converges maximally to $f$. That is, let $(p_m^{*})_{m=0}^\infty$ be the best uniform approximation polynomials for $f$ in $K$. Then
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{K}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{K}^{1/m},\]
	where ${\norm{\cdot}}_K$ is the maximum norm on $K$. Furthermore if $f$ is an entire function, then $(p_m)_{m=0}^\infty$ converges superlinearly to $f$
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{\mathbb{C}}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{\mathbb{C}}^{1/m} = 0. \]
	For entire functions $f$ the corresponding matrix polynomials achieves similar superlinear convergence
	\[\limsup\limits_{m \rightarrow \infty}\norm{f(A)v-p_m(A)v}_{2}^{1/m} = 0, \]
	for $A\in\mathbb{C}^{n\times n}, v\in\mathbb{C}^n$.
	
	\paragraph{Early termination:}
	The Newton interpolation polynomial $p_m$ can be constructed iteratively since the corresponding Leja interpolation points $(\xi_k)_{k=0}^{m}$ are defined recursively. Therefore if the approximation $p_n \approx f$ is accurate enough after $n<m$ steps the interpolation can be stopped early to reduce the cost of the interpolation. Note that this is not possible with Chebyshev nodes.
	
	\paragraph{Leja sequence can be stored:}
	For a given $K$ the Leja interpolation nodes only need to be computed once and for all. These values can be stored a priori and loaded once they are needed for the interpolation. If $f$ is fixed the same is also true for the corresponding divided differences. \\
	
	In summary the Leja points offer convergence properties similar to Chebyshev nodes for interpolation, while having computational advantages. All results hold true for the corresponding matrix interpolation polynomials.
	
	\subsection{Approximating the matrix exponential function:} \label{sec:ApproxMatrixExponential}
	Inspired by the previous subsection we try to find a low-cost approximation of the action of the matrix exponential $e^Av$ using Leja interpolation polynomials. From now on, we will fix 
	\[K=[-c,c], ~~ f = e^\cdot  ~~\text{and}~~ \xi_0 = c \]
	for $c>0$. With $L_{m,c}$ we denote the Leja interpolation polynomial on the interval $[-c,c]$ with Leja points $(\xi_j)_{j=0}^{m}$. We use the well-known property of the exponential function
	\[e^Av = (e^{s^{-1}A})^sv, ~~\text{with}~~ s\in\mathbb{N}.\]
	Now we can approximate the action of the matrix exponential in $s$ substeps
	\[v_0\defneq v, ~~ v_{j+1}\defneq L_{m,c}(s^{-1}A)v_j, ~~\text{and}~~ v_s \approx e^Av.\]
	So far we placed no restrictions on $m$, $s$ and $c$. We choose optimal parameters based on the backward-error analysis done in \cite{lejarev}.
	
	\paragraph{Bounding the backward error} For a given matrix $A$ we interpret the Leja interpolation polynomial as the exact solution of a perturbed matrix exponential function
	\[
	L_{m,c}(s^{-1}A)^sv \eqdefn e^{A + \Delta A}v\\
	\]
	Our goal is to bound the backward error 
	\[
	\frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}, 
	\]
	for a given tolerance $\operatorname{tol}$. Furthermore we want to minimize the cost of the interpolation. A priori it is unclear for which values $m$, $s$ and $c$ the inequality is satisfied. The authors of \cite{lejarev} conducted a backward error analysis and chose an approach which puts an upper bound on $c$ depending only on $s$ and $m$. For various tolerances $\operatorname{tol}$ they precomputed values $\theta_m$, see \ref{table:thetam}, which satisfy
	\[
	\text{If}\ \norm{s^{-1}A}\le \theta_m\ \text{and}\ 0 \le c \le \theta_m\ \text{then}\ \frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}.
	\]
	\begin{table}[tbp]
		\begin{tabular}{r|rrrrrrr}
			$m$ &        5 &       10 &       15 &       20 &       25 &       30 &       35 \\\hline
			half & 6.43e-01 & 2.12e+00 & 3.55e+00 & 5.00e+00 & 6.37e+00 & 7.51e+00 & 8.91e+00 \\
			single & 9.62e-02 & 8.33e-01 & 1.96e+00 & 3.26e+00 & 4.69e+00 & 5.96e+00 & 7.44e+00 \\
			double & 1.74e-03 & 1.14e-01 & 5.31e-01 & 1.23e+00 & 2.16e+00 & 3.18e+00 & 4.34e+00 \\
			\\
			$m$ &       40 &       45 &       50 &       55 &       60 &       65 &       70 \\\hline
			half & 1.00e+01 & 1.10e+01 & 1.23e+01 & 1.35e+01 & 1.48e+01 & 1.59e+01 & 1.71e+01 \\
			single & 8.71e+00 & 1.00e+01 & 1.15e+01 & 1.27e+01 & 1.40e+01 & 1.52e+01 & 1.64e+01 \\
			double & 5.48e+00 & 6.67e+00 & 7.99e+00 & 9.24e+00 & 1.06e+01 & 1.18e+01 & 1.32e+01 \\
			\\
			$m$ &       75 &       80 &       85 &       90 &       95 &      100 \\ \hline
			half & 1.84e+01 & 1.94e+01 & 2.07e+01 & 2.20e+01 & 2.30e+01 & 2.42e+01 \\
			single & 1.76e+01 & 1.87e+01 & 1.99e+01 & 2.12e+01 & 2.23e+01 & 2.35e+01 \\
			double & 1.46e+01 & 1.58e+01 & 1.71e+01 & 1.86e+01 & 1.99e+01 & 2.13e+01
		\end{tabular}
		\caption{Samples of the precomputed values $\theta_m$. The backward error of the Leja interpolation is bounded if $c\le\theta_m$, where $[-c,c]$ is the interpolation interval and $m$ the interpolation degree. Half, single and double correspond to the tolerances $2^{-10}$, $2^{-24}$ and $2^{-53}$ respectively \cite[Table 1]{lejarev}.}
		\label{table:thetam}
	\end{table}	
	For our purposes it is important to note that the optimal choice for $c$ is given by $c=\rho(s^{-1}A)$, where $\rho(A)$ is the spectral radius of $A$. However, computing $\rho(A)$ introduces additional costs for the algorithms proposed in \cite{lejarev}. Our matrix-free implementation relies on the computations of the spectral radius, but it does not need to compute the operator norm $\norm{A}$, see Section \ref{sec:matrixfreeimplementation}.
	
	\paragraph{Choosing cost-minimizing parameters}
	The cost of the Leja interpolation mainly depends on the the number of matrix-vector products
	\[
	C_{m} = sm. 
	\]
	In order to minimize the costs of the interpolation $C_m$ we select the smallest $m$ for any given $s$ such that
	\[
	\norm{s^{-1}A} \le \theta_m
	\]
	is satisfied. This leads to the optimal choice for $m$ and $s$ 
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil.
	\end{split} \label{eq:ms}
	\end{align}
	In our algorithm we set $m_{\operatorname{max}} = 100$ in order to avoid over- and underflow errors.
	
	\paragraph{Shifting the matrix}
	The cost of the interpolation can be decreased by employing a shift $\mu\in\mathbb{C}$. Let $I$ be the identity matrix. We replace the matrix $A$ with $A-\mu I$ for all computations. If the shifted matrix $A-\mu I$ satisfies $\norm{A -\mu I} < \norm A$ then the cost $C_{m_*}$ of the interpolation decreases.
	We compensate for the shift by multiplying with $e^\mu$ since
	\[
	e^{A} = e^{\mu}e^{A-\mu I}.
	\]
	A well-chosen shift centers the eigenvalues of $A-\mu I$ around $0$. Such a shift can be found by using Gerschgorin's circle theorem. This is, however, not possible in the matrix-free case.
	
	\section{Matrix-free implementation}\label{sec:matrixfreeimplementation}
Matrix-free methods are algorithms which use linear functions, but do not explicitly rely on their respective matrix-representations. These methods are preferable when saving and loading matrix coefficients becomes prohibitively expensive. As a motivational example we perform a cost analysis in terms of memory operations for finite difference schemes in section \ref{sec:FDS}.
We develop a matrix-free version of the Leja method for the action of the matrix exponential in section \label{sec:MatrixFreeLejaMethod}. Finally, we discuss the advantages and disadvantages of this new algorithm.

%In this section we demonstrate this for finite difference schemes and develop a matrix-free version of the Leja method for the action of the matrix exponential. This is  
%The cost of the Leja interpolation mainly depend on the number of matrix-vector multiplications. For large matrices it is memory-inefficient to load the coefficients first and compute a matrix-.
%We will demonstrate 

%In this section the a matrix-free version of the Leja method for the action of a matrix exponential. After a brief motivation we discuss the main advantages and disadvantages of such an approach. \\
%Performance on most modern systems is limited by memory bandwidth. A matrix-free implementation gives us the opportunity to significantly save memory and thus increase performance.

\subsection{Matrix-free methods for finite difference schemes} \label{sec:FDS}
Discretizing partial differential equations often leads to stencils, which are fixed, geometric update rules for vector elements. In the simplest case, which is the only one we consider, each point on a grid is updated by a linear combination of itself and its neighbours. This can be expressed as an linear function $A$ acting on a vector $u$. 

On modern systems the computation of these updates is often restricted by the available memory bandwidth. It is often beneficial to use algorithms, which do not rely on the matrix representation of $A$ to save memory. We demonstrate this for finite difference schemes of the following form.

\begin{align}
\partial_{t}u = Au = \sum_{j=-n}^{n} a_{k+j}u_{k+j} \label{eq:stencil}
\end{align}
for $n\in\mathbb{N}$, $u\in\mathbb{R}^N$ and periodic boundaries i.e. $a_{l+N} = a_{l}$ and $u^{(i)}_{l+N}=u^{(i)}_{l}$ for all $l\in\mathbb{Z}$, $i\in\{0,1\}$.
We assume the discretization was performed on a regular grid and all real numbers are stored in double-precision format.
The solution of \eqref{eq:stencil} can be approximated with the Leja method using the matrix-representation of $A$. The Newton interpolation itself can be computed without the explicit knowledge of the matrix coefficients. For fixed interpolation parameters we can compare the performance of the Leja method for the regular and the matrix-free case by counting the number of read and write operations necessary to compute $Av$, where $v$ is an arbitary vector.

%Since the cost of the Leja method mainly depend on the number of matrix-vector products, we can performing a cost analysis in terms of memory operations for $Av$, where $v$ is an arbitrary vector, to investigate the performance of the Leja method.\\ 

%We count the number of read and write operations necessary to compute $Au$. 
Reading $v$ and writing the result of $Av$ costs $2N$ memory operations in total. If $A$ is stored as a dense matrix then $N^2$ entries have to be loaded. These costs are reduced to $(2n+1)N$ for sparse matrices, since only $2n+1$ entries have to be loaded for each row in $A$. However, depending on the sparse format chosen, additional information has to be accessed to specify the index of each matrix entry. For example, the compressed sparse row (CSR) format requires storing $N$ and $(2n+1)N$ integers for the row and column indices respectively. Integers need four bytes of memory space compared to 16 bytes for floating point numbers in double precision. Therefore $(40n + 20)N$ bytes need to be accessed for the matrix in CSR format. In total $(40n + 52)N$ bytes are read or written for the computation of $Au$.
In the matrix-free case the $(2n+1)$ coefficients $a_{-n}, \dots, a_{n}$ only need to be loaded once. This reduces the number of memory operations for the calculation of $Au$ to $2N + n$. In total $32N + 16n$ bytes need to be either read or written. For a simple 5-point stencil, which is commonly used to calculate the numerical derivative in two dimensions, this reduces the number of memory operations by a factor of 4.

This example shows that most of the memory operations are incurred by loading the coefficients of the matrix $A$. Furthermore it motivates the development of a fully matrix-free Leja method for the matrix exponential function. 

Blabla implement as a linear operator, i.e.

loading only the relevant $(2n+1)$ coefficients $a_{-n}, \dots, a_{n}$. This reduces the number of memory operations for the calculation of $Au$ to $2N + n$. In total $32N + 16n$ bytes need to be either read or written. For a simple 5-point stencil, which is commonly used to calculate the numerical derivative in two dimensions, this reduces the number of memory operations necessary by a factor of 4.

	
	In this section we introduce a matrix-free version of the Leja method for the action of a matrix exponential. This algorithm 
	
	We demonstrated that a matrix-free representation of finite difference schemes can save memory. Finite difference methods are part of a more general class of  stencil codes, which are fixed, geometric update rules for vector elements using a pattern depending on itself its neighbours, 
	Blah blah, stencil codes. More generally stencil code 
	
	We consider stencils for partial differential equations. They are fixed, geometric update rules for vector elements using a pattern depending on itself its neighbours. On modern systems the memory bandwidth is th 
	We demonstrate this for finite difference methods of the form

\begin{align}
\partial_{t}u = Au = \sum_{j=-n}^{n} a_{k+j}u_{k+j} \label{eq:stencil}
\end{align}
for $n\in\mathbb{N}$, $u\in\mathbb{R}^N$ and periodic boundaries i.e. $a_{l+N} = a_{l}$ and $u^{(i)}_{l+N}=u^{(i)}_{l}$ for all $l\in\mathbb{Z}$, $i\in\{0,1\}$.
We assume the discretization was performed on a regular grid assume and all real numbers are stored in double-precision format.
The solution of \eqref{eq:stencil} can be calculated with the Leja method. For a fast computation it is crucial to minimize the cost of computing $Au$.    
We investigate the number of read and write operations. For 

As an motivational example consider stencil codes, which are fixed, geometric update rules for vector elements using a pattern depending on itself its neighbours. Stencils naturally arise in finite difference and finite element schemes. \\ 
Consider 

Even if the matrix is stored in a sparse format it is still necessary to save $N(2n+1)$ non-zero elements. In the case of two-dimensional five-point stencil method on a rectangular grid we have $n=2$ and $N=N_{x}N_{y}$, where $N_{x}$ and $N_{y}$ is the number of grid points on the $x$- and $y$-axis respectively. This leads to a storage demand of $20N_xN_y$ bytes when saving matrix entries as float data. This means that we surpass $1$ megabyte when we choose $N_x,N_y > 250$, which might cause cache misses. This is especially true for L1 and L2 caches. This problem is further magnified for stencils on three-dimensional domains. \\	

	
	%%%%%%%%%%%%%%%% CHANGE THIS CHANGE THIS CHANGE THIS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	In this section we introduce a matrix-free version of the Leja method for the action of a matrix exponential. After a brief motivation we discuss the main advantages and disadvantages of such an approach. \\
	Performance on most modern systems is limited by memory bandwidth. A matrix-free implementation gives us the opportunity to significantly save memory and thus increase performance. For example, a matrix-free implementation reduces the amount of memory transactions compared to a
	sparse matrix implementation based on the CSR format by at least a
	factor of 5.
	Consider finite-difference schemes
	\[
	v_k = \sum_{j=-n}^{n} a_{k+j}v_{k+j}
	\]
	for $v\in\mathbb{C}^N$, $n\in\mathbb{N}$ and periodic boundaries i.e. $a_{l+N} = a_{l}$ and $v_{l+N}=v_{l}$ for all $l\in\mathbb{Z}$. 
	This finite difference scheme can be written as an matrix-vector multiplication with an $(2n+1)$-diagonal matrix. Even if the matrix is stored in a sparse format it is still necessary to save $N(2n+1)$ non-zero elements. In the case of two-dimensional five-point stencil method on a rectangular grid we have $n=2$ and $N=N_{x}N_{y}$, where $N_{x}$ and $N_{y}$ is the number of grid points on the $x$- and $y$-axis respectively. This leads to a storage demand of $20N_xN_y$ bytes when saving matrix entries as float data. This means that we surpass $1$ megabyte when we choose $N_x,N_y > 250$, which might cause cache misses. This is especially true for L1 and L2 caches. This problem is further magnified for stencils on three-dimensional domains. \\	
	We avoid these difficulties with matrix-free methods, meaning that matrix representations are not explicitly given. Instead we consider linear operators of the form
	\begin{align*}
	\begin{split}
	A\!:\mathbb{C}^{N}&\to\mathbb{C}^{N}\\
	v&\mapsto A(v)
	\end{split}.
	\end{align*}
	Linear operators can be more convenient to work with in applications where the matrix representation is not easily available. They can also be more memory efficient. For the finite difference schemes we discussed at the beginning of this section we only need to store $(2n+1)$ values independent of $N$. This serves as a motivation to propose an alternative for the classical Leja method we discussed in Section \ref{sec:LejaMethod}.
	\subsection{Matrix-free Leja method} \label{sec:MatrixFreeLejaMethod}
	For the most part it is unproblematic to use linear operators for the Leja method instead of matrices, since the Newton interpolation only relies on computation of matrix-vector products. However, difficulties arise when the interpolation parameters need to be determined. Without the matrix representation it can be expensive to compute the operator norm $\norm A$ in \eqref{eq:ms}. We will circumvent this problem by replacing $\norm{A}$ with the spectral radius $\rho(A)$. 
	
	The backward error analysis in \cite{lejarev} holds true for every operator norm.
	We use a well-known result from the matrix analysis literature \cite[Lemma 5.6.10.]{matrixanalysis}. For every $A$ and for every $\varepsilon>0$ exists an induced operator norm $\norm{\cdot}_{A,\varepsilon}$ such that
	\[
	\rho(A) \le \norm{A}_{A,\varepsilon} \le \rho(A) + \varepsilon. 
	\]
	While the first inequality holds true for every operator norm, it is not possible to select an operator norm independent of either $A$ or $\varepsilon$ for the second one. We choose $\varepsilon$ small enough, such that
	\[
	\norm{s^{-1}A}_{A,\varepsilon} \le \underset{\rho(s^{-1}A) < \theta_m}{\min}\theta_m.
	\]
	For this choice of $\norm{\cdot}_{A,\varepsilon}$ the cost-minimizing parameters are given by
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil.
	\end{split}\label{eq:msmatrixfree}
	\end{align}
	The explicit knowledge of $\norm{\cdot}_{A,\varepsilon}$ is no longer required. Additionally we can choose $c=\rho(A)$ without introducing additional costs, since we have to compute $\rho(A)$ to determine $m_*$ and $s_*$. 
	For positive and negative semi-definite operators $A$ we select the shift $\mu = -\rho(A)/2$ and $\mu = \rho(A)/2$ respectively. This shift works particularly well if the absolutely smallest eigenvalue of $A$ is close to $0$.\\
	This approach has some drawbacks though. While we are able to bound the backward error
	\[
	\frac{\norm{\Delta A}_{A,\varepsilon}}{\norm{A}_{A,\varepsilon}} \le \operatorname{tol}  
	\]
	we can no longer specify in which norm this error has to be bound. Furthermore, it can be hard to find a good shift $\mu$ for non-semi-definite operators.
	\paragraph{Power method}
	The spectral radius $\rho(A)$ can be cheaply approximated using the power method. 
	%For each $j\in\{1\dots N\}$ let $(\lambda_j,v_j)$ be an eigenpair of $A$. 
	%Furthermore let all eigenvectors be normalized and all eigenvalues be sorted in descending order with respect to their modulus. 
	Given an initial vector $b_0\in\mathbb{C}^N$ the $n$-th iteration of the power method is given by
	\[
		b_{n+1} = \frac{Ab_n}{\norm{Ab_n}}.
	\]
	Let $\lambda_{1}$ be a maximal eigenvalue with respect to the modulus. Furthermore let $\lambda_2$ be a maximal eigenvalue satisfying $\abs{\lambda_{1}}\neq\abs{\lambda_2}$. 
	Both eigenvalues are not necessarily unique.
	For each iteration we get an approximation to the spectrum of $A$
	\[ 
	\norm{Ab_{n}} = \rho(A) + \mathcal{O}\left(\left\lvert\frac{\lambda_2}{\lambda_1}\right\rvert^{n+1}\right).
	\]  
	This can be used to compute \eqref{eq:msmatrixfree}. However, the power method underestimates the spectrum of $A$, which might cause the Leja method to not converge. Therefore we have to multiply the estimate with a safety factor. The convergence speed depends on the eigenvalues of $A$. In particular if $\abs{\lambda_{1}}\gg\abs{\lambda_{2}}$ we expect fast convergence for the power method. A more thorough analysis for the discretized linear advection-diffusion equation will be conducted in Section \ref{sec:LinearADe}.

	From now on we denote matrix-free Leja method for the matrix exponential function as \texttt{expleja}. Depending on the chosen tolerance, see Table \ref{table:thetam}, we will refer to the algorithm as half, single or double precision \texttt{expleja} respectively.
	
	\section{Linear advection-diffusion equation}\label{sec:LinearADe}
	In this section we consider a simple initial value problem which serves as a test-bed for future experiments. We also want to examine the power method, an algorithm to (under)estimate the largest eigenvalue of an operator with respect to the modulus.\\
	Consider the one-dimensional advection-diffusion equation
	\begin{align}
	\begin{split}
	\partial_tu &= a\partial_{xx}u + b\partial_xu ~~\text{with}~~ a,b\ge 0 ~~\text{and}\\
	u_0(x) &= e^{-80\cdot(x-0.45)^2}\hphantom{,} ~~\text{with}~~ x\in[0,1]
	\end{split}\label{eq:LinearAdvDif}
	\end{align}
	on the time interval $[0,0.1]$. For a fixed $N\in\mathbb N$ we approximate the diffusive part of the differential equation with second-order central differences on an equidistant grid with grid size $h = \frac{1}{N-1}$ and grid points $x_k = kh$, $k=0\dots,N-1$
	\[\partial_{xx}u(x_k) = \frac{u(x_{k+1}) - 2u(x_k) + u(x_{k-1})}{{h}^2} + \mathcal{O}({h}^2).\]
	In order to avoid numerical instabilities we discretize the advective part with forward differences, similar to the upwind scheme
	\[\partial_{x}u(x_k) = \frac{u(x_{k+1}) - u(x_k)}{h} + \mathcal{O}(h).\]
	The resulting system of ordinary differential equation is given by
	\begin{align*}
	\partial_tu &= Au.
	\end{align*}
	In order to measure the relative strength of advection compared to diffusion we employ the P\'eclet number $\operatorname{Pe} = \frac{b}{a}$.
	\begin{figure}[ht]
		\newcommand{\precision}{single}
		\newcommand{\Pe}{Pe=10.0}
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
		\caption{Approximation of $e^{0.1A}u_0$ using single precision \texttt{expleja} for a fixed interpolation degree $m=100$ and varying number of substeps $s$. The relative error is measured in the Eucledian norm. The reference solution was computed using the double precision \texttt{expleja} algorithm.}
		\label{fig:Experiment1}
	\end{figure} 
	\noindent The solution of the differential equation is given by $e^{0.1A}u_0$, which can be approximated using the Leja method, as shown in Figure \ref{fig:Experiment1}. For the matrix-free case we need to compute the spectral radius of $A$, which can be done using the power method.
	
	
	\subsection{Analysis of the power method}\label{poweritanalysis}
	\begin{figure}[t]
		\newcommand{\boundary}{periodic}
		\centering
		\includegraphics[width=1.\columnwidth]{{../Figures/Spectrum/\boundary}.pdf}
		\caption{The spectrum of $A$. We assume \boundary\ boundary conditions.}
		\label{fig:spectrum}
	\end{figure}
	%All eigenvalues of the discretized one-dimensional using the upwind scheme are -1/h.   
	%	The eigenvalues of discretized one-dimensional Laplace operator $A_{Dif}\in\mathbb{R}^{N\times N}$ on the interval $[0,1]$ with periodic boundary conditions are given by
	%\[
	%\lambda_j =
	%\begin{cases*}
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi (j-1)}{2(N+1)}\right),\quad\text{if j is odd}\\
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi j}{2(N+1)}\right),\quad\text{if j is even}
	%\end{cases*} 
	%,\quad j=1,\dots, N.
	%\]
	We investigate the rate of convergence of the power method to the largest eigenvalue (with respect to the modulus) $\lambda_{max}$ of $A$.
	For our analysis we assume periodic boundary conditions
	\[ A = \frac{a}{h^2}
	\begin{bmatrix}
	-2     & 1  & 0      & \cdots & 0  & 1      \\
	1      & -2 & 1      &        &    & 0      \\
	0      & 1  & \ddots & \ddots &    & \vdots \\
	\vdots &    & \ddots & \ddots & 1  & 0      \\
	0      &    &        & 1      & -2 & 1      \\
	1      & 0  & \cdots & 0      & 1  & -2
	\end{bmatrix}
	+\frac{b}{h}
	\begin{bmatrix}
	-1     & 1  & 0      & \cdots & \cdots  &   0    \\
	0      & -1 & 1      &        &    & \vdots      \\
	\vdots &    & \ddots & \ddots &    & \vdots	 \\
	\vdots &    & 		 & \ddots & 1  &  0      \\
	0      &    &        &        & -1 & 1      \\
	1      & 0  & \cdots & \cdots & 0  & -1
	\end{bmatrix}.
	\]
	Consider the discrete Fourier basis
	\[
	v_j = \frac{1}{\sqrt N}\colvec{4}{e^{i\frac{2\pi}{N}j 0}}{e^{i\frac{2\pi}{N}j 1}}{\vdots}{e^{i\frac{2\pi}{N}j(N-1)}}, \quad j\in{0\dots N-1}.
	\]
	Each $v_j$ is an eigenvector of $A$
	\[
	Av_j = \lambda_jv_j, 
	\]
	where the eigenvalues $\lambda_j$ are given by
	\begin{align*}
	\lambda_j &= \frac{a}{h^2}\left(e^{i\frac{2\pi}{N}j} - 2 + e^{-i\frac{2\pi}{N}j}\right)-\frac{b}{h}\left(e^{i\frac{2\pi}{N}j}-1\right) \\ 
	&=-\left(\frac{4a}{h^2}-\frac{2b}{h}\right) \sin^2\!\left(\frac{\pi j}{N}\right) + i\frac{b}{h}\sin\!\left(\frac{2\pi j}{N}\right),
	\end{align*}
	see Figure \ref{fig:spectrum}. We restrict our analysis to the case $a=0$ and $b=1$, i.e.
	\begin{align*}
	\lambda_j =-\frac{4}{h^2}\sin^2\!\left(\frac{\pi j}{N}\right).
	\end{align*}
	The modulus is maximized for $j=\lfloor N/2\rfloor$, i.e.  $\lambda_{max} = \lambda_{\lfloor N/2\rfloor}$.
	From now on we study the behaviour of $v = \frac{1}{N}\sum_{j=0}^{N-1} v_j$, the normalized sum of all eigenvectors. Let $n\in\mathbb{N}$ be the number of power iterations. We begin our analysis with the following auxiliary calculation
	\begin{align*}
	\norm{A^nv}_2
	&= \sqrt{\frac{1}{N}\sum_{j=0}^{N-1}{\abs{\lambda_j}^{2n}}}
	= \frac{2^{2n}}{h^{2n}}\sqrt{I_{N,n}} ,
	\end{align*}
	where
	\begin{align*}
	I_{N,n} &= \frac{1}{N}\sum_{j=0}^{N-1} \sin^{4n}\!\left(\frac{\pi j}{N}\right).
	\end{align*}
	%&\xrightarrow[]{N\to\infty} (4a)^{2n} \int_{0}^{1} \sin^{4n}(\pi x)dx = \\
	%&= (4a)^{2n} \frac{2}{\pi} \int_{0}^{\frac\pi 2} \sin^{4n}(x)dx \\
	%&= (4a)^{2n} \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5),
	%where $\operatorname{B}$ is the beta function. 
	The first equality holds since all eigenvectors are orthogonal. We interpret the sum of sine functions as an integral approximated by the composite trapezoidal rule
	\[
		I_n\defneq\int_{0}^{1}{\sin^{4n}\!\left(\pi x\right)} \approx I_{N,n},
	\]
	with nodes $\frac{j}{N}$ for $j=0$, \dots, $N$.
	Furthermore if we use the nodes $\frac{k}{N}$ for $k=0$, \dots, $2N$ we have
	\[
	2I_n = \int_{0}^{2}{\sin^{4n}\!\left(\pi x\right)} \approx
	\frac{1}{N}\sum_{k=0}^{2N-1}\sin^{4n}\!\left(\frac{\pi k}{N}\right)
	= \frac{2}{N}\sum_{j=0}^{N-1}\sin^{4n}\!\left(\frac{\pi j}{N}\right)
	= 2 I_{N,n}.
	\]
	due to symmetry. Since $\sin^{4n}$ is a $2\pi$-peroidic trigonometric polynomial of degree $4n$ the trapezoidal rule is exact if $2N>4n$. In general the error converges exponentially to $0$. For readers unfamiliar with these properties we refer to \cite[Corollary 3.3]{trapezoidal}. 
	By comparing both trapezoidal approximations we establish $I_n = I_{n,N}$ if $N > 2n$.\\
	We underestimate the largest eigenvalue $\lambda_{max}$ by a factor of
	\begin{align*}
	\frac{\norm{A^{n+1}v}_2}{\norm{A^nv}_2}\frac{1}{\abs{\lambda_{max}}}
	= \frac{\frac{2^{2n+2}}{h^{2n+2}}\sqrt{I_{N,n+1}}}{\frac{2^{2n}}{h^{2n}}\sqrt{I_{N,n}}}\frac{1}{\abs{\lambda_{max}}} = \sqrt{\frac{I_{N,n+1}}{I_{N,n}}}\sin^{-2}\!\left(\frac{\pi}{N}\left\lfloor\frac{N}{2}\right\rfloor\right).
	\end{align*}
	using the power method. From now on we assume $N > 2(n+1)$. By using the properties of the beta function $\operatorname{B}$ we get
	\[
	I_{N,n} = I_{n} = \int_{0}^{1} \sin^{4n}(\pi x)dx = \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5) = \frac{\Gamma(2n + 0.5)\Gamma(0.5)}{\pi \Gamma(2n + 1)}
	\]
	\noindent where $\Gamma$ is the gamma function. Finally we can simplify the ratio
	\begin{align*}
	\frac{I_{N,n+1}}{I_{N,n}} &=   
	\frac{\Gamma(2n + 1)\Gamma(2n + 2.5)}{\Gamma(2n + 3)\Gamma(2n + 0.5)} \\&= 
	\frac{(2n)!}{(2n+2)!}
	\frac{\Gamma(2n + 2.5)}{\Gamma(2n + 0.5)}\\&=
	\frac{(2n)!}{(2n+2)!}
	\frac{2^{4n}\sqrt\pi}{2^{4n+4}\sqrt\pi}
	\frac{(4n+4)!}{(4n)!}
	\frac{(2n)!}{(2n+2)!} \\&=
	\frac{(4n+4)(4n+3)(4n+2)(4n+1)}{16(2n+2)^2(2n+1)^2} \\&=
	\frac{(4n+3)(4n+1)}{(4n+4)(4n+2)}\\&=
	\left(1-\frac{1}{4n+4}\right)\left(1-\frac{1}{4n+2}\right)
	\end{align*}
	For the third equality we applied the duplication formula for the gamma function. All in all the power method underestimates the largest eigenvalue $\lambda_{max}$ by a factor of 
	\[
	\frac{\norm{A^{n+1}v}_2}{\norm{A^{n}v}_2}\frac{1}{\abs{\lambda_{max}}} =
	\sqrt{\left(1-\frac{1}{4n+4}\right)\left(1-\frac{1}{4n+2}\right)}\sin^{-2}\!\left(\frac{\pi}{N}\left\lfloor\frac{N}{2}\right\rfloor\right) \approx 1-\frac{1}{4n+3}
	\]
	assuming $N>2(n+1)$. %For some sample values of $\alpha_n$ consider Table \ref{table:alphan}. We observe that the relative increase of $\alpha_n$ is  
	%\begin{table}
	%	\centering
	%	\def\arraystretch{1.5}
	%	\begin{tabular}{c|ccccccccc}	
	%		n & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\ 
	%		\hline 
	%		$\alpha_n$ & 0.676 & 0.727 & 0.906 & 0.931 & 0.946 & 0.955 & 0.962 & 0.967 & 0.971\\ 
	%	\end{tabular} 
	%	\caption{Number of power iterations $n$ versus $\alpha_n$. In the limit $N\to\infty$ the power method underestimates the absolutely largest eigenvalue by the factor $\alpha_n$ for the sum of all normalized eigenvectors of $A$.}
	%	\label{table:alphan}
	%\end{table}
	\subsection{Experiments for the power method}
	As discussed in Section \ref{sec:matrixfreeimplementation} we need to estimate the largest eigenvalue $\lambda_{max}$ of $A$ for the matrix-free \texttt{expleja} algorihm. However, we can only guarantee convergence if we overestimate $\lambda_{max}$. Therefore we have to include a safety factor $\text{sf}$ by which we multiply the output of the power method. 
	%The question naturally arises, how many power iterations $n$ and which safety factor $\text{sf}\ge 1$ is necessary for convergence.
	%The power method analysis in section \ref{poweritanalysis} partially answers this question. For large $N$ and for the vector $v=\sum_{j=0}^{N-1}v_j$ the choice 
	%\[ \text{sf} = \frac{1}{\alpha_n}\]
	%is optimal. 
	
	%In the matrix-free case the linear operator $A$ is not explicitly given. In order to compute the operator norm ${\norm A}_2$ we use power iterations to estimate the absolutely largest eigenvalue of $A$. A priory it is not clear how many power iterations $it$ are necessary for a good approximation. 
	\begin{figure}[h]
		\newcommand{\Pe}{Pe=1.0}
		\newcommand{\precision}{single}
		\newcommand{\safe}{sf=1.1}
		
		\centering
		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment1LinOp/1, \precision, \Pe, \safe.pdf}
		\caption{Space dimension $N$ vs costs \texttt{mv} per timestep $s$ for matrix-free single precision \texttt{expleja}. The interpolation degree $m$ is fixed to 100. The Newton interpolation can still terminate early resulting in smaller numbers of $\texttt{mv}/s$. The number of power iterations are denoted by $n$. For this experiment we chose $\operatorname{Pe}=1.0$ and $\text{sf}=1.1$. Results are only shown if they achieve \precision\ precision.} \label{fig:Poweriterations}
	\end{figure}
	We solve the initial value problem \ref{eq:LinearAdvDif} for a fixed number of substeps $s$ with fixed interpolation degree $m=100$ per substep, while still allowing for an early termination of the interpolation if the error is small enough. See Figure \ref{fig:Poweriterations}. While the algorithm did not converge for all combinations of $N$, $\text{sf}$ and P\'eclet numbers we tried, we observe that $n=4$ and $\text{sf}=1.1$ is a robust choice independent of $N$ for our initial vector $u_0$.\\
	For all future experiments we choose $\text{sf}=1.1$ and $n=4$ for the matrix-free \texttt{expleja} algorithm. As an additional optimization we save the approximated eigenvector corresponding to $\lambda_{max}$ when integrating over multiple time steps. We use it as the initial vector for the power method for the next time step. Furthermore we terminate the power method early if the relative change of the approximated eigenvalue is smaller than $0.01$. 
	%	We investigate the rate of convergence for the power method given $A$ and an initial vector $v$. Consider $v=\frac{1}{N}\sum_{j=1}^N v_{j}$, where each $v_j$ is the normalized eigenvector corresponding to the eigenvalue $\lambda_j$. Let $N$ be even.
	%	After $n$ power iteration we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of
	%	\begin{align*}
	%	\frac{\norm{A^{n-1}v}_2}{\norm{A^nv}_2}\abs{\lambda_N} &= 
	%	\sqrt{\frac{\sum_{j=1}^{N}{\lambda_j^{2(n-1)}}}{\sum_{j=1}^{N}{\lambda_j^{2n}}}}\abs{\lambda_N} \\
	%	&= \sqrt{\frac{\sum_{j=1}^{N/2}\sin^{4(n-1)}
	%			\left(\frac{\pi j}{N+1}\right)}{\sum_{j=1}^{N/2}\sin^{4n}
	%			\left(\frac{\pi j}{N+1}\right)}}\sin^2\left(\frac{\pi N}{2(N+1)}\right).
	%	\end{align*}
	%	The first equality holds since all eigenvectors are orthogonal. In order to continue our analysis and get some asymptotic bounds we interpret the sum of sine functions as an integral approximated by the trapezoidal rule. We use the nodes $j/(N+1)$ for $j=0,\dots,{N+1}$.
	%	\[
	%	\int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} =
	%	\frac{1}{(N+1)}\left(2\sum_{j=1}^{N}\sin^{4n}\left(\frac{\pi j}{(N+1)}\right) + \frac{1}{2}\right)
	%	+ \mathcal{O}\left(\frac{1}{12(N+1)^2}\right)
	%	\]
	%	Note that the error of the approximation is strictly positive since the second derivative  math.stackexchange\footnote{\url{https://math.stackexchange.com/questions/50447/integration-of-powers-of-the-sin-x}} we can blissfully accept the identity
	%	\[I_n \defneq \int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} = \frac{\Gamma(2n+0.5)}{\sqrt{\pi}\ \Gamma(2n+1)}. \]
	%	In order to simplify our calculations we take the limit of $N$
	%	\[ 
	%	\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} \xrightarrow[]{N\to\infty}
	%	\sqrt{\frac{I_n}{I_{n-1}}}, 
	%	\]
	%	where
	%	
	%	\begin{align*}
	%	\frac{I_n}{I_{n-1}} &= 
	%	\frac{\Gamma(2n - 1)\Gamma(2n + 0.5)}{\Gamma(2n + 1)\Gamma(2n - 1.5)} \\&=   
	%	\frac{(2n - 2)!}{(2n)!}
	%	\frac{\Gamma(2n + 0.5)}{\Gamma(2n - 1.5)}
	%	\frac{\Gamma(2n)}{\Gamma(2n)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n-2)} \\&=
	%	\frac{1}{2n(2n-1)} 
	%	\frac{2^{1-4n}\sqrt\pi}{2^{5-4n}\sqrt\pi}
	%	\frac{\Gamma(4n)}{\Gamma(4n-4)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n)} \\&=
	%	\frac{1}{32n(2n-1)} 
	%	\frac{(4n-1)!}{(4n-5)!}
	%	\frac{(2n-3)!}{(2n-1)!} \\&=
	%	\frac{(4n-1)(4n-2)(4n-3)(4n-4)}{32n(2n-1)^2(2n-2)} \\&=
	%	\frac{(4n-1)(4n-3)}{8n(2n-1)} \\&=
	%	\frac{4n-1}{4n}\frac{4n-3}{4n-2} \\&=
	%	\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)
	%	%%\\&=1 - \frac{1}{2n}\frac{8n-3}{8n-4} 
	%	\end{align*}
	%	For the third equality we applied the duplication formula for the gamma function. All in all we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of 
	%	\[
	%	\lim_{N\to\infty}\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} =
	%	\sqrt{\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)} \approx
	%	1-\frac{1}{4n-1}
	%	\]
	%	at the limit $N\to\infty$. 
	
	\section{Matrix-free Leja based exponential integrators} \label{sec:expint}
	Exponential integrators are a class of numerical integrators which excel at solving stiff differential equations. Unlike most ordinary differential equation (ODE) solvers their construction is based on the variation-of-constants formula. Consider the semilinear initial value problem
	\begin{align}
	\begin{split}
	\partial_tu &= F(u) = Au + g(u) \\ 
	u(0) &= u_0
	\end{split}\label{semilinear}
	\end{align}
	where $A = \partial_uF$ and $g(u) = F(u)-Au$ is the linear and nonlinear part of $F$ respectively. The solution of the ODE is given by the variation-of-constants formula
	\[
	u(t) = e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u(\tau))d\tau.
	\]
	Similar to Runge-Kutta methods we replace the integrand with a polynomial approximation. Unlike Runge-Kutta methods we leave the matrix exponential untouched and only replace $g$. The most well-known Rosenbrock-type exponential integrator, the exponential Rosenbrock-Euler method, can be obtained by using the left hand rule. By replacing $g$ with $g(u_0)$ we get
	\[
	u(t) \approx e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u_0)d\tau = e^{At}u_0 + \varphi_1(tA)g(u_0),
	\]
	where $\varphi_1(z) = \frac{e^z-1}z$. The exponential Rosenbrock-Euler method is of order 2 and is exact for linear problems, i.e. if g(u)=0. We will refer to it as \texttt{exprb2} in Section \ref{sec:NE}.
	\subsection{Higher order Rosenbrock methods}
	Exponential Rosenbrock methods are a special class of exponential integrators which efficiently solve semi-linear problems \eqref{semilinear}. For a given time step size $\tau$ the numerical solution $u_1$ is given by
	\begin{align}
	\begin{split}
	U_{i} &= e^{c_i \tau A}u_0 + \tau\sum_{j=1}^{i-1}a_{ij}(\tau A)g(U_{j}), \quad \\
	u_{1} &= e^{    \tau A}u_0 + \tau\sum_{i=1}^{s}b_i(\tau A)g(U_{i}),
	\end{split}\label{exprbscheme}
	\end{align}
	where $s\in\mathbb{N}$ and $a_{ij}$, $b_{i}$ are matrix functions. The numerical scheme can be represented as a Butcher tableau
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc}
			$c_1$ &  &  &  & \\
			$c_2$ & $a_{21}(\tau A)$ &  &  & \\
			$\vdots$ & $\vdots$ &  $\ddots$  &  & \\
			$c_s$ & $a_{s1}(\tau A)$ & $\ldots$ & $a_{s,s-1}(\tau A)$  & \\
			\hline
			&$b_1(\tau A)$ & $\ldots$ & $b_{s-1}(\tau A)$ & $b_s(\tau A)$
		\end{tabular}
		.
	\end{table} \noindent The functions $a_{ij}$ and $b_{i}$ are typically given as linear combinations of the $\varphi_k$-functions, which in turn are recursively defined as 
	\[\varphi_{k+1}(z) = \frac{\varphi_k(z)-1}z, \quad \varphi_0(z) = e^z, \quad k\in\mathbb{N}.\]
	For example consider the embedded method
	\begin{table}[H]
		\vspace{-1em}
		\centering
		\renewcommand\arraystretch{1.2}
		\[
		\begin{array}
		{c|ccc}
		0\\
		\frac{1}{2} & \frac{1}{2}\varphi_1(\frac{1}{2}\cdot)\\
		1& 0& \varphi_1\\
		\hline
		\texttt{exprb3} & \varphi_1 - 14\varphi_3 & 16\varphi_3 & -2\varphi_3  \\
		\texttt{exprb4} & \varphi_1 - 14\varphi_3 + 36\varphi_4 & 16\varphi_3 -48\varphi_4 & -2\varphi_3 + 12\varphi_4 
		\end{array},
		\]
		\vspace{-2em}
	\end{table}
	\noindent that is 
	\begin{alignat*}{2}
	U_{1} &= u_0,\\
	U_{2} &= e^{\frac{\tau}{2} A}u_0 &&+ \tau\varphi_1\left(\frac{\tau}{2} A\right)g(U_1) = u_0 + \frac{\tau}{2}\varphi_1\left(\frac{\tau}{2}A\right)F(u_0),\\
	U_{3} &= e^{\tau A}u_0 &&+ \tau\varphi_1(\tau A)g(U_2) = e^{\tau A}F(U_2),\\
	\tilde{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3)(\tau A)g(u_0) + 16\varphi_3(\tau A)g(U_2) - 2\varphi_3(\tau A)g(U_3),\\
	\hat{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3 + 36\varphi_4)(\tau A)g(u_0) + (16\varphi_3 -48\varphi_4)(\tau A)g(U_2) \\
	& &&+ (-2\varphi_3 + 12\varphi_4)(\tau A)g(U_3),
	\end{alignat*}
	where $\tilde{u}_1$ and $\hat{u}_1$ is the numerical solution given by \texttt{exprb3} and \texttt{exprb4} respectively.
	\noindent This scheme is known as $\texttt{exprb43}$ \cite[Example 2.24]{bible}. It uses \texttt{exprb3} as a third-order estimator for its fourth-order method \texttt{exprb4}. Both integrators are well suited for numerical computations since all internal stages can be cheaply computed using the exponential Euler method. \\
	Under the simplifying assumptions
	\[
	\sum_{j=1}^s b_j = \varphi_1, \quad  \sum_{j=1}^s a_{ij} = c_i\varphi_1(c_i\cdot) 
	\]
	for $1\le i\le s$ the scheme \eqref{exprbscheme} can be expressed as 
	\begin{align}
	\begin{split}
	U_i &= u_0 + c_i\tau\varphi_1(c_i \tau A)F(u_0) + \tau\sum_{j=2}^{i-1}a_{ij}(\tau A)D_j, \\
	D_j &= g(U_j) - g(u_0), \quad 2\le j\le s, \\
	u_1 &= u_0 +    \tau\varphi_1(    \tau A)F(u_0) + \tau\sum_{i=2}^{s}     b_i(\tau A)D_i.
	\end{split}\label{eq:Djscheme}
	\end{align}
	The main advantage of this reformulation lies in the fact that the norm of all $D_j$ is expected to be small. This can be exploited by the Leja method by allowing an early termination of the Newton interpolation. \\
	For an efficient implementation of exponential Rosenbrock integrators it is crucial to compute only a single action of a matrix function per stage $U_i$ and for the solution $u_1$. Since the most frequently employed methods depend on linear combinations of $\varphi_k$-functions this can be done using the matrix exponential function.
	
	\subsection{Computing the action of the $\varphi$-functions}
	Exponential integrators rely on the efficient computation of $\varphi_k$-functions. In the matrix case $A\in\mathbb{C}^{N\times N}$ this can be done by slightly expanding $A$, see \cite[Theorem 2.1]{action}.\\
	Let $V = [V_p\dots V_2, V_1]\in\mathbb{C}^{N\times p}$, $u\in\mathbb{C}^{N\times 1}$, $\tau\in\mathbb{C}$ and
	
	\begin{align*}
	\tilde{A} = 
	\left[ \begin{array}
	{cc}A& V \\0 & J\\
	\end{array}\right],  \quad
	J = 
	\left[ \begin{array}
	{cc}0& I_{p-1} \\0 & 0\\
	\end{array}\right],
	\end{align*}
	where $I_{n}$ is the $n\times n$ identity matrix. Let $e_n$ denote the $n$-th $p\times 1$ unity vector. Then
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_j} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{j}\tau^k\varphi_k(\tau A)V_{p-j+k}, 
	\quad j\in\{1,\dots,p\}. 
	\end{align*}
	In particular for $j=p$ we have
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_p} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{p}\tau^k\varphi_k(\tau A)V_{k}.
	\hphantom{V_{-1}, \quad j\in\{1,\dots,p\}.}
	\end{align*}
	This formulation can be directly applied to each stage in \eqref{eq:Djscheme} assuming $a_{ij}$ and $b_j$ are linear combinations of $\varphi_k$-functions. Therefore for each stage only a single action of an expanded matrix exponential has to be evaluated. In total this has to be done $s$ times for an exponential Rosenbrock method with $s$ stages. \\
	%Many exponential Rosenbrock methods can use this relation to reduce computational costs. For most practical integrators each stage only requires a single action of an expanded matrix exponential has to be evaluated. This is in particular true for the exponential Rosenbrock-Euler methods which can be solved in a single step.
	For a matrix-free implementation of $\tilde A$ given an operator $A$ we can simply compute the action of $\tilde{A}$ as follows
	\begin{align}
	\tilde{A}\colvec{2}{v}{w} = \colvec{2}{Av}{0} + \colvec{2}{Vw}{Jw}, \quad v\in\mathbb{C}^{N\times 1}, w\in\mathbb{C}^{p\times 1}. %\label{eq:tildeAv}
	\end{align}
	The Leja method only relies on matrix-vector multiplications with $\tilde{A}$ and therefore the explicit knowledge of $A$ is not required. To summarize, an efficient matrix-free implementation of exponential Rosenbrock-methods can be achieved using the Leja method. In particular \texttt{exprb3} and \texttt{exprb4} can be evaluated by computing three actions of matrix exponentials. 
	
%	
%	\section{Nonlinear Advection-Diffusion-Reaction Equation}
%	Consider the one-dimensional advection-diffusion-reaction equation
%	\begin{align*}
%	\partial_tu &= \alpha\partial_x(u+\partial_xu) + \beta\partial_x(u^2) + u(u-0.5) \quad \alpha,\beta,\ge 0\\
%	u_0(t) &= e^{-80\cdot(t-0.45)^2}, \quad t\in[0,0.1]
%	\end{align*}
%	\begin{align*}
%	\partial_tu &= \alpha\nabla(u+\nabla u) + \beta(\partial_x + \partial_y)(u^2) + u(u-0.5) ~~\text{and}\\
%	u_0(t) &= e^{-80\cdot(t-0.45)^2} ~~\text{with}~~ \alpha,\beta,\ge 0  ~~\text{and}~~ t\in[0,0.1]
%	\end{align*}
%	on the domain $\Omega = [0,1]$.
%	
	\section{Numerical experiments}\label{sec:NE}
	In this section we will apply matrix-free exponential Rosenbrock integrators to multiple advection-diffusion-reaction equations. All experiments are conducted in Python 3.7 \cite{python} with NumPy 1.18.1 \cite{numpy} and SciPy 1.4.1 \cite{numpy}. We use an AMD Ryzen 7 2700 Processor on a Windows machine.
	Consider $d$-dimensional nonlinear variants of the initial value problem in section \ref{sec:LinearADe}. Let $\alpha$, $\beta > 0$ and
	\begin{alignat*}
	\partial\partial_tu &= F(u) &&~~\text{with}~~ t\in[0,0.1],\\
	u_0(x) &= e^{-80\cdot(\norm{x}_2^2-0.45)^2} &&~~\text{with}~~ x\in[0,1]^d,
	\end{alignat*}
	with
	\begin{align*}
		F(u) &= 
		\alpha\nabla((u+1)\nabla u) 
		+ 2\beta (u\cdot\nabla u) 
		+ u(u-0.5).
	\end{align*}
	For all experiments we assume Dirichlet boundary conditions. \\ We compare the behavior of exponential integrators with other matrix-free ODE solvers.
	
	\paragraph{Crank-Nicolson method:}
	We refer to the Crank-Nicolson method of order 2 as \texttt{cn2}.
	In our implementation of \texttt{cn2} we use the Newton-Raphson method to solve the nonlinear system of equations. We treat the Jacobian-vector product $v\mapsto F'(u)v$ as a linear operator. For the resulting system of linear equations we use the SciPy package \texttt{scipy.sparse.linalg.gmres}. For all experiments the relative tolerance is set to $\texttt{tol}/N_\tau$, where $N_\tau$ is the total number of time steps used for solving the ODE. No preconditioner was used for \texttt{gmres}. The Crank-Nicolson method is unconditionally stable and therefore does not have to satisfy the Courant-Friedrichs-Lewy (CFL) conditions for the differential equations we investigate in our experiments.
	
	\paragraph{Explicit Runge-Kutta method:}
	We refer to the explicit midpoint method of order 2 as \texttt{rk2} and refer to the classical Runge-Kutta method of order 4 as \texttt{rk4}. No explicit Runge-Kutta method is A-stable. Therefore small time step sizes have to be chosen when solving stiff differential equations. In our experiments this is the case when the considered differential equation is diffusion-dominated.
	
	\paragraph{Exponential Rosenbrock methods:}
	We refer to the exponential Rosenbrock methods of order 2, 3 and 4 discussed in \ref{sec:expint} as \texttt{exprb2}, \texttt{exprb3} and \texttt{exprb4} respectively.
	The action of the matrix exponential is approximated with the matrix-free \texttt{expleja} algorithm. At each time step we compute the optimal interpolation degree and substep parameter according to \eqref{eq:msmatrixfree}. 
	The maximal interpolation degree is set to 100. Note that the total number of matrix-vector multiplication per time step can still exceed 100 since we have to compute the spectral radius. This typically happens for $s=1$. At each time step we compute the spectral radius using the power method with at most four power iterations.\\
	\\
	Our goal is to investigate the respective computational costs of these methods while achieving a prescribed relative tolerance \texttt{tol}.
%We investigate cost incurred by each integrator in terms of function evaluations \texttt{Feval}, directional derivatives evaluations \texttt{dFeval} and matrix-vector multiplications \texttt{mv}. From now on the cost of evaluating $F$ once will be defined as 1 \texttt{Feval}.
	\subsection{Cost analysis}
	%Let $N_\tau\in\mathbb{N}$ be the total number of time steps. 
	For our cost analysis we investigate the relative number of memory operations incurred by each integrator. Let $N^d$ be the number of grid points. We show that it can be sufficient to measure these costs in terms of function evaluations. By this we mean counting the occurrences of the following procedure. 
	\begin{enumerate}
		\item Load an $N^d$-dimensional vector $v$ into memory.
		\item Evaluate a function $f(v)$ which requires a similar number of read and write operations as $F(v)$.
		\item Store the result in memory.
	\end{enumerate}
	For both \texttt{rk2} and \texttt{rk4} the dominant costs per stage are incurred by one evaluation of $F$. Therefore their cost per time step amounts to two and four function evaluations respectively.
	For \texttt{cn2} and all exponential integrators the cost analysis is much more involved. These methods use expensive subroutines with costs mainly depending on Jacobian-vector products. For $u, v\in\mathbb{R}^{N}$ the directional derivative can be approximated as a finite difference
	\[
	F'(u)v \approx \frac{F(u+\epsilon v)-F(u)}{\epsilon} ~~\text{with}~~ \epsilon > 0.
	\]
	This requires $2N^d$ read operations for loading $u$ and $v$, two evaluations of $F$ and $N^d$ write operations to store the result. 
	However, $F(u)$ can be precomputed once per time step. With this optimization the directional derivative costs one function evaluation plus an extra $2N^d$ read operations for looking up $v$ and $F(u)$. \texttt{In our experiments we neglected the additional costs of $2N^d$. What does Lukas think about that?} \\
	All in all it is reasonable to compare the number of read and write memory operations of each integrator by counting the number of function evaluations.
	
	\subsection{One-dimensional advection-diffusion equation}
	Let $\alpha,\beta,\ge 0$ and 
	\begin{align*}
		F(u) = \alpha\partial_x((u+1)\partial_xu) + 2\beta u\partial_x u + u(u-0.5).
	\end{align*}
	With $\alpha$ and $\beta$ we control the relative strength of the diffusion compared to the advection respectively. The differential equation can be rewritten using the product rule.
	The 
	This leads to
	\begin{align*}
		F(u) &= 
			\alpha((u+1)\partial_{xx}u + (\partial_{x}u)^2) 
			+ 2\beta u\partial_x(u) 
			+ u(u-0.5)
	\end{align*}
	The total derivative is given by
	\begin{align*}
		\partial_{u}F &= 
			\alpha((u+1)\partial_{xx} + (\partial_{xx}u)\operatorname{Id} + (\partial_{x}u)\operatorname{Id} + u\partial_{x}) 
			+ 2\beta(\partial_xu\operatorname{Id} + u\partial_x) 
			+ (2u-0.5)\operatorname{Id}
			,
	\end{align*}
	where $\operatorname{Id}$ is the identity function. We will treat $A = \partial_{u}F$ as a linear operator for all calculations.
	All instances of $\partial_{xx}$ are discretized with centred finite differences. 
%   We approximate $\partial_{x}$ using the upwind scheme for numerical stability. The matrix representation of both discretized operators is given by
%	\[
%		 B = \frac{1}{h^2}
%		\begin{bmatrix}
%			-2 & 1      &        &    \\
%			1  & \ddots & \ddots &    \\
%			   & \ddots & \ddots & 1  \\
%			   &        & 1      & -2
%		\end{bmatrix}
%		,~~ C = \frac{1}{h}
%		\begin{bmatrix}
%			-1 & 1      &        &    \\
%			  & \ddots & \ddots &    \\
%			   & & \ddots & 1  \\
%			   &        &       & -1
%		\end{bmatrix}.
%	\]
%	
	By considering only the terms depending $\partial_{xx}$ we get the modified problem
	\[
		\partial_{t}u = \alpha (u+1)\partial_{xx}u.
	\] 
	Since num
	\[
		\tau \le \frac{h^2}{2\alpha} \eqdefn C_{dif}
	\]
	restricts the maximum possible step size for explicit Runge-Kutta methods. This constraint can be seen in Figure \ref{fig:multi1DNonlinear} on the bottom row for the original problem.
	\begin{figure}[H]
		\newcommand{\adv}{\detokenize{α}=0.1}
		\newcommand{\dif}{\detokenize{β}=0.01}
		\centering
		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment2/multi, \adv, \dif.pdf}
		\caption{}
		\label{fig:multi1DNonlinear}
	\end{figure}
	
	\subsection{Two-dimensional advection-diffusion equation}
	Let $\alpha,\beta,\ge 0$ and 
	\begin{align*}
	\partial_tu &= F(u) = \alpha\nabla(u\nabla_xu) + \beta\partial_x(u^2) + u(u-0.5)
	\end{align*}
	With $\alpha$ and $\beta$ we control the relative strength of the diffusion compared to the advection respectively. The differential equation can be rewritten using $\partial_x(u\partial_xu) = \partial_{xx}u + u\partial_{x}u$ and $\partial_x(u^2) = 2u\partial_{x}u$
	This leads to
	\[
	F(u) = 
	\alpha(\partial_{xx}u + u\partial_{x}u) 
	+ 2\beta u\partial_x(u) 
	+ u(u-0.5)
	\]
	All instances $\partial_{xx}$ are discretized with centred finite differences. We approximate $\partial_{x}$ using the upwind scheme for numerical stability.
	
	\subsection{Experiment 1: Linear advection diffusion equation}
	Consider the one-dimensional advection-diffusion equation
	\begin{align*}
	\partial_tu &= a\partial_{xx}u + b\partial_xu \quad a,b\ge 0\\
	u_0(t) &= e^{-80\cdot(t-0.45)^2} \quad t\in[0,0.1]
	\end{align*}
	with homogeneous Dirichlet boundary conditions on the domain $\Omega = [0,1]$. 
	For a fixed $N\in\mathbb N$ we approximate the diffusive part with second-order central differences on an equidistant grid with grid size $h = \frac{1}{N}$ and grid points $x_i = ih$, $i=0\dots,N$.
	\[\partial_{xx}u(x_i) = \frac{u(x_{i+1}) - 2u(x_i) + u(x_{i-1})}{{h}^2} + \mathcal{O}({h}^2)\]
	In order to limit numerical instabilities we discretize the advective part with forward differences, similar to the upwind scheme.\footnote{Maybe create a seperate section on hybrid difference schemes? There we can also analyze the resulting matrix $A$ itself and plot the eigenvalues. I need sources for that though.}
	\[\partial_{x}u(x_i) = \frac{u(x_{i+1}) - u(x_i)}{h} + \mathcal{O}(h)\]
	The resulting system of ordinary differential equation is given by
	\begin{align*}
	\partial_tu &= Au.
	\end{align*} 
	Some eigenvalues of $A$ can have an extremely large negative real part. Therefore, since no explicit Runge-Kutta method is A-stable, this imposes very stingend conditions on the time step size $\tau$ for $\operatorname{rk2}$ and $\operatorname{rk4}$. We will refer to the Courant-Friedrich-Lewy (CFL) conditions imposed by the advective and diffusive part of $A$ respectively by $C_{adv}$ and $C_{dif}$.  
	\[ C_{adv} = \frac{b\tau}{h} \le 1, \quad C_{dif} = \frac{a\tau}{h^2} \le \frac{1}{2}\] 
	
	
	
	In our case the problem is fully linear and therefore $\texttt{exprb2}$ simplifies to the computation of the action of the matrix exponential function with the Leja method. We write $\texttt{expleja}$ for the single precision Leja method approximation. The reference solution was computed with double precision and therefore uses different nodes.
	
	In order to keep the solution from vanishing, we fix $b = 1$ and only consider coefficients $a\in[0,1]$. The advection-diffusion ratio scaled by the grid size $h$ is represented by the grid P\'eclet number
	
	
	\subsection{Experiment 2: 1D Nonlinear advection diffusion equation}
	\[ \partial_tu = \alpha\partial_{x}((u+1)\partial_{x}u) + \partial_{x}(u^2) + u(u-0.5) \]
	
	We discretize, solve again with rk2, rk4, cn2 and exprb2.
	
	\section{Appendix}
	Matrix analysis, Horn and Johnson, Lemma 5.6.10
	\[\rho(A) = \inf\{\norm{A} : \norm{\cdot} \text{ is an induced operator norm} \} \]
	
	%
	%\subsection{CFL Condition} \label{CFL}
	%We conduct a Von Neumann stability analysis for the diffusion equation
	%\[ \partial_tu = \partial_{xx}u. \]
	%The eigenfunctions of $\partial_{xx}$ are given by
	%\[ u_k(x) = e^{ikx} \]
	%with eigenvalues $-k^2$.
	
	\subsection{Experiment Linear}
	\newcommand{\Pe}{Pe=10.0}
	\newcommand{\precision}{single}
	\newcommand{\safe}{sf=1.1}
	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/12, \precision, \Pe.pdf}
%		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
%	\end{figure}
	
	\section{TODO}

	\begin{itemize}
		\item Section 3, adapt introduction with RAM constraints, not cache
		\item Section 4.1 Explain $v=\sum_{j=1}^N v_j$, Gleichverteilung aller EV		
	\end{itemize}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/multi, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/3, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/4, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	
	\newpage
	\subsection{Experiment Nonlinear 1D}
	\newcommand{\adv}{\detokenize{α}=0.1}
	\newcommand{\dif}{\detokenize{β}=1}
	\renewcommand{\precision}{half}
	{1, \precision, \adv, \dif}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{../Figures/Experiment2/multi, \adv, \dif.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/2, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/3, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/4, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\subsection{Experiment Nonlinear 2D}
	{1, \precision, \adv, \dif}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment_2D/multi, \adv, \dif.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{../Figures/Experiment2/multi, \adv, \dif.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/2, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/3, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/4, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	
	%DESCRIPTION OF THE EXPERIMENT, NUMBER OF POWERITS, SAFETYFACTOR, HOW STABLE IS THE COMPUTATION???
	%First that the matrix $A = A(t)$ changes at every time step and therefore
	
	
	
	
	\clearpage
	\begin{thebibliography}{13}
		\bibitem{rosenbr} M. Caliari, A. Ostermann. Implementation of exponential Rosenbrock-type integrators, Applied Numerical Mathematics 59 (2009), 568-581.
		\bibitem{action} A. Al-Mohy, N. Higham. Computing the action of the matrix exponential, with an application to exponential integrators, SIAM Journal on Scientific Computing 33 (2011), 488-511.
		\bibitem{newt} L. Reichel. Newton interpolation at Leja points, BIT Numerical Mathematics 30 (1990), 332-346.
		\bibitem{advdif} M. Caliari, M. Vianello, L. Bergamaschi. Interpolating discrete advection-diffusion propagators at Leja sequences, Journal of Computational and Applied Mathematics 172 (2004), 79-99.
		\bibitem{lejarev} M. Caliari, P. Kandolf, A. Ostermann, S. Rainer. The Leja method revisited: backward error analysis for the matrix exponential, SIAM Journal on Scientific Computation, Accepted for publication (2016). arXiv:1506.08665.
		\bibitem{bible} M. Hochbruck, A. Ostermann. Exponential integrators, Acta Numerica 19 (2010), 209-286
		\bibitem{polynomialmethods} P. Novati, Polynomial methods for the computation of functions of large unsymmetric matrices, Ph.D. Thesis in Computational Mathematics, University of Trieste, advisor I. Moret (2000).
		\bibitem{newtoninterpolation} L. Reichel, Newton interpolation at Leja points, BIT 30 (2) (1990), 332–346.
		\bibitem{matrixanalysis} R. Horn, C. Johnson, Matrix Analysis, Cambridge University Press (2012).
		\bibitem{trapezoidal} L. N. Trefethen, J.A.C. Weideman, The exponentially convergent trapezoidal rule, SIAM Review 56-3 (2014), 385-458.
		
		\bibitem{python} Python Software Foundation. Python Language Reference, version 3.7. Available at https://www.python.org. Manual at https://docs.python.org/3/. [Online; accessed 2020-02-19]
		\bibitem{numpy} S. v. d. Walt, C. Colbert, G Varoquaux. The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science \& Engineering, 13 (2011), 22-30.
		\bibitem{numpyguide} T. Oliphant. A guide to NumPy, USA: Trelgol Publishing, (2006).
		\bibitem{scipy} P. Virtanen, R. Gommers, T. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. v. d. Walt, M. Brett, J. Wilson, J. Millman, N. Mayorov, A. Nelson, E. Jones, R. Kern, E. Larson, C. Carey, İ. Polat, Y. Feng, E. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. Quintero, C. Harris, A. Archibald, A. Ribeiro, F. Pedregosa, P. v. Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, in press.
		\bibitem{matplotlib} J. Hunter. Matplotlib: A 2D Graphics Environment, Computing in Science \& Engineering, 9, 90-95 (2007).
		\bibitem{pandas} W. McKinney. Data Structures for Statistical Computing in Python, Proceedings of the 9th Python in Science Conference, 51-56 (2010).
	\end{thebibliography}
	
\end{document}
