% !TeX spellcheck = en_US
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman,english]{babel}
\usepackage{blindtext}
\usepackage{color}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{subcaption}
\usepackage{upgreek}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{mathdots}
\usepackage[title]{appendix}
\usepackage{pgffor}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}

%\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\}
%\setlength{\parindent}{0em}

\newcount\colveccount
\newcommand*\colvec[1]{
	\global\colveccount#1
	\begin{bmatrix}
		\colvecnext
	}
	\def\colvecnext#1{
		#1
		\global\advance\colveccount-1
		\ifnum\colveccount>0
		\\
		\expandafter\colvecnext
		\else
	\end{bmatrix}
	\fi
}

\begin{document}
	
	
	\theoremstyle{definition}
	\newtheorem{defn}{Definition}[section]
	\newtheorem{bsp}{Beispiel}[section]
	\newtheorem{satz}{Satz}[section]
	\newtheorem{prop}{Proposition}
	\newtheorem{lem}[satz]{Lemma}
	\newtheorem*{bem}{Bemerkung}
	\newtheorem*{rem}{Remark}
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
	
	%\title{The Leja method in Python}
	%\subtitle{supervised by Peter Kandolf, Alexander Ostermann}
	%\maketitle
	%\author{Maximilian Samsinger}
	\begin{titlepage}
		\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
		\vspace{3cm}
		\begin{center}
			{\Large Master Thesis}
			\vspace{50pt}\\
			\textbf{\Huge Matrix-free Leja based exponential integrators in Python}
			\vspace{40pt}\\
			\textbf{\Large Maximilian Samsinger}\vspace{20pt}\\
			{\large\today}
			\vspace{120pt}\\
			{\Large Supervised by Lukas Einkemmer and\\
				Alexander Ostermann\vspace{10pt}}
		\end{center}
	\end{titlepage}
	\textsf{
		{\hspace{-16pt}\large\textbf{Leopold-Franzens-Universität Innsbruck}}
		\begin{figure}[!htp]
			\begin{flushright}
				\includegraphics[scale=0.1]{./uni_logo}
			\end{flushright}
		\end{figure}\\\\
		{\Large\textbf{Eidesstattliche Erklärung}}\\\\
		Ich erkläre hiermit an Eides statt durch meine eigenhändige Unterschrift, dass ich die\\ vorliegende Arbeit selbständig verfasst und keine anderen als die angegebenen Quellen und\\ Hilfsmittel verwendet habe. Alle Stellen, die wörtlich oder inhaltlich den angegebenen Quellen \\entnommen wurden, sind als solche kenntlich gemacht.\\\\
		Ich erkläre mich mit der Archivierung der vorliegenden Masterarbeit einverstanden.
		\vspace{40pt}\\
		\begin{center}
			\ensuremath{\overline{\mbox{Datum}\hspace{8em}}
				\hspace{10em}
				\overline{\mbox{Unterschrift}\hspace{10em}}
			}
			\thispagestyle{empty}
	\end{center}}
	\pagebreak
	
	
	
	\begin{center}\textbf{\Huge Matrix-free Leja based exponential integrators in Python}\end{center}
	\begin{center}\textbf{Abstract}\end{center}
	\begin{abstract}
		In this master thesis we develop an algorithm to approximate the action of a matrix exponential function for matrix-free linear operators. This is achieved by using a modified version of the real Leja method. We choose optimal interpolation parameters based on a spectral radius estimate computed by the power method. With this procedure we construct exponential Rosenbrock-type integrators to solve stiff advection-diffusion-reaction equations. We compare the performance of these integrators with other matrix-free differential equation solvers. 
 		As part of this thesis we publish the code for the matrix-free Leja method for the action of the matrix exponential function on GitHub \footnote{INSERT LINK HERE}.
	\end{abstract}
	
	\setcounter{page}{1}
	
	\section{Introduction}
	%%%%%%%%
	%Examples needed
	%%%%%%%%%%
	Consider the action of the matrix exponential function 
	\[e^Av ~~\text{where}~~ A\in\mathbb{C}^{N\times N} ~~\text{and}~~ v\in\mathbb{C}^N.\] 
	Due to computational constraints it can be difficult or impossible to compute $e^A$ in a first step and then the action $e^Av$ in a separate step. This is especially true in applications where $N>10000$ is common. Furthermore the matrix exponential of a sparse matrix is in general no longer sparse. Therefore it is more feasible to compute the action of the matrix exponential in a single step. This can be done by approximating the matrix exponential with a matrix polynomial $p_m$ of degree $m$ in $A$
	\[e^Av \approx p_m(A)v.\]
	This approach has many benefits. The cost of the computation of $p_m(A)v$ mainly depends on the calculation of $m$ matrix-vector multiplications with $A$, which is inexpensive for sparse matrices. Furthermore the explicit knowledge of $A$ itself is no longer required. The matrix $A$ can be replaced by a linear operator, which can be more convenient to work with and save memory. 
	
	We introduce an polynomial interpolation procedure, the Leja method, in Section~\ref{sec:LejaMethod}. The Leja method has many computational advantages. All interpolation nodes can be precomputed and stored for later use. The interpolation itself is done iteratively and can be interrupted if the interpolation error is small enough. For entire functions the Leja interpolation has beneficial convergence properties which translate well to the corresponding matrix functions. After this brief introduction to the Leja method we discuss its application to the action of the matrix exponential function. We consider an approach which bounds the backward error of the interpolation. This will lead to an algorithm which provides cost-minimizing parameters for the Leja interpolation based on the operator norm $\norm{A}$ of $A$. \\
	In Section~\ref{sec:matrixfreeimplementation} we adapt the algorithm in order to obtain a matrix-free version. The implementation of the Newton interpolation is straight-forward for linear operators. However, without the matrix representation it can be computationally infeasible to calculate the operator norm. Instead we replace all instances of $\norm{A}$ with the spectral radius, which can be cheaply estimated using the power method. This modification introduces new challenges. On the one hand it is unclear how many power iterations are necessary for a sufficient estimate of the spectral radius. On the other hand it is no longer possible to specify the norm for the backward error bound. \\
	In Section~\ref{sec:LinearADe} we study the matrix-free Leja method for the discretized linear, one-dimensional advection-diffusion equation. For this specific problem we experimentally and numerically investigate the behavior of the power method and identify a suitable choice for the number of power iterations. Furthermore, this choice is reasonable in the nonlinear case as well. In order to verify that claim we introduce exponential Rosenbrock-type integrators in Section~\ref{sec:expint}. Finally, we compare the performance of three matrix-free Leja based exponential integrators of different order against other matrix-free differential equation solvers in Section~\ref{sec:NE}.
	

	\section{The Leja method} \label{sec:LejaMethod}
	This section serves as an introduction to the Leja method for approximating the action of the exponential function. We briefly cover the key concepts and definitions in Section~\ref{sec:LejaInterpolation} and~\ref{sec:ApproxMatrixExponential}. For more details and all proofs we refer to~\cite{advdif} and~\cite{lejarev}. 
	
	\subsection{Leja interpolation} \label{sec:LejaInterpolation}
	Let $K\in\mathbb{C}$ be a compact set in the complex plane and $\xi_0\in K$ be arbitrary. The sequence $(\xi_k)_{k=0}^{\infty}$ recursively defined as
	\[\xi_k = \underset{{\xi\in K}}{\operatorname{arg\,max}}\displaystyle\prod_{j=0}^{k-1}\abs{\xi-\xi_j}\]
	is called a Leja sequence. Due to the maximum principle all elements in the sequence realize their maximum on the border $\partial K$. Typically $\xi_0$ is also chosen on $\partial K$.\\
	For analytical functions $f\!:K\to\mathbb{C}$ the Newton interpolation polynomial $p_m$ with nodes $(\xi_k)_{k=0}^{m}$ has the following beneficial properties. 
	\paragraph{Convergence properties:}
	The sequence $(p_m)_{m=0}^\infty$ converges maximally to $f$. That is, let $(p_m^{*})_{m=0}^\infty$ be the best uniform approximation polynomials for $f$ in $K$. Then
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{K}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{K}^{1/m},\]
	where ${\norm{\cdot}}_K$ is the maximum norm on $K$. Furthermore if $f$ is an entire function, then $(p_m)_{m=0}^\infty$ converges superlinearly to $f$
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{\mathbb{C}}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{\mathbb{C}}^{1/m} = 0. \]
	For entire functions $f$ the corresponding matrix polynomials achieves similar superlinear convergence
	\[\limsup\limits_{m \rightarrow \infty}\norm{f(A)v-p_m(A)v}_{2}^{1/m} = 0, \]
	for $A\in\mathbb{C}^{n\times n}, v\in\mathbb{C}^n$.
	
	\paragraph{Early termination:}
	The Newton interpolation polynomial $p_m$ can be constructed iteratively since the corresponding Leja interpolation points $(\xi_k)_{k=0}^{m}$ are defined recursively. Therefore if the approximation $p_n \approx f$ is accurate enough after $n<m$ steps the interpolation can be stopped early to reduce the cost of the interpolation. Note that this is not possible with Chebyshev nodes.
	
	\paragraph{Leja sequence can be stored:}
	For a given $K$ the Leja interpolation nodes only need to be computed once and for all. These values can be stored a priori and loaded once they are needed for the interpolation. If $f$ is fixed the same is also true for the corresponding divided differences. \\
	
	In summary the Leja points offer convergence properties similar to Chebyshev nodes for interpolation, while having computational advantages. All results hold true for the corresponding matrix interpolation polynomials.
	
	\subsection{Approximating the matrix exponential function:} \label{sec:ApproxMatrixExponential}
	Inspired by the previous subsection we try to find a low-cost approximation of the action of the matrix exponential $e^Av$ using Leja interpolation polynomials. From now on, we will fix 
	\[K=[-c,c], ~~ f = e^\cdot  ~~\text{and}~~ \xi_0 = c \]
	for $c>0$. With $L_{m,c}$ we denote the Leja interpolation polynomial on the interval $[-c,c]$ with Leja points $(\xi_j)_{j=0}^{m}$. We use the well-known property of the exponential function
	\[e^Av = (e^{s^{-1}A})^sv, ~~\text{with}~~ s\in\mathbb{N}.\]
	Now we can approximate the action of the matrix exponential in $s$ substeps
	\[v_0\defneq v, ~~ v_{j+1}\defneq L_{m,c}(s^{-1}A)v_j, ~~\text{and}~~ v_s \approx e^Av.\]
	So far we placed no restrictions on $m$, $s$ and $c$. We choose optimal parameters based on the backward-error analysis done in~\cite{lejarev}.
	
	\paragraph{Bounding the backward error} For a given matrix $A$ we interpret the Leja interpolation polynomial as the exact solution of a perturbed matrix exponential function
	\[
	L_{m,c}(s^{-1}A)^sv \eqdefn e^{A + \Delta A}v\\
	\]
	Our goal is to bound the backward error 
	\[
	\frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}, 
	\]
	for a given tolerance $\operatorname{tol}$. Furthermore we want to minimize the cost of the interpolation. A priori it is unclear for which values $m$, $s$ and $c$ the inequality is satisfied. The authors of~\cite{lejarev} conducted a backward error analysis and chose an approach which puts an upper bound on $c$ depending only on $s$ and $m$. For various tolerances $\operatorname{tol}$ they precomputed values $\theta_m$, see~\ref{table:thetam}, which satisfy
	\[
	\text{If}\ \norm{s^{-1}A}\le \theta_m\ \text{and}\ 0 \le c \le \theta_m\ \text{then}\ \frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}.
	\]
	\begin{table}[tbp]
		\begin{tabular}{r|rrrrrrr}
			$m$ &        5 &       10 &       15 &       20 &       25 &       30 &       35 \\\hline
			half & 6.43e-01 & 2.12e+00 & 3.55e+00 & 5.00e+00 & 6.37e+00 & 7.51e+00 & 8.91e+00 \\
			single & 9.62e-02 & 8.33e-01 & 1.96e+00 & 3.26e+00 & 4.69e+00 & 5.96e+00 & 7.44e+00 \\
			double & 1.74e-03 & 1.14e-01 & 5.31e-01 & 1.23e+00 & 2.16e+00 & 3.18e+00 & 4.34e+00 \\
			\\
			$m$ &       40 &       45 &       50 &       55 &       60 &       65 &       70 \\\hline
			half & 1.00e+01 & 1.10e+01 & 1.23e+01 & 1.35e+01 & 1.48e+01 & 1.59e+01 & 1.71e+01 \\
			single & 8.71e+00 & 1.00e+01 & 1.15e+01 & 1.27e+01 & 1.40e+01 & 1.52e+01 & 1.64e+01 \\
			double & 5.48e+00 & 6.67e+00 & 7.99e+00 & 9.24e+00 & 1.06e+01 & 1.18e+01 & 1.32e+01 \\
			\\
			$m$ &       75 &       80 &       85 &       90 &       95 &      100 \\ \hline
			half & 1.84e+01 & 1.94e+01 & 2.07e+01 & 2.20e+01 & 2.30e+01 & 2.42e+01 \\
			single & 1.76e+01 & 1.87e+01 & 1.99e+01 & 2.12e+01 & 2.23e+01 & 2.35e+01 \\
			double & 1.46e+01 & 1.58e+01 & 1.71e+01 & 1.86e+01 & 1.99e+01 & 2.13e+01
		\end{tabular}
		\caption{Samples of the precomputed values $\theta_m$. The backward error of the Leja interpolation is bounded if $c\le\theta_m$, where $[-c,c]$ is the interpolation interval and $m$ the interpolation degree. Half, single and double correspond to the tolerances $2^{-10}$, $2^{-24}$ and $2^{-53}$ respectively~\cite[Table 1]{lejarev}.}
		\label{table:thetam}
	\end{table}	
	For our purposes it is important to note that the optimal choice for $c$ is given by $c=\rho(s^{-1}A)$, where $\rho(A)$ is the spectral radius of $A$. However, computing $\rho(A)$ introduces additional costs for the algorithms proposed in~\cite{lejarev}. Our matrix-free implementation relies on the computations of the spectral radius, but it does not need to compute the operator norm $\norm{A}$, see Section~\ref{sec:matrixfreeimplementation}.
	
	\paragraph{Choosing cost-minimizing parameters}
	The cost of the Leja interpolation mainly depends on the the number of matrix-vector products
	\[
	C_{m} = sm. 
	\]
	In order to minimize the costs of the interpolation $C_m$ we select the smallest $m$ for any given $s$ such that
	\[
	\norm{s^{-1}A} \le \theta_m
	\]
	is satisfied. This leads to the optimal choice for $m$ and $s$ 
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil.
	\end{split} \label{eq:ms}
	\end{align}
	In our algorithm we set $m_{\operatorname{max}} = 100$ in order to avoid over- and underflow errors.
	
	\paragraph{Shifting the matrix}
	The cost of the interpolation can be decreased by employing a shift $\mu\in\mathbb{C}$. Let $I$ be the identity matrix. We replace the matrix $A$ with $A-\mu I$ for all computations. If the shifted matrix $A-\mu I$ satisfies $\norm{A -\mu I} < \norm A$ then the cost $C_{m_*}$ of the interpolation decreases.
	We compensate for the shift by multiplying with $e^\mu$ since
	\[
	e^{A} = e^{\mu}e^{A-\mu I}.
	\]
	A well-chosen shift centers the eigenvalues of $A-\mu I$ around $0$. Such a shift can be found by using Gerschgorin's circle theorem. This is, however, not possible in the matrix-free case.
	
	\section{Matrix-free implementation}\label{sec:matrixfreeimplementation}
Matrix-free methods are algorithms which use linear functions, but do not explicitly rely on their respective matrix-representations. These methods are preferable when saving and loading matrix coefficients becomes prohibitively expensive. As a motivational example we perform a cost analysis in terms of memory operations for finite difference schemes in section~\ref{sec:FDS}.
We develop a matrix-free version of the Leja method for the action of the matrix exponential in section~\ref{sec:MatrixFreeLejaMethod}. Finally, we discuss the advantages and disadvantages of this new algorithm.

%In this section we demonstrate this for finite difference schemes and develop a matrix-free version of the Leja method for the action of the matrix exponential. This is  
%The cost of the Leja interpolation mainly depend on the number of matrix-vector multiplications. For large matrices it is memory-inefficient to load the coefficients first and compute a matrix-.
%We will demonstrate 

%In this section the a matrix-free version of the Leja method for the action of a matrix exponential. After a brief motivation we discuss the main advantages and disadvantages of such an approach. \\
%Performance on most modern systems is limited by memory bandwidth. A matrix-free implementation gives us the opportunity to significantly save memory and thus increase performance.

\subsection{Matrix-free methods for finite difference schemes} \label{sec:FDS}
Discretizing partial differential equations often leads to stencils, which are fixed update rules that take the geometry of the problem into account. In the simplest case, which is the only one we consider, each point on a grid is updated by a linear combination of itself and its neighbours. This can be expressed as a linear function $A$ acting on a vector $u$. 

%Performance on most modern systems is limited by memory bandwidth. A matrix-free implementation gives us the opportunity to significantly save memory and thus increase performance.

On modern systems the available memory bandwidth is the bottleneck for the computation of these updates. It is beneficial to use algorithms, which do not rely on the matrix representation of $A$. They give us the opportunity to significantly reduce the number of memory transactions and thus increase performance. We demonstrate this for finite difference schemes of the following form.

\begin{align}
\partial_{t}u = Au = \sum_{j=-n}^{n} a_{k+j}u_{k+j} \label{eq:stencil}
\end{align}
for $n\in\mathbb{N}$, $u\in\mathbb{R}^N$ and periodic boundaries i.e. $a_{l+N} = a_{l}$ and $u_{l+N}=u_{l}$ for all $l\in\mathbb{Z}$, $i\in\{0,1\}$.
We assume the discretization was performed on a equidistant grid and all real numbers are stored in double-precision floating-point format.
The solution of \eqref{eq:stencil} can be approximated with the Leja method. The Newton interpolation itself can be computed without the explicit knowledge of the matrix coefficients. For fixed interpolation parameters we can compare the performance of the Leja method for the regular and the matrix-free case by counting the number of read and write operations necessary to compute $Av$, where $v$ is an arbitary vector.

%Since the cost of the Leja method mainly depend on the number of matrix-vector products, we can performing a cost analysis in terms of memory operations for $Av$, where $v$ is an arbitrary vector, to investigate the performance of the Leja method.\\ 

%We count the number of read and write operations necessary to compute $Au$. 
Reading $v$ and writing the result of $Av$ costs $2N$ memory operations in total. If $A$ is stored as a dense matrix then $N^2$ entries have to be loaded. These costs are reduced to $(2n+1)N$ for sparse matrices, since only $2n+1$ entries have to be loaded for each row in $A$. However, depending on the sparse format chosen, additional information has to be accessed that specifies the index of each matrix entry. For example, the compressed sparse row (CSR) format requires storing $N$ and $(2n+1)N$ integers for the row and column indices respectively. Integers need four bytes of memory space compared to 8 bytes for floating point numbers in double precision. Therefore $(24n + 16)N$ bytes need to be accessed for the matrix in CSR format. In total $(24n + 32)N$ bytes are read or written for the computation of $Au$.
In the matrix-free case the $(2n+1)$ coefficients $a_{-n}, \dots, a_{n}$ only need to be loaded once. This reduces the number of memory operations for the calculation of $Au$ to $2N + 2n + 1$. In total $16N + 16n + 8$ bytes need to be either read or written. For a simple 5-point stencil, which is commonly used to calculate the numerical derivative in two dimensions, this reduces the number of memory operations by a factor of up to 5.

This example shows that most of the memory operations are incurred by loading the coefficients of the matrix $A$. Furthermore it motivates the development of a fully matrix-free Leja method for the matrix exponential function. 


	%%%%%%%%%%%%%%%% CHANGE THIS CHANGE THIS CHANGE THIS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	In this section we introduce a matrix-free version of the Leja method for the action of a matrix exponential. After a brief motivation we discuss the main advantages and disadvantages of such an approach. \\
%	Performance on most modern systems is limited by memory bandwidth. A matrix-free implementation gives us the opportunity to significantly save memory and thus increase performance. For example, a matrix-free implementation reduces the amount of memory transactions compared to asparse matrix implementation based on the CSR format by at least a factor of 5.

%	Linear operators can be more convenient to work with in applications where the matrix representation is not easily available. They can also be more memory efficient. For the finite difference schemes we discussed at the beginning of this section we only need to store $(2n+1)$ values independent of $N$. This serves as a motivation to propose an alternative for the classical Leja method we discussed in Section~\ref{sec:LejaMethod}.
	\subsection{Matrix-free Leja method} \label{sec:MatrixFreeLejaMethod}
	For the most part it is unproblematic to use linear operators for the Leja method instead of matrices, since the Newton interpolation only relies on computation of matrix-vector products. However, difficulties arise when the interpolation parameters need to be determined. Without the matrix representation it can be expensive to compute the operator norm $\norm A$ in \eqref{eq:ms}. We will circumvent this problem by replacing $\norm{A}$ with the spectral radius $\rho(A)$. 
	
	The backward error analysis in~\cite{lejarev} holds true for every operator norm.
	We use a well-known result from the matrix analysis literature~\cite[Lemma 5.6.10.]{matrixanalysis}. For every $A$ and for every $\varepsilon>0$ exists an induced operator norm $\norm{\cdot}_{A,\varepsilon}$ such that
	\[
	\rho(A) \le \norm{A}_{A,\varepsilon} \le \rho(A) + \varepsilon. 
	\]
	While the first inequality holds true for every operator norm, it is not possible to select an operator norm independent of either $A$ or $\varepsilon$ for the second one. We choose $\varepsilon$ small enough, such that
	\[
	\norm{s^{-1}A}_{A,\varepsilon} \le \underset{\rho(s^{-1}A) < \theta_m}{\min}\theta_m.
	\]
	For this choice of $\norm{\cdot}_{A,\varepsilon}$ the cost-minimizing parameters are given by
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil.
	\end{split}\label{eq:msmatrixfree}
	\end{align}
	The explicit knowledge of $\norm{\cdot}_{A,\varepsilon}$ is no longer required. Additionally we can choose $c=\rho(A)$ without introducing additional costs, since we have to compute $\rho(A)$ to determine $m_*$ and $s_*$. 
	For positive and negative semi-definite operators $A$ we select the shift $\mu = -\rho(A)/2$ and $\mu = \rho(A)/2$ respectively. This shift works particularly well if the absolutely smallest eigenvalue of $A$ is close to $0$.\\
	This approach has some drawbacks. Although we are able to bound the backward error
	\[
	\frac{\norm{\Delta A}_{A,\varepsilon}}{\norm{A}_{A,\varepsilon}} \le \operatorname{tol}  
	\]
	we can no longer specify in which norm this error has to be bound. Furthermore, it can be hard to find a good shift $\mu$ for non-semi-definite operators.
	\paragraph{Power method}
	The spectral radius $\rho(A)$ can be cheaply approximated using the power method. 
	%For each $j\in\{1\dots N\}$ let $(\lambda_j,v_j)$ be an eigenpair of $A$. 
	%Furthermore let all eigenvectors be normalized and all eigenvalues be sorted in descending order with respect to their modulus. 
	Given an initial vector $b_0\in\mathbb{C}^N$ the $n$-th iteration of the power method is given by
	\[
		b_{n+1} = \frac{Ab_n}{\norm{Ab_n}}.
	\]
	In the limit $\norm{Ab_{n}}$ converges to the spectral radius of $A$. This can be used to compute \eqref{eq:msmatrixfree}. However, the power method underestimates $\rho(A)$, which might cause the Leja method to not converge. Therefore we have to multiply the estimate with a safety factor. The convergence speed depends on the eigenvalues of $A$. In particular if $\abs{\lambda_{1}}\gg\abs{\lambda_{2}}$ we expect fast convergence for the power method. A more thorough analysis for the discretized linear advection-diffusion equation will be conducted in Section~\ref{sec:LinearADe}.

	From now on we denote matrix-free Leja method for the matrix exponential function as \texttt{expleja}. Depending on the chosen tolerance, see Table~\ref{table:thetam}, we will refer to the algorithm as half, single or double precision \texttt{expleja} respectively.
	
	\section{Linear advection-diffusion equation}\label{sec:LinearADe}
	In this section we consider a simple initial value problem which serves as a test-bed for future experiments. We also want to examine the power method, an algorithm to (under)estimate the largest eigenvalue of an operator with respect to the modulus.\\
	Consider the one-dimensional advection-diffusion equation
	\begin{align}
	\begin{split}
	\partial_tu &= a\partial_{xx}u + b\partial_xu ~~\text{with}~~ a,b\ge 0 ~~\text{and}\\
	u_0(x) &= e^{-80\cdot(x-0.45)^2}\hphantom{,} ~~\text{with}~~ x\in[0,1]
	\end{split}\label{eq:LinearAdvDif}
	\end{align}
	on the time interval $[0,0.1]$. For a fixed $N\in\mathbb N$ we approximate the diffusive part of the differential equation with second-order central differences on an equidistant grid with grid size $h = \frac{1}{N-1}$ and grid points $x_k = kh$, $k=0\dots,N-1$
	\[\partial_{xx}u(x_k) = \frac{u(x_{k+1}) - 2u(x_k) + u(x_{k-1})}{{h}^2} + \mathcal{O}({h}^2).\]
	In order to avoid numerical instabilities we discretize the advective part with forward differences, similar to the upwind scheme
	\[\partial_{x}u(x_k) = \frac{u(x_{k+1}) - u(x_k)}{h} + \mathcal{O}(h).\]
	The resulting system of ordinary differential equation is given by
	\begin{align*}
	\partial_tu &= Au.
	\end{align*}
	In order to measure the relative strength of advection compared to diffusion we employ the P\'eclet number $\operatorname{Pe} = \frac{b}{a}$.
	\begin{figure}[ht]
		\newcommand{\precision}{single}
		\newcommand{\Pe}{Pe=10.0}
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
		\caption{Approximation of $e^{0.1A}u_0$ using single precision \texttt{expleja} for a fixed interpolation degree $m=100$ and varying number of substeps $s$. The relative error is measured in the Eucledian norm. The reference solution was computed using the double precision \texttt{expleja} algorithm.}
		\label{fig:Experiment1}
	\end{figure} 
	\noindent The solution of the differential equation is given by $e^{0.1A}u_0$, which can be approximated using the Leja method, as shown in Figure~\ref{fig:Experiment1}. For the matrix-free case we need to compute the spectral radius of $A$, which can be done using the power method.
	
	
	\subsection{Analysis of the power method}\label{poweritanalysis}
	\begin{figure}[t]
		\newcommand{\boundary}{periodic}
		\centering
		\includegraphics[width=1.\columnwidth]{{../Figures/Spectrum/\boundary}.pdf}
		\caption{The spectrum of $A$. We assume \boundary\ boundary conditions.}
		\label{fig:spectrum}
	\end{figure}
	%All eigenvalues of the discretized one-dimensional using the upwind scheme are -1/h.   
	%	The eigenvalues of discretized one-dimensional Laplace operator $A_{Dif}\in\mathbb{R}^{N\times N}$ on the interval $[0,1]$ with periodic boundary conditions are given by
	%\[
	%\lambda_j =
	%\begin{cases*}
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi (j-1)}{2(N+1)}\right),\quad\text{if j is odd}\\
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi j}{2(N+1)}\right),\quad\text{if j is even}
	%\end{cases*} 
	%,\quad j=1,\dots, N.
	%\]
	We investigate the rate of convergence of the power method to the largest eigenvalue (with respect to the modulus) $\lambda_{max}$ of $A$.
	For our analysis we assume periodic boundary conditions
	\[ A = \frac{a}{h^2}
	\begin{bmatrix}
	-2     & 1  & 0      & \cdots & 0  & 1      \\
	1      & -2 & 1      &        &    & 0      \\
	0      & 1  & \ddots & \ddots &    & \vdots \\
	\vdots &    & \ddots & \ddots & 1  & 0      \\
	0      &    &        & 1      & -2 & 1      \\
	1      & 0  & \cdots & 0      & 1  & -2
	\end{bmatrix}
	+\frac{b}{h}
	\begin{bmatrix}
	-1     & 1  & 0      & \cdots & \cdots  &   0    \\
	0      & -1 & 1      &        &    & \vdots      \\
	\vdots &    & \ddots & \ddots &    & \vdots	 \\
	\vdots &    & 		 & \ddots & 1  &  0      \\
	0      &    &        &        & -1 & 1      \\
	1      & 0  & \cdots & \cdots & 0  & -1
	\end{bmatrix}.
	\]
	Consider the discrete Fourier basis
	\[
	v_j = \frac{1}{\sqrt N}\colvec{4}{e^{i\frac{2\pi}{N}j 0}}{e^{i\frac{2\pi}{N}j 1}}{\vdots}{e^{i\frac{2\pi}{N}j(N-1)}}, \quad j\in{0\dots N-1}.
	\]
	Each $v_j$ is an eigenvector of $A$
	\[
	Av_j = \lambda_jv_j, 
	\]
	where the eigenvalues $\lambda_j$ are given by
	\begin{align*}
	\lambda_j &= \frac{a}{h^2}\left(e^{i\frac{2\pi}{N}j} - 2 + e^{-i\frac{2\pi}{N}j}\right)-\frac{b}{h}\left(e^{i\frac{2\pi}{N}j}-1\right) \\ 
	&=-\left(\frac{4a}{h^2}-\frac{2b}{h}\right) \sin^2\!\left(\frac{\pi j}{N}\right) + i\frac{b}{h}\sin\!\left(\frac{2\pi j}{N}\right),
	\end{align*}
	see Figure~\ref{fig:spectrum}. We restrict our analysis to the case $a=0$ and $b=1$, i.e.
	\begin{align*}
	\lambda_j =-\frac{4}{h^2}\sin^2\!\left(\frac{\pi j}{N}\right).
	\end{align*}
	The modulus is maximized for $j=\lfloor N/2\rfloor$, i.e.  $\lambda_{max} = \lambda_{\lfloor N/2\rfloor}$.
	The convergence speed of the power method depends on the starting vector $v$, which can represented as a linear combination of eigenvectors $v_j$. The simplest, non-trivial choice for $v$ is given by $\frac{1}{N}\sum_{j=0}^{N-1} v_j$, which is the normalized sum of all eigenvectors. We use this choice for the remainder of this section as it is comparatively easy to study, while providing insight to a more general setting. 
	Let $n\in\mathbb{N}$ be the number of power iterations. We begin our analysis with the following auxiliary calculation
	\begin{align*}
	\norm{A^nv}_2
	&= \sqrt{\frac{1}{N}\sum_{j=0}^{N-1}{\abs{\lambda_j}^{2n}}}
	= \frac{2^{2n}}{h^{2n}}\sqrt{I_{N,n}} ,
	\end{align*}
	where
	\begin{align*}
	I_{N,n} &= \frac{1}{N}\sum_{j=0}^{N-1} \sin^{4n}\!\left(\frac{\pi j}{N}\right).
	\end{align*}
	%&\xrightarrow[]{N\to\infty} (4a)^{2n} \int_{0}^{1} \sin^{4n}(\pi x)dx = \\
	%&= (4a)^{2n} \frac{2}{\pi} \int_{0}^{\frac\pi 2} \sin^{4n}(x)dx \\
	%&= (4a)^{2n} \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5),
	%where $\operatorname{B}$ is the beta function. 
	The first equality holds since all eigenvectors are orthogonal. We interpret the sum of sine functions as an integral approximated by the composite trapezoidal rule
	\[
		I_n\defneq\int_{0}^{1}{\sin^{4n}\!\left(\pi x\right)} \approx I_{N,n},
	\]
	with nodes $\frac{j}{N}$ for $j=0$, \dots, $N$.
	Furthermore if we use the nodes $\frac{k}{N}$ for $k=0$, \dots, $2N$ we have
	\[
	2I_n = \int_{0}^{2}{\sin^{4n}\!\left(\pi x\right)} \approx
	\frac{1}{N}\sum_{k=0}^{2N-1}\sin^{4n}\!\left(\frac{\pi k}{N}\right)
	= \frac{2}{N}\sum_{j=0}^{N-1}\sin^{4n}\!\left(\frac{\pi j}{N}\right)
	= 2 I_{N,n}.
	\]
	due to symmetry. Since $\sin^{4n}$ is a $2\pi$-peroidic trigonometric polynomial of degree $4n$ the trapezoidal rule is exact if $2N>4n$. In general the error converges exponentially to $0$. For readers unfamiliar with these properties we refer to~\cite[Corollary 3.3]{trapezoidal}. 
	By comparing both trapezoidal approximations we establish $I_n = I_{n,N}$ if $N > 2n$.\\
	We underestimate the largest eigenvalue $\lambda_{max}$ by a factor of
	\begin{align*}
	\frac{\norm{A^{n+1}v}_2}{\norm{A^nv}_2}\frac{1}{\abs{\lambda_{max}}}
	= \frac{\frac{2^{2n+2}}{h^{2n+2}}\sqrt{I_{N,n+1}}}{\frac{2^{2n}}{h^{2n}}\sqrt{I_{N,n}}}\frac{1}{\abs{\lambda_{max}}} = \sqrt{\frac{I_{N,n+1}}{I_{N,n}}}\sin^{-2}\!\left(\frac{\pi}{N}\left\lfloor\frac{N}{2}\right\rfloor\right).
	\end{align*}
	using the power method. From now on we assume $N > 2(n+1)$. By using the properties of the beta function $\operatorname{B}$ we get
	\[
	I_{N,n} = I_{n} = \int_{0}^{1} \sin^{4n}(\pi x)dx = \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5) = \frac{\Gamma(2n + 0.5)\Gamma(0.5)}{\pi \Gamma(2n + 1)}
	\]
	\noindent where $\Gamma$ is the gamma function. Finally we can simplify the ratio
	\begin{align*}
	\frac{I_{N,n+1}}{I_{N,n}} &=   
	\frac{\Gamma(2n + 1)\Gamma(2n + 2.5)}{\Gamma(2n + 3)\Gamma(2n + 0.5)} \\&= 
	\frac{(2n)!}{(2n+2)!}
	\frac{\Gamma(2n + 2.5)}{\Gamma(2n + 0.5)}\\&=
	\frac{(2n)!}{(2n+2)!}
	\frac{2^{4n}\sqrt\pi}{2^{4n+4}\sqrt\pi}
	\frac{(4n+4)!}{(4n)!}
	\frac{(2n)!}{(2n+2)!} \\&=
	\frac{(4n+4)(4n+3)(4n+2)(4n+1)}{16(2n+2)^2(2n+1)^2} \\&=
	\frac{(4n+3)(4n+1)}{(4n+4)(4n+2)}\\&=
	\left(1-\frac{1}{4n+4}\right)\left(1-\frac{1}{4n+2}\right)
	\end{align*}
	For the third equality we applied the duplication formula for the gamma function. All in all the power method underestimates the largest eigenvalue $\lambda_{max}$ by a factor of 
	\[
	\frac{\norm{A^{n+1}v}_2}{\norm{A^{n}v}_2}\frac{1}{\abs{\lambda_{max}}} =
	\sqrt{\left(1-\frac{1}{4n+4}\right)\left(1-\frac{1}{4n+2}\right)}\sin^{-2}\!\left(\frac{\pi}{N}\left\lfloor\frac{N}{2}\right\rfloor\right) \approx 1-\frac{1}{4n+3}
	\]
	assuming $N>2(n+1)$. %For some sample values of $\alpha_n$ consider Table~\ref{table:alphan}. We observe that the relative increase of $\alpha_n$ is  
	%\begin{table}
	%	\centering
	%	\def\arraystretch{1.5}
	%	\begin{tabular}{c|ccccccccc}	
	%		n & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\ 
	%		\hline 
	%		$\alpha_n$ & 0.676 & 0.727 & 0.906 & 0.931 & 0.946 & 0.955 & 0.962 & 0.967 & 0.971\\ 
	%	\end{tabular} 
	%	\caption{Number of power iterations $n$ versus $\alpha_n$. In the limit $N\to\infty$ the power method underestimates the absolutely largest eigenvalue by the factor $\alpha_n$ for the sum of all normalized eigenvectors of $A$.}
	%	\label{table:alphan}
	%\end{table}
	\subsection{Experiments for the power method}
	As discussed in Section~\ref{sec:matrixfreeimplementation} we need to estimate the largest eigenvalue $\lambda_{max}$ of $A$ for the matrix-free \texttt{expleja} algorihm. However, we can only guarantee convergence if we overestimate $\lambda_{max}$. Therefore we have to include a safety factor $\text{sf}$ by which we multiply the output of the power method. 
	%The question naturally arises, how many power iterations $n$ and which safety factor $\text{sf}\ge 1$ is necessary for convergence.
	%The power method analysis in section~\ref{poweritanalysis} partially answers this question. For large $N$ and for the vector $v=\sum_{j=0}^{N-1}v_j$ the choice 
	%\[ \text{sf} = \frac{1}{\alpha_n}\]
	%is optimal. 
	
	%In the matrix-free case the linear operator $A$ is not explicitly given. In order to compute the operator norm ${\norm A}_2$ we use power iterations to estimate the absolutely largest eigenvalue of $A$. A priory it is not clear how many power iterations $it$ are necessary for a good approximation. 
	\begin{figure}[h]
		\newcommand{\Pe}{Pe=1.0}
		\newcommand{\precision}{single}
		\newcommand{\safe}{sf=1.1}
		
		\centering
		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment1LinOp/1, \precision, \Pe, \safe.pdf}
		\caption{Space dimension $N$ vs costs \texttt{mv} per timestep $s$ for matrix-free single precision \texttt{expleja}. The interpolation degree $m$ is fixed to 100. The Newton interpolation can still terminate early resulting in smaller numbers of $\texttt{mv}/s$. The number of power iterations are denoted by $n$. For this experiment we chose $\operatorname{Pe}=1.0$ and $\text{sf}=1.1$. Results are only shown if they achieve \precision\ precision.} \label{fig:Poweriterations}
	\end{figure}
	We solve the initial value problem~\ref{eq:LinearAdvDif} for a fixed number of substeps $s$ with fixed interpolation degree $m=100$ per substep, while still allowing for an early termination of the interpolation if the error is small enough. See Figure~\ref{fig:Poweriterations}. While the algorithm did not converge for all combinations of $N$, $\text{sf}$ and P\'eclet numbers we tried, we observe that $n=4$ and $\text{sf}=1.1$ is a robust choice independent of $N$ for our initial vector $u_0$.\\
	For all future experiments we choose $\text{sf}=1.1$ and $n=4$ for the matrix-free \texttt{expleja} algorithm. As an additional optimization we save the approximated eigenvector corresponding to $\lambda_{max}$ when integrating over multiple time steps. We use it as the initial vector for the power method for the next time step. Furthermore we terminate the power method early if the relative change of the approximated eigenvalue is smaller than $0.01$. In our experiments the combination of these two optimizations frequently lead to an early termination of the power method. 
	%	We investigate the rate of convergence for the power method given $A$ and an initial vector $v$. Consider $v=\frac{1}{N}\sum_{j=1}^N v_{j}$, where each $v_j$ is the normalized eigenvector corresponding to the eigenvalue $\lambda_j$. Let $N$ be even.
	%	After $n$ power iteration we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of
	%	\begin{align*}
	%	\frac{\norm{A^{n-1}v}_2}{\norm{A^nv}_2}\abs{\lambda_N} &= 
	%	\sqrt{\frac{\sum_{j=1}^{N}{\lambda_j^{2(n-1)}}}{\sum_{j=1}^{N}{\lambda_j^{2n}}}}\abs{\lambda_N} \\
	%	&= \sqrt{\frac{\sum_{j=1}^{N/2}\sin^{4(n-1)}
	%			\left(\frac{\pi j}{N+1}\right)}{\sum_{j=1}^{N/2}\sin^{4n}
	%			\left(\frac{\pi j}{N+1}\right)}}\sin^2\left(\frac{\pi N}{2(N+1)}\right).
	%	\end{align*}
	%	The first equality holds since all eigenvectors are orthogonal. In order to continue our analysis and get some asymptotic bounds we interpret the sum of sine functions as an integral approximated by the trapezoidal rule. We use the nodes $j/(N+1)$ for $j=0,\dots,{N+1}$.
	%	\[
	%	\int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} =
	%	\frac{1}{(N+1)}\left(2\sum_{j=1}^{N}\sin^{4n}\left(\frac{\pi j}{(N+1)}\right) + \frac{1}{2}\right)
	%	+ \mathcal{O}\left(\frac{1}{12(N+1)^2}\right)
	%	\]
	%	Note that the error of the approximation is strictly positive since the second derivative  math.stackexchange\footnote{\url{https://math.stackexchange.com/questions/50447/integration-of-powers-of-the-sin-x}} we can blissfully accept the identity
	%	\[I_n \defneq \int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} = \frac{\Gamma(2n+0.5)}{\sqrt{\pi}\ \Gamma(2n+1)}. \]
	%	In order to simplify our calculations we take the limit of $N$
	%	\[ 
	%	\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} \xrightarrow[]{N\to\infty}
	%	\sqrt{\frac{I_n}{I_{n-1}}}, 
	%	\]
	%	where
	%	
	%	\begin{align*}
	%	\frac{I_n}{I_{n-1}} &= 
	%	\frac{\Gamma(2n - 1)\Gamma(2n + 0.5)}{\Gamma(2n + 1)\Gamma(2n - 1.5)} \\&=   
	%	\frac{(2n - 2)!}{(2n)!}
	%	\frac{\Gamma(2n + 0.5)}{\Gamma(2n - 1.5)}
	%	\frac{\Gamma(2n)}{\Gamma(2n)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n-2)} \\&=
	%	\frac{1}{2n(2n-1)} 
	%	\frac{2^{1-4n}\sqrt\pi}{2^{5-4n}\sqrt\pi}
	%	\frac{\Gamma(4n)}{\Gamma(4n-4)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n)} \\&=
	%	\frac{1}{32n(2n-1)} 
	%	\frac{(4n-1)!}{(4n-5)!}
	%	\frac{(2n-3)!}{(2n-1)!} \\&=
	%	\frac{(4n-1)(4n-2)(4n-3)(4n-4)}{32n(2n-1)^2(2n-2)} \\&=
	%	\frac{(4n-1)(4n-3)}{8n(2n-1)} \\&=
	%	\frac{4n-1}{4n}\frac{4n-3}{4n-2} \\&=
	%	\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)
	%	%%\\&=1 - \frac{1}{2n}\frac{8n-3}{8n-4} 
	%	\end{align*}
	%	For the third equality we applied the duplication formula for the gamma function. All in all we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of 
	%	\[
	%	\lim_{N\to\infty}\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} =
	%	\sqrt{\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)} \approx
	%	1-\frac{1}{4n-1}
	%	\]
	%	at the limit $N\to\infty$. 
	
	\section{Matrix-free Leja based exponential integrators} \label{sec:expint}
	Exponential integrators are a class of numerical integrators which excel at solving stiff differential equations. Unlike most ordinary differential equation (ODE) solvers their construction is based on the variation-of-constants formula. Consider the semilinear initial value problem
	\begin{align}
	\begin{split}
	\partial_tu &= F(u) = Au + g(u) \\ 
	u(0) &= u_0
	\end{split}\label{semilinear}
	\end{align}
	where $A = \partial_uF$ and $g(u) = F(u)-Au$ is the linear and nonlinear part of $F$ respectively. The solution of the ODE is given by the variation-of-constants formula
	\[
	u(t) = e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u(\tau))d\tau.
	\]
	Similar to Runge-Kutta methods we replace the integrand with a polynomial approximation. Unlike Runge-Kutta methods we leave the matrix exponential untouched and only replace $g$. The most well-known Rosenbrock-type exponential integrator, the exponential Rosenbrock-Euler method, can be obtained by using the left hand rule. By replacing $g$ with $g(u_0)$ we get
	\[
	u(t) \approx e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u_0)d\tau = e^{At}u_0 + \varphi_1(tA)g(u_0),
	\]
	where $\varphi_1(z) = \frac{e^z-1}z$. The exponential Rosenbrock-Euler method is of order 2 and is exact for linear problems, i.e. if g(u)=0. We will refer to it as \texttt{exprb2} in Section~\ref{sec:NE}.
	\subsection{Higher order Rosenbrock methods}
	Exponential Rosenbrock methods are a special class of exponential integrators which efficiently solve semi-linear problems \eqref{semilinear}. For a given time step size $\tau$ the numerical solution $u_1$ is given by
	\begin{align}
	\begin{split}
	U_{i} &= e^{c_i \tau A}u_0 + \tau\sum_{j=1}^{i-1}a_{ij}(\tau A)g(U_{j}), \quad \\
	u_{1} &= e^{    \tau A}u_0 + \tau\sum_{i=1}^{s}b_i(\tau A)g(U_{i}),
	\end{split}\label{exprbscheme}
	\end{align}
	where $s\in\mathbb{N}$ and $a_{ij}$, $b_{i}$ are matrix functions. The numerical scheme can be represented as a Butcher tableau
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc}
			$c_1$ &  &  &  & \\
			$c_2$ & $a_{21}(\tau A)$ &  &  & \\
			$\vdots$ & $\vdots$ &  $\ddots$  &  & \\
			$c_s$ & $a_{s1}(\tau A)$ & $\ldots$ & $a_{s,s-1}(\tau A)$  & \\
			\hline
			&$b_1(\tau A)$ & $\ldots$ & $b_{s-1}(\tau A)$ & $b_s(\tau A)$
		\end{tabular}
		.
	\end{table} \noindent The functions $a_{ij}$ and $b_{i}$ are typically given as linear combinations of the $\varphi_k$-functions, which in turn are recursively defined as 
	\[\varphi_{k+1}(z) = \frac{\varphi_k(z)-1}z, \quad \varphi_0(z) = e^z, \quad k\in\mathbb{N}.\]
	For example consider the embedded method
	\begin{table}[H]
		\vspace{-1em}
		\centering
		\renewcommand\arraystretch{1.2}
		\[
		\begin{array}
		{c|ccc}
		0\\
		\frac{1}{2} & \frac{1}{2}\varphi_1(\frac{1}{2}\cdot)\\
		1& 0& \varphi_1\\
		\hline
		\texttt{exprb3} & \varphi_1 - 14\varphi_3 & 16\varphi_3 & -2\varphi_3  \\
		\texttt{exprb4} & \varphi_1 - 14\varphi_3 + 36\varphi_4 & 16\varphi_3 -48\varphi_4 & -2\varphi_3 + 12\varphi_4 
		\end{array},
		\]
		\vspace{-2em}
	\end{table}
	\noindent that is 
	\begin{alignat*}{2}
	U_{1} &= u_0,\\
	U_{2} &= e^{\frac{\tau}{2} A}u_0 &&+ \tau\varphi_1\left(\frac{\tau}{2} A\right)g(U_1) = u_0 + \frac{\tau}{2}\varphi_1\left(\frac{\tau}{2}A\right)F(u_0),\\
	U_{3} &= e^{\tau A}u_0 &&+ \tau\varphi_1(\tau A)g(U_2) = e^{\tau A}F(U_2),\\
	\tilde{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3)(\tau A)g(u_0) + 16\varphi_3(\tau A)g(U_2) - 2\varphi_3(\tau A)g(U_3),\\
	\hat{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3 + 36\varphi_4)(\tau A)g(u_0) + (16\varphi_3 -48\varphi_4)(\tau A)g(U_2) \\
	& &&+ (-2\varphi_3 + 12\varphi_4)(\tau A)g(U_3),
	\end{alignat*}
	where $\tilde{u}_1$ and $\hat{u}_1$ is the numerical solution given by \texttt{exprb3} and \texttt{exprb4} respectively.
	\noindent This scheme is known as $\texttt{exprb43}$~\cite[Example 2.24]{bible}. It uses \texttt{exprb3} as a third-order estimator for its fourth-order method \texttt{exprb4}. Both integrators are well suited for numerical computations since all internal stages can be cheaply computed using the exponential Euler method. \\
	Under the simplifying assumptions
	\[
	\sum_{j=1}^s b_j = \varphi_1, \quad  \sum_{j=1}^s a_{ij} = c_i\varphi_1(c_i\cdot) 
	\]
	for $1\le i\le s$ the scheme \eqref{exprbscheme} can be expressed as 
	\begin{align}
	\begin{split}
	U_i &= u_0 + c_i\tau\varphi_1(c_i \tau A)F(u_0) + \tau\sum_{j=2}^{i-1}a_{ij}(\tau A)D_j, \\
	D_j &= g(U_j) - g(u_0), \quad 2\le j\le s, \\
	u_1 &= u_0 +    \tau\varphi_1(    \tau A)F(u_0) + \tau\sum_{i=2}^{s}     b_i(\tau A)D_i.
	\end{split}\label{eq:Djscheme}
	\end{align}
	The main advantage of this reformulation lies in the fact that the norm of all $D_j$ is expected to be small. This can be exploited by the Leja method by allowing an early termination of the Newton interpolation. \\
	For an efficient implementation of exponential Rosenbrock integrators it is crucial to compute only a single action of a matrix function per stage $U_i$ and for the solution $u_1$. Since the most frequently employed methods depend on linear combinations of $\varphi_k$-functions this can be done using the matrix exponential function.
	
	\subsection{Computing the action of the $\varphi$-functions}
	Exponential integrators rely on the efficient computation of $\varphi_k$-functions. In the matrix case $A\in\mathbb{C}^{N\times N}$ this can be done by slightly expanding $A$, see~\cite[Theorem 2.1]{action}.\\
	Let $V = [V_p\dots V_2, V_1]\in\mathbb{C}^{N\times p}$, $u\in\mathbb{C}^{N\times 1}$, $\tau\in\mathbb{C}$ and
	
	\begin{align*}
	\tilde{A} = 
	\left[ \begin{array}
	{cc}A& V \\0 & J\\
	\end{array}\right],  \quad
	J = 
	\left[ \begin{array}
	{cc}0& I_{p-1} \\0 & 0\\
	\end{array}\right],
	\end{align*}
	where $I_{n}$ is the $n\times n$ identity matrix. Let $e_n$ denote the $n$-th $p\times 1$ unity vector. Then
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_j} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{j}\tau^k\varphi_k(\tau A)V_{p-j+k}, 
	\quad j\in\{1,\dots,p\}. 
	\end{align*}
	In particular for $j=p$ we have
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_p} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{p}\tau^k\varphi_k(\tau A)V_{k}.
	\hphantom{V_{-1}, \quad j\in\{1,\dots,p\}.}
	\end{align*}
	This formulation can be directly applied to each stage in \eqref{eq:Djscheme} assuming $a_{ij}$ and $b_j$ are linear combinations of $\varphi_k$-functions. Therefore for each stage only a single action of an expanded matrix exponential has to be evaluated. In total this has to be done $s$ times for an exponential Rosenbrock method with $s$ stages. \\
	%Many exponential Rosenbrock methods can use this relation to reduce computational costs. For most practical integrators each stage only requires a single action of an expanded matrix exponential has to be evaluated. This is in particular true for the exponential Rosenbrock-Euler methods which can be solved in a single step.
	For a matrix-free implementation of $\tilde A$ given an operator $A$ we can simply compute the action of $\tilde{A}$ as follows
	\begin{align}
	\tilde{A}\colvec{2}{v}{w} = \colvec{2}{Av}{0} + \colvec{2}{Vw}{Jw}, \quad v\in\mathbb{C}^{N\times 1}, w\in\mathbb{C}^{p\times 1}. %\label{eq:tildeAv}
	\end{align}
	The Leja method only relies on matrix-vector multiplications with $\tilde{A}$ and therefore the explicit knowledge of $A$ is not required. To summarize, an efficient matrix-free implementation of exponential Rosenbrock-methods can be achieved using the Leja method. In particular \texttt{exprb3} and \texttt{exprb4} can be evaluated by computing three actions of matrix exponentials. 
	
%	
%	\section{Nonlinear Advection-Diffusion-Reaction Equation}
%	Consider the one-dimensional advection-diffusion-reaction equation
%	\begin{align*}
%	\partial_tu &= \alpha\partial_x(u+\partial_xu) + \beta\partial_x(u^2) + u(u-0.5) \quad \alpha,\beta,\ge 0\\
%	u_0(t) &= e^{-80\cdot(t-0.45)^2}, \quad t\in[0,0.1]
%	\end{align*}
%	\begin{align*}
%	\partial_tu &= \alpha\nabla(u+\nabla u) + \beta(\partial_x + \partial_y)(u^2) + u(u-0.5) ~~\text{and}\\
%	u_0(t) &= e^{-80\cdot(t-0.45)^2} ~~\text{with}~~ \alpha,\beta,\ge 0  ~~\text{and}~~ t\in[0,0.1]
%	\end{align*}
%	on the domain $\Omega = [0,1]$.
%	
	\section{Numerical experiments}\label{sec:NE}
	In this section we will apply matrix-free exponential Rosenbrock integrators to multiple advection-diffusion-reaction equations. All experiments are conducted in Python 3.7~\cite{python} with NumPy 1.18.1~\cite{numpy} and SciPy 1.4.1~\cite{numpy}. We use an AMD Ryzen 7 2700 Processor on a Windows machine.
	Consider $d$-dimensional nonlinear variants of the initial value problem in section~\ref{sec:LinearADe}. Let $\alpha$, $\beta > 0$ and
	\begin{alignat*}
	\partial\partial_tu &= F(u) &&~~\text{with}~~ t\in[0,0.1],\\
	u_0(x) &= e^{-80\cdot(\norm{x}_2^2-0.45)^2} &&~~\text{with}~~ x\in[0,1]^d,
	\end{alignat*}
	with
	\begin{align*}
		F(u) &= 
		\alpha\nabla((u+1)\nabla u) 
		+ \beta\ \vec{1}\!\cdot\!\nabla u^2
		+ u(u-0.5).
	\end{align*}
	For all experiments we assume Dirichlet boundary conditions. We compare the behavior of exponential integrators with other matrix-free ODE solvers.
	
	\paragraph{Crank-Nicolson method}
	We refer to the Crank-Nicolson method of order 2 as \texttt{cn2}.
	In our implementation of \texttt{cn2} we use the Newton-Raphson method to solve the nonlinear system of equations. We treat the Jacobian-vector product $v\mapsto F'(u)v$ as a linear operator. For the resulting system of linear equations we use the SciPy package \texttt{scipy.sparse.linalg.gmres}. For all experiments the relative tolerance is set to $\texttt{tol}/N_\tau$, where $N_\tau$ is the total number of time steps used for solving the ODE. No preconditioner was used for \texttt{gmres}. The Crank-Nicolson method is unconditionally stable and therefore does not have to satisfy the Courant-Friedrichs-Lewy (CFL) conditions for the differential equations we investigate in our experiments.
	
	\paragraph{Explicit Runge-Kutta method}
	We refer to the explicit midpoint method of order 2 as \texttt{rk2} and refer to the classical Runge-Kutta method of order 4 as \texttt{rk4}. No explicit Runge-Kutta method is A-stable. Therefore small time step sizes have to be chosen when solving stiff differential equations. In our experiments this is the case when the considered differential equation is diffusion-dominated.
	
	\paragraph{Exponential Rosenbrock methods}
	We refer to the exponential Rosenbrock methods of order 2, 3 and 4 discussed in~\ref{sec:expint} as \texttt{exprb2}, \texttt{exprb3} and \texttt{exprb4} respectively.
	The action of the matrix exponential is approximated with the matrix-free \texttt{expleja} algorithm. At each time step we compute the optimal interpolation degree and substep parameter $s$ according to \eqref{eq:msmatrixfree}. For \texttt{expleja} we set the relative tolerance per time step to $\texttt{tol}/s$.
	The maximal interpolation degree is set to 100. Note that the total number of matrix-vector multiplication per time step $s$ can still exceed 100 for $s>1$. At each time step we compute the spectral radius using the power method with at most four power iterations.\\
	\\
	Our goal is to investigate the respective computational costs of these methods while achieving a prescribed relative tolerance \texttt{tol}.
%We investigate cost incurred by each integrator in terms of function evaluations \texttt{Feval}, directional derivatives evaluations \texttt{dFeval} and matrix-vector multiplications \texttt{mv}. From now on the cost of evaluating $F$ once will be defined as 1 \texttt{Feval}.
	
	\subsection{Cost analysis}
	%The cost for each integrator mainly depends on the number of times $F$ or its directional derivatives have to be evaluated.
	For our cost analysis we investigate the number of memory operations incurred by each integrator. The main cost per stage for explicit Runge-Kutta methods is given by one function evaluation. Therefore the cost per time step for \texttt{rk2} and \texttt{rk4} amounts to two and four evaluations of $F$ respectively. The cost analysis for \texttt{cn2} and all exponential integrators is much more involved. These methods rely on expensive subroutines with cost predominantly depending on Jacobian-vector products. For each experiment we keep track of the total number of times $F$, its directional derivative or similarly expensive functions have to be computed.
	Let $N^d$ be the number of grid points. We assume each vector or matrix element is stored as a floating point number in double precision. For sparse matrices, we assume each index is stored as an integer. Evaluating $F$ requires loading an $N^d$ array into memory and storing the result. The directional derivative can be computed as a Jacobian-vector product or approximated as a finite difference 
	\[
	F'(u)v \approx \frac{F(u+\epsilon v)-F(u)}{\epsilon} ~~\text{with}~~ \epsilon > 0
	\]
	for $u, v\in\mathbb{R}^{N}$.
	Either way $u$ and $v$ need to be loaded into memory and the result needs to be stored, resulting in $3N^d$ memory operations. In a matrix-free setting all other costs are negligible. If $(u,v)\mapsto F'(u)v$ is implemented as a sparse matrix-vector product additional costs for loading the matrix coefficients have to be considered. Discretizing the diffusive part of $F$ with a second order central difference scheme results in a Jacobian of $F$ with $(2d+1)N$ entries. All other discretizations do not affect this structure. If the matrix is stored in CSR-format an additional $8(2d+1)N^d$ bytes for all entries and $4(2d+2)N^d$ bytes for all indices have to be accessed to compute the directional derivative. For a more elaborate explanation see section~\ref{sec:FDS}.
	In summary
	\begin{enumerate}
		\item evaluating $u \mapsto F(u)$ requires $16N^d$ bytes,
		\item evaluating $(u,v) \mapsto F'(u)v$ requires $24N^d$ bytes in the matrix-free case and
		\item evaluating $(u,v) \mapsto F'(u)v$ requires $(24d+40)N^d$ bytes for the CSR-matrix case
	\end{enumerate}
	to be either read or written in memory. 
	
	\subsection{Discretization of the problem}
	The differential equation can be rewritten using the product rule
	\begin{align*}
		\partial_{t}u = F(u) 
			&= \alpha\nabla((u+1)\nabla u) 
				+ \beta\ \vec{1}\!\cdot\!\nabla u^2
				+ u(u-0.5) \\
			&= \alpha(u+1)\Delta u + \alpha(\nabla u)^2 
				+ \beta\ \vec{1}\!\cdot\!\nabla u^2 + u(u-0.5) 				
%			&= \alpha\nabla(0.5\nabla u^2 + \nabla u) 
%				+ \beta\ \vec{1}\!\cdot\!\nabla u^2 + u(u-0.5) \\
%			&= \alpha\Delta(0.5 u^2 + u) 
%				+ \beta\ \vec{1}\!\cdot\!\nabla u^2 + u(u-0.5).
	\end{align*}
	By considering only the terms depending on $\Delta $ and $\vec{1}\!\cdot\!\nabla $ we get the modified problems
	\begin{align*}
		\partial_{t}u &= \alpha(u+1)\Delta u,
		 %= \alpha\sum_{k=1}^d \partial_{x_k^2}(0.5 u^2 + u), 
		 \\
		\partial_{t}u &= \beta~\vec{1}\!\cdot\!\nabla u^2 .
		 %= \beta\sum_{k=1}^d \partial_{x_k^2} u^2
	\end{align*}
	In order to mitigate numerical instabilities we discretize $\Delta$ with second order centred differences and $\vec{1}\!\cdot\!\nabla$ with the first-order upwind scheme. Both discretizations induce severe restrictions on the maximal time step size for explicit Runge-Kutta schemes. These conditions are known as Courant-Friedrichs-Lewy (CFL) conditions. In our experiments $\abs{u}$ is bounded by 1. Therefore the CFL conditions of the modified problems are given by
	\begin{align} \label{eq:CFL}
		C_{dif} = d\frac{2\alpha\tau}{h^2} \le \frac{1}{2}, \quad C_{adv} = d\frac{\beta\tau}{h} \le 1.
	\end{align}
	This constraint can be seen in Figure~\ref{fig:multi1DNonlinear} and Figure~\ref{fig:multi2DNonlinear} on the second row. 
	
	\begin{figure}[t]
		\newcommand{\dif}{\detokenize{α}=0.1}
		\newcommand{\adv}{\detokenize{β}=0.01}
		\centering
		\includegraphics[width=\columnwidth]{../Figures/Experiment2/multi, \dif, \adv.pdf}
		\caption{One-dimensional matrix-free case. Comparison of different matrix-free integrators given relative tolerances \textit{half} ($\operatorname{tol} = 2^{-10}$) and single \textit{single} ($\operatorname{tol} = 2^{-24}$). We calculated the cost-minimizing time step size for each integrator, as shown in the second row. The top row displays the corresponding cost incurred. The bottom row shows the average number of Jacobian-vector products computed per time step. We chose $\alpha\dif$, $\beta\adv$. The dotted line indicates the largest time step size, where the CFL conditions are not violated.}
		\label{fig:multi1DNonlinear}
	\end{figure}
	
	%As discussed in the previous section the maximal time step size of \texttt{rk2} and \texttt{rk4} is severely limited by the CFL conditions~\ref{eq:CFL}.  while all other integrators are unaffected. 
	
%	\begin{figure}[H]
%		\newcommand{\dif}{\detokenize{α}=0.1}
%		\newcommand{\adv}{\detokenize{β}=1}
%		\centering
%		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment2/3, single, \detokenize{α}=0.1, \detokenize{β}=1.pdf}
%		\caption{This will be merged with Figure~\ref{fig:multi1DNonlinear} to a $3\times 2$ plot.}
%	\end{figure}
	
	
	\subsection{Results} \label{sec:Experiment1dim}
	We consider $\alpha\in\{0.1,0.01\}$, $\beta\in\{1, 0.1, 0.01\}$. For each integrator we calculate the cost-minimizing time step size given $\alpha$, $\beta$ and a relative tolerance $\operatorname{tol}\in \{2^{-10},2^{-24}\}$. This was done by comparing their computational costs for multiple pre-chosen, but fixed time step sizes $\tau$ as follows. For each integrator we subdivided the time domain into $N_\tau$ equidistant parts. The time step taken by each integrators corresponds $\tau = 0.1/N_\tau$. Initially we set $N_\tau=1$. Whenever the computation finished we check if the desired tolerance has been reached and no further cost improvements can be expected. If this is not the case we restart the experiment after replacing $N_\tau$ with a larger integer value close to $1.1N_\tau$. 
	    
	The results for the one-dimensional and two-dimensional advection-diffusion equations are shown in Figure~\ref{fig:multi1DNonlinear} and Figure~\ref{fig:multi2DNonlinear} respectively. 
	These figures show the matrix-free case with $\alpha=0.1$, $\beta=0.1$. Since \texttt{exprb3} was always outperformed by either \texttt{exprb2} or \texttt{exprb4} we excluded from all plots to increase readability. The results for CSR-matrix case, as well as all other choices for $\alpha$ and $\beta$, are shown in Appendix~\ref{sec:1dim} and \ref{sec:2dim}. We discuss the results of the matrix-free case, unless otherwise noted.
	
	\paragraph{Runge-Kutta methods}
	The stiffness of the differential equation increases with $N$ and $\alpha$, leading to higher computational costs for all integrators. For sufficiently large values of $N$ the maximal time step size of \texttt{rk2} and \texttt{rk4} is limited by the CFL conditions~\ref{eq:CFL}. This causes both integrators to have relatively high optimal costs in the \textit{half} precision setting ($\operatorname{tol}=2^{-10}$). In the \textit{single} precision setting ($\operatorname{tol}=2^{-24}$) integrators naturally have to take smaller time step sizes to achieve the necessary relative tolerance. This is more beneficial for Runge-Kutta methods compared to all other integrators. In the CSR-matrix case Runge-Kutta methods outperform all other methods considered, since they do not rely on the expensive computation of Jacobian-vector products. Generally \texttt{rk2} outperforms \texttt{rk4} in every setting we considered. No hyperparameter can be adjusted to further decrease the computational costs of both Runge-Kutta methods, which is not the case for \texttt{cn2}, \texttt{exprb2} and \texttt{exprb4}.
	
	\paragraph{Exponential integrators}
	Exponential integrators are not limited by CFL conditions, allowing \texttt{exprb2} and \texttt{exprb4} to take larger time step sizes. However, larger time step sizes lead to higher costs for the computation of \texttt{expleja}. Despite the cost increase both \texttt{exprb2} and \texttt{exprb4} minimize their respective costs when choosing relatively large time steps. In the half precision setting \texttt{exprb2} outperforms \texttt{exprb4}. This relationship is flipped in the single precision setting; unlike the exponential Euler method \texttt{exprb4} can take large time steps due to its higher order, despite the tolerance constraint. We observe that \texttt{exprb4} is in particular well suited in the single precision setting given a low diffusion coefficient ($\alpha = 0.01$). In the half precision setting \texttt{exprb2} is a robust choice for all $\alpha$ and $\beta$ we considered. We remark that the costs of the exponential integrators can be further decreased by changing the tolerance setting used by \texttt{expleja}. However, the settings we used for \texttt{expleja} match \cite{lejarev} and are comparable to the choice we made for \texttt{cn2}. 
	
	\paragraph{Crank-Nicholson method}
	The Crank-Nicholson method is unconditionally stable and is therefor not restricted by the CFL conditions. In the half precision setting we observe that the Crank-Nicholson method is a suitable choice for diffusion-dominated problems. This is no longer the case in the single precision setting, where it is outperformed by every other integrator. The tolerance setting used by \texttt{cn2} can be overly restrictive. However, in some experiments even conservative, but static choices such as $\texttt{tol}/100$ lead to solutions with insufficient accuracy for all considered time step sizes.
	
	\begin{figure}[t]
	\newcommand{\dif}{\detokenize{α}=0.1}
	\newcommand{\adv}{\detokenize{β}=0.01}
	\centering
	\includegraphics[width=\columnwidth]{../Figures/Experiment_2D/multi, \dif, \adv.pdf}
	\caption{Two-dimensional matrix-free case. Comparison of different matrix-free integrators given relative tolerances \textit{half} ($\operatorname{tol} = 2^{-10}$) and single \textit{single} ($\operatorname{tol} = 2^{-24}$). We calculated the cost-minimizing time step size for each integrator, as shown in the second row. The top row displays the corresponding cost incurred. The bottom row shows the average number of Jacobian-vector products computed per time step. We chose $\alpha\dif$, $\beta\adv$. The dotted line indicates the largest time step size, where the CFL conditions are not violated.}
	\label{fig:multi2DNonlinear}
	\end{figure}
	
	\section{Conclusion}
	We developed a matrix-free variant of the Leja method for the matrix exponential function. All computations relying on the calculation of a matrix norm have been replaced by estimates of the spectral radius. This esimate was in turn approximated by the power method. We applied the matrix-free Leja method to exponential integrators a advection-diffusion equation   Finally we applied the matrix-free Leja method to exponential integrators and compared the result against other matrix-free methods.


\begin{itemize}
	\item TODO: Plots for CSR Matrix case for the Appendix. Spoiler: \texttt{rk2} always wins.
\end{itemize}

\clearpage


\begin{thebibliography}{13}
	\bibitem{rosenbr} M. Caliari, A. Ostermann. Implementation of exponential Rosenbrock-type integrators, Applied Numerical Mathematics 59 (2009), 568-581.
	\bibitem{action} A. Al-Mohy, N. Higham. Computing the action of the matrix exponential, with an application to exponential integrators, SIAM Journal on Scientific Computing 33 (2011), 488-511.
	\bibitem{newt} L. Reichel. Newton interpolation at Leja points, BIT Numerical Mathematics 30 (1990), 332-346.
	\bibitem{advdif} M. Caliari, M. Vianello, L. Bergamaschi. Interpolating discrete advection-diffusion propagators at Leja sequences, Journal of Computational and Applied Mathematics 172 (2004), 79-99.
	\bibitem{lejarev} M. Caliari, P. Kandolf, A. Ostermann, S. Rainer. The Leja method revisited: backward error analysis for the matrix exponential, SIAM Journal on Scientific Computation, Accepted for publication (2016). arXiv:1506.08665.
	\bibitem{bible} M. Hochbruck, A. Ostermann. Exponential integrators, Acta Numerica 19 (2010), 209-286
	\bibitem{polynomialmethods} P. Novati, Polynomial methods for the computation of functions of large unsymmetric matrices, Ph.D. Thesis in Computational Mathematics, University of Trieste, advisor I. Moret (2000).
	\bibitem{newtoninterpolation} L. Reichel, Newton interpolation at Leja points, BIT 30 (2) (1990), 332–346.
	\bibitem{matrixanalysis} R. Horn, C. Johnson, Matrix Analysis, Cambridge University Press (2012).
	\bibitem{trapezoidal} L. N. Trefethen, J.A.C. Weideman, The exponentially convergent trapezoidal rule, SIAM Review 56-3 (2014), 385-458.
	
	\bibitem{python} Python Software Foundation. Python Language Reference, version 3.7. Available at https://www.python.org. Manual at https://docs.python.org/3/. [Online; accessed 2020-02-19]
	\bibitem{numpy} S. v. d. Walt, C. Colbert, G Varoquaux. The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science \& Engineering, 13 (2011), 22-30.
	\bibitem{numpyguide} T. Oliphant. A guide to NumPy, USA: Trelgol Publishing, (2006).
	\bibitem{scipy} P. Virtanen, R. Gommers, T. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. v. d. Walt, M. Brett, J. Wilson, J. Millman, N. Mayorov, A. Nelson, E. Jones, R. Kern, E. Larson, C. Carey, İ. Polat, Y. Feng, E. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. Quintero, C. Harris, A. Archibald, A. Ribeiro, F. Pedregosa, P. v. Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, in press.
	\bibitem{matplotlib} J. Hunter. Matplotlib: A 2D Graphics Environment, Computing in Science \& Engineering, 9, 90-95 (2007).
	\bibitem{pandas} W. McKinney. Data Structures for Statistical Computing in Python, Proceedings of the 9th Python in Science Conference, 51-56 (2010).
\end{thebibliography}
\clearpage

	
\newcommand{\Pe}{Pe=10.0}
\newcommand{\precision}{single}
\newcommand{\safe}{sf=1.1}
\newcommand{\plotwidth}{\columnwidth}

%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/12, \precision, \Pe.pdf}
%		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
%	\end{figure}

\begin{appendices}
	\section{One-dimensional advection-diffusion equation}\label{sec:1dim}
\end{appendices}
\newcommand{\dif}{\detokenize{α}=0.1}
\newcommand{\adv}{\detokenize{β}=0.01}
\renewcommand{\precision}{single}
This shows the result for all the experiments specified in Section~\ref{sec:Experiment1dim}. We consider $\alpha\in\{0.1, 0.01\}$, $\beta\in\{1, 0.1, 0.01\}$.
\newpage
\foreach \DIF in {0.1, 0.01}
\foreach \ADV in {1, 0.1, 0.01}
{
	%Nonlinear 1D  {\precision, $\alpha\dif$, $\beta\adv$}
	\begin{figure}
		\centering
		\includegraphics[width=\plotwidth]{../Figures/Experiment2/multi, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}


%}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%\foreach \PRECISION in {half, single}
%{
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment2/2, \PRECISION, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%{	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment2/3, single, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%\foreach \PRECISION in {half, single}
%{	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment2/4, \PRECISION, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%}
\clearpage
\begin{appendices}
\section{Two-dimensional advection-diffusion equation}\label{sec:2dim}
\end{appendices}

\foreach \DIF in {0.1, 0.01}
\foreach \ADV in {1, 0.1, 0.01}
{
	\begin{figure}[H]
		\centering
		\includegraphics[width=\plotwidth]{../Figures/Experiment_2D/multi, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
%}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%\foreach \PRECISION in {half, single}
%{	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment_2D/2, \PRECISION, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%{		
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment_2D/3, half, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
}
%\foreach \DIF in {0.1, 0.01}
%\foreach \ADV in {1, 0.1, 0.01}
%\foreach \PRECISION in {half, single}
%{		
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment_2D/4, \PRECISION, \detokenize{α}=\DIF, \detokenize{β}=\ADV.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%	
	
	%DESCRIPTION OF THE EXPERIMENT, NUMBER OF POWERITS, SAFETYFACTOR, HOW STABLE IS THE COMPUTATION???
	%First that the matrix $A = A(t)$ changes at every time step and therefore
	
	
	
	


%	\subsection{Fully Linear Case}
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment1/multi, \Pe.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment1/3, \precision, \Pe.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=\plotwidth]{../Figures/Experiment1/4, \precision, \Pe.pdf}
%		%	\caption{A picture of a gull.}
%	\end{figure}
\end{document}
