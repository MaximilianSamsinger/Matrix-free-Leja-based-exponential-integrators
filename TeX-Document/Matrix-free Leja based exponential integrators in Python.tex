% !TeX spellcheck = en_GB
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman,english]{babel}
\usepackage{blindtext}
\usepackage{color}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{subcaption}
\usepackage{upgreek}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{mathdots}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}

%\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\}
%\setlength{\parindent}{0em}

\newcount\colveccount
\newcommand*\colvec[1]{
	\global\colveccount#1
	\begin{bmatrix}
		\colvecnext
	}
	\def\colvecnext#1{
		#1
		\global\advance\colveccount-1
		\ifnum\colveccount>0
		\\
		\expandafter\colvecnext
		\else
	\end{bmatrix}
	\fi
}

\begin{document}
	
	
	\theoremstyle{definition}
	\newtheorem{defn}{Definition}[section]
	\newtheorem{bsp}{Beispiel}[section]
	\newtheorem{satz}{Satz}[section]
	\newtheorem{prop}{Proposition}
	\newtheorem{lem}[satz]{Lemma}
	\newtheorem*{bem}{Bemerkung}
	\newtheorem*{rem}{Remark}
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
	
	%\title{The Leja method in Python}
	%\subtitle{supervised by Peter Kandolf, Alexander Ostermann}
	%\maketitle
	%\author{Maximilian Samsinger}
	\begin{titlepage}
		\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
		\vspace{3cm}
		\begin{center}
			{\Large Master Thesis}
			\vspace{50pt}\\
			\textbf{\Huge Matrix-free Leja based exponential integrators in Python}
			\vspace{40pt}\\
			\textbf{\Large Maximilian Samsinger}\vspace{20pt}\\
			{\large\today}
			\vspace{120pt}\\
			{\Large Supervised by Lukas Einkemmer and\\
				Alexander Ostermann\vspace{10pt}}
		\end{center}
	\end{titlepage}
	\textsf{
		{\hspace{-16pt}\large\textbf{Leopold-Franzens-Universität Innsbruck}}
		\begin{figure}[!htp]
			\begin{flushright}
				\includegraphics[scale=0.1]{./uni_logo}
			\end{flushright}
		\end{figure}\\\\
		{\Large\textbf{Eidesstattliche Erklärung}}\\\\
		Ich erkläre hiermit an Eides statt durch meine eigenhändige Unterschrift, dass ich die\\ vorliegende Arbeit selbständig verfasst und keine anderen als die angegebenen Quellen und\\ Hilfsmittel verwendet habe. Alle Stellen, die wörtlich oder inhaltlich den angegebenen Quellen \\entnommen wurden, sind als solche kenntlich gemacht.\\\\
		Ich erkläre mich mit der Archivierung der vorliegenden Masterarbeit einverstanden.
		\vspace{40pt}\\
		\begin{center}
			\ensuremath{\overline{\mbox{Datum}\hspace{8em}}
				\hspace{10em}
				\overline{\mbox{Unterschrift}\hspace{10em}}
			}
			\thispagestyle{empty}
	\end{center}}
	\pagebreak
	
	
	
	\begin{center}\textbf{\Huge Matrix-free Leja based exponential integrators in Python}\end{center}
	\begin{center}\textbf{Abstract}\end{center}
	\begin{abstract}
		In this thesis we develop an algorithm to approximate the action of a matrix exponential function for linear operators. 
	\end{abstract}
	
	\setcounter{page}{1}
	
	\section{Introduction}
	%%%%%%%%
	%Examples needed
	%%%%%%%%%%
	Consider the action of the matrix exponential function 
	\[e^Av,\quad A\in\mathbb{C}^{N\times N}, v\in\mathbb{C}^N.\] 
	Due to computational constraints it can be difficult or impossible to compute $e^A$ in a first step and then the action $e^Av$ in a separate step. This is especially true in applications where $N>10000$ is common. Furthermore the matrix exponential of a sparse matrix is in general no longer sparse. Therefore it is more feasible to compute the action of the matrix exponential in a single step. This can be done by approximating the matrix exponential with a matrix polynomial $p_m$ of degree $m$ in $A$
	\[e^Av \approx p_m(A)v.\]
	This approach has many advantages. The cost of the computation of $p_m(A)v$ mainly depends on the calculation of $m$ matrix-vector multiplications with $A$. Furthermore the explicit knowledge of $A$ itself is no longer required. $A$ can be replaced by a linear operator, which can be more convenient and save memory.
	
	\section{The Leja method} \label{sec:LejaMethod}
	This section serves as an introduction to the Leja method for approximating the action of the exponential function. We briefly cover the key concepts and definitions in section \ref{sec:LejaInterpolation} and \ref{sec:ApproxMatrixExponential}. For more details and all proofs we refer to \cite{advdif} and \cite{lejarev}. 
	
	\subsection{Leja interpolation} \label{sec:LejaInterpolation}
	Let $K\in\mathbb{C}$ be a compact set in the complex plane and $\xi_0\in K$ be arbitrary. The sequence $(\xi_k)_{k=0}^{\infty}$ recursively defined as
	\[\xi_k = \underset{{\xi\in K}}{\operatorname{arg\,max}}\displaystyle\prod_{j=0}^{k-1}\abs{\xi-\xi_j}\]
	is called a Leja sequence. Due to the maximum principle all elements in the sequence realize their maximum on the border $\partial K$. Typically $\xi_0$ is also chosen on $\partial K$.\\
	For analytical functions $f\!:K\to\mathbb{C}$ the Newton interpolation polynomial $p_m$ with nodes $(\xi_k)_{k=0}^{m}$ has the following beneficial properties. 
	\paragraph{Convergence properties:}
	The sequence $(p_m)_{m=0}^\infty$ converges maximally to $f$. That is, let $(p_m^{*})_{m=0}^\infty$ be the best uniform approximation polynomials for $f$ in $K$. Then
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{K}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{K}^{1/m},\]
	where ${\norm{\cdot}}_K$ is the maximum norm on $K$. Furthermore if $f$ is an entire function, then $(p_m)_{m=0}^\infty$ converges superlinearly to $f$
	\[\limsup\limits_{m \rightarrow \infty}\norm{f-p_m}_{\mathbb{C}}^{1/m} = \limsup\limits_{m \rightarrow \infty}\norm{f-p_m^{*}}_{\mathbb{C}}^{1/m} = 0. \]
	For entire functions $f$ the corresponding matrix polynomials achieves similar superlinear convergence
	\[\limsup\limits_{m \rightarrow \infty}\norm{f(A)v-p_m(A)v}_{2}^{1/m} = 0, \]
	for $A\in\mathbb{C}^{n\times n}, v\in\mathbb{C}^n$.
	
	\paragraph{Early termination:}
	The Newton interpolation polynomial $p_m$ can be constructed iteratively since the corresponding Leja interpolation points $(\xi_k)_{k=0}^{m}$ are defined recursively. Therefore if the approximation $p_n \approx f$ is accurate enough after $n<m$ steps the interpolation can be stopped early to reduce the cost of the interpolation. Note that this is not possible with Chebyshev nodes.
	
	\paragraph{Leja sequence can be stored:}
	For a given $K$ the Leja interpolation nodes only need to be computed once and for all. These values can be stored a priori and loaded once they are needed for the interpolation. If $f$ is fixed the same is also true for the corresponding divided differences. \\
	
	In summary the Leja points offer convergence properties similar to Chebyshev nodes for interpolation, while having computational advantages. All results hold true for the corresponding matrix interpolation polynomials.
	
	\subsection{Approximating the matrix exponential function:} \label{sec:ApproxMatrixExponential}
	Inspired by the previous subsection we try to find a low-cost approximation of the action of the matrix exponential $e^Av$ using Leja interpolation polynomials. From now on, we will fix 
	\[K=[-c,c], ~~ f = e^\cdot  ~~\text{and}~~ \xi_0 = c \]
	for $c>0$. With $L_{m,c}$ we denote the Leja interpolation polynomial on the interval $[-c,c]$ with Leja points $(\xi_j)_{j=0}^{m}$. We use the well-known property of the exponential function
	\[e^Av = (e^{s^{-1}A})^sv, ~~\text{with}~~ s\in\mathbb{N}.\]
	Now we can approximate the action of the matrix exponential in $s$ substeps
	\[v_0\defneq v, ~~ v_{j+1}\defneq L_{m,c}(s^{-1}A)v_j, ~~\text{and}~~ v_s \approx e^Av.\]
	So far we placed no restrictions on $m$, $s$ and $c$. We choose optimal parameters based on the backward-error analysis done in \cite{lejarev}.
	
	\paragraph{Bounding the backward error} For a given matrix $A$ we interpret the Leja interpolation polynomial as the exact solution of a perturbed matrix exponential function
	\[
	L_{m,c}(s^{-1}A)^sv \eqdefn e^{A + \Delta A}v\\
	\]
	Our goal is to bound the backward error 
	\[
	\frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}, 
	\]
	for a given tolerance $\operatorname{tol}$. Furthermore we want to minimize the cost of the interpolation. A priori it is unclear for which values $m$, $s$ and $c$ the inequality is satisfied. The authors of \cite{lejarev} conducted a backward error analysis and chose an approach which puts an upper bound on $c$ depending only on $s$ and $m$. For various tolerances $\operatorname{tol}$ they precomputed values $\theta_m$, see \ref{table:thetam}, which satisfy
	\[
	\text{If}\ \norm{s^{-1}A}\le \theta_m\ \text{and}\ 0 \le c \le \theta_m\ \text{then}\ \frac{\norm{\Delta A}}{\norm{A}} \le \operatorname{tol}.
	\]
	\begin{table}[tbp]
		\begin{tabular}{r|rrrrrrr}
			$m$ &        5 &       10 &       15 &       20 &       25 &       30 &       35 \\\hline
			half & 6.43e-01 & 2.12e+00 & 3.55e+00 & 5.00e+00 & 6.37e+00 & 7.51e+00 & 8.91e+00 \\
			single & 9.62e-02 & 8.33e-01 & 1.96e+00 & 3.26e+00 & 4.69e+00 & 5.96e+00 & 7.44e+00 \\
			double & 1.74e-03 & 1.14e-01 & 5.31e-01 & 1.23e+00 & 2.16e+00 & 3.18e+00 & 4.34e+00 \\
			\\
			$m$ &       40 &       45 &       50 &       55 &       60 &       65 &       70 \\\hline
			half & 1.00e+01 & 1.10e+01 & 1.23e+01 & 1.35e+01 & 1.48e+01 & 1.59e+01 & 1.71e+01 \\
			single & 8.71e+00 & 1.00e+01 & 1.15e+01 & 1.27e+01 & 1.40e+01 & 1.52e+01 & 1.64e+01 \\
			double & 5.48e+00 & 6.67e+00 & 7.99e+00 & 9.24e+00 & 1.06e+01 & 1.18e+01 & 1.32e+01 \\
			\\
			$m$ &       75 &       80 &       85 &       90 &       95 &      100 \\ \hline
			half & 1.84e+01 & 1.94e+01 & 2.07e+01 & 2.20e+01 & 2.30e+01 & 2.42e+01 \\
			single & 1.76e+01 & 1.87e+01 & 1.99e+01 & 2.12e+01 & 2.23e+01 & 2.35e+01 \\
			double & 1.46e+01 & 1.58e+01 & 1.71e+01 & 1.86e+01 & 1.99e+01 & 2.13e+01
		\end{tabular}
		\caption{Samples of the precomputed values $\theta_m$. The backward error of the Leja interpolation is bounded if $c\le\theta_m$, where $[-c,c]$ is the interpolation interval and $m$ the interpolation degree. Half, single and double correspond to the tolerances $2^{-10}$, $2^{-24}$ and $2^{-53}$ respectively \cite[Table 1]{lejarev}.}
		\label{table:thetam}
	\end{table}	
	For our purposes it is important to note that the optimal choice for $c$ is given by $c=\rho(s^{-1}A)$, where $\rho(A)$ is the spectral radius of $A$. However, computing $\rho(A)$ introduces additional costs for the algorithms proposed in \cite{lejarev}. Our matrix-free implementation relies on the computations of the spectral radius, but it does not need to compute the operator norm $\norm{A}$, see section \ref{matrixfreeimplementation}.
	
	\paragraph{Choosing cost-minimizing parameters}
	The cost of the Leja interpolation mainly depends on the the number of matrix-vector products
	\[
	C_{m} = sm. 
	\]
	In order to minimize the costs of the interpolation $C_m$ we select the smallest $m$ for any given $s$ such that
	\[
	\norm{s^{-1}A} \le \theta_m
	\]
	is satisfied. This leads to the optimal choice for $m$ and $s$ 
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\norm{A}}{\theta_m}}\right\rceil.
	\end{split} \label{eq:ms}
	\end{align}
	In our algorithm we set $m_{\operatorname{max}} = 100$ in order to avoid over- and underflow errors.
	
	\paragraph{Shifting the matrix}
	The cost of the interpolation can be decreased by employing a shift $\mu\in\mathbb{C}$. Let $I$ be the identity matrix. We replace the matrix $A$ with $A-\mu I$ for all computations. If the shifted matrix $A-\mu I$ satisfies $\norm{A -\mu I} < \norm A$ then the cost $C_{m_*}$ of the interpolation decreases.
	We compensate for the shift by multiplying with $e^\mu$ since
	\[
	e^{A} = e^{\mu}e^{A-\mu I}.
	\]
	A well-chosen shift centers the eigenvalues of $A-\mu I$ around $0$. Such a shift can be found by using Gerschgorin's circle theorem. This is, however, not possible in the matrix-free case.
	
	\section{Matrix-free implementation}\label{matrixfreeimplementation}
	In this section we introduce a matrix-free version of the Leja method for the action of a matrix exponential. After a brief motivation we discuss the main advantages and disadvantages of such an approach. \\
	Modern CPUs can utilize a high-speed memory type called cache, which reduces the time for the processor to access data from the main memory. At the time of this writing high-end consumer CPU cache sizes are in the order of megabytes. Consider finite-difference schemes of the form
	\[
	v_k = \sum_{j=-n}^{n} a_{j+k}v_{j+k}
	\]
	for $v\in\mathbb{C}^N$, $n\in\mathbb{N}$ and periodic boundaries i.e. $a_{l+N} = a_{l}$ and $v_{l+N}=v_{l}$ for all $l\in\mathbb{Z}$. 
	This finite difference scheme can be written as an matrix-vector multiplication with an $(2n+1)$-diagonal matrix. Even if the matrix is stored in a sparse format it is still necessary to save $N(2n+1)$ non-zero elements. In the case of two-dimensional five-point stencil method on a rectangular grid we have $n=2$ and $N=N_{x}N_{y}$, where $N_{x}$ and $N_{y}$ is the number of grid points on the $x$- and $y$-axis respectively. This leads to a storage demand of $20N_xN_y$ bytes when saving matrix entries as float data. This means that we surpass $1$ megabyte when we choose $N_x,N_y > 250$, which might cause cache misses. This is especially true for L1 and L2 caches. This problem is further magnified for stencils on three-dimensional domains. \\	
	We avoid these difficulties with matrix-free methods, meaning that matrix representations are not explicitly given. Instead we consider linear operators of the form
	\begin{align*}
	\begin{split}
	A\!:\mathbb{C}^{N}&\to\mathbb{C}^{N}\\
	v&\mapsto A(v)
	\end{split}.
	\end{align*}
	Linear operators can be more convenient to work with in applications where the matrix representation is not easily available. They can also be more memory efficient. For the finite difference schemes we discussed at the beginning of this section we only need to store $(2n+1)$ values independent of $N$. This serves as a motivation to propose an alternative for the classical Leja method we discussed in section \ref{sec:LejaMethod}.
	\paragraph{Matrix-free Leja method}
	For the most part it is unproblematic to use linear operators for the Leja method instead of matrices, since the Newton interpolation only relies on computation of matrix-vector products. However, difficulties arise when the interpolation parameters need to be determined. Without the matrix representation it can be expensive to compute the operator norm $\norm A$ in \eqref{eq:ms}. We will circumvent this problem by replacing $\norm{A}$ with the spectral radius $\rho(A)$. 
	
	The backward error analysis in \cite{lejarev} holds true for every operator norm.
	We use a well-known result from the matrix analysis literature \cite[Lemma 5.6.10.]{matrixanalysis}. For every $A$ and for every $\varepsilon>0$ exists an induced operator norm $\norm{\cdot}_{A,\varepsilon}$ such that
	\[
	\rho(A) \le \norm{A}_{A,\varepsilon} \le \rho(A) + \varepsilon. 
	\]
	The first inequality holds true for every operator norm. We choose $\varepsilon$ small enough, such that
	\[
	\norm{s^{-1}A}_{A,\varepsilon} \le \underset{\rho(s^{-1}A) < \theta_m}{\min}\theta_m.
	\]
	For this choice of $\norm{\cdot}_{A,\varepsilon}$ the cost-minimizing parameters are given by
	\begin{align}
	\begin{split}
	m_* = \underset{2\le m\le m_{\operatorname{max}}}{\operatorname{arg\ min}}  \left\{{\left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil}m\right\} ~~\text{and}~~
	s_* =  \left\lceil{\frac{\rho(A)}{\theta_m}}\right\rceil.
	\end{split}\label{eq:msmatrixfree}
	\end{align}
	The explicit knowledge of $\norm{\cdot}_{A,\varepsilon}$ is no longer required. Additionally we can choose $c=\rho(A)$ without introducing additional costs, since we have to compute $\rho(A)$ to determine $m_*$ and $s_*$. 
	For positive and negative semi-definite operators $A$ we can choose the shift $\mu = -\rho(A)/2$ and $\mu = \rho(A)/2$ respectively. This shift works particularly well if the absolutely smallest eigenvalue of $A$ is close to $0$.\\
	This approach has some drawbacks though. While we are able to bound the backward error
	\[
	\frac{\norm{\Delta A}_{A,\varepsilon}}{\norm{A}_{A,\varepsilon}} \le \operatorname{tol}  
	\]
	we can no longer specify in which norm this error has to be bound. Furthermore, it is hard to find a good shift $\mu$ for non-semi-definite operators.
	\paragraph{Power method}
	The spectral radius $\rho(A)$ can be cheaply approximated using the power method. 
	%For each $j\in\{1\dots N\}$ let $(\lambda_j,v_j)$ be an eigenpair of $A$. 
	%Furthermore let all eigenvectors be normalized and all eigenvalues be sorted in descending order with respect to their modulus. 
	Given an initial vector $b_0\in\mathbb{C}^N$ the $n$-th iteration of the power method is given by
	\[
		b_{n+1} = \frac{Ab_n}{\norm{Ab_n}}.
	\]
	Let $\lambda_{1}$ be a maximal eigenvalue with respect to the modulus. Furthermore let $\lambda_2$ be a maximal eigenvalue satisfying $\abs{\lambda_{1}}\neq\abs{\lambda_2}$. 
	Both eigenvalues are not necessarily unique.
	For each iteration we get an approximation to the spectrum of $A$
	\[ 
	\norm{Ab_{n}} = \rho(A) + \mathcal{O}\left(\left\lvert\frac{\lambda_2}{\lambda_1}\right\rvert^{n+1}\right).
	\]  
	This can be used to compute \eqref{eq:msmatrixfree}. However, the power method underestimates the spectrum of $A$, which might cause the Leja method to not converge. Therefore we have to multiply the estimate with a safety factor. The convergence speed depends on the eigenvalues of $A$. This will be further analysed in section \ref{sec:LinearADe}.
	From now on we denote the Leja method for the matrix exponential function as \texttt{expleja}. Depending on the chosen tolerance, see Table \ref{table:thetam}, we will refer to the algorithm as half, single or double precision \texttt{expleja} respectively.
	
	\section{Linear advection-diffusion equation}\label{sec:LinearADe}
	In this section we consider a simple initial value problem which serves as a test-bed for future experiments. We also want to examine the power method, an algorithm to (under)-estimate the absolutely largest eigenvalue of an operator.\\
	Consider the one-dimensional advection-diffusion equation
	\begin{align}
	\begin{split}
	\partial_tu &= a\partial_{xx}u + b\partial_xu ~~\text{with}~~ a,b\ge 0 ~~\text{and}\\
	u_0(t) &= e^{-80\cdot(t-0,45)^2}\hphantom{1} ~~\text{with}~~ t\in[0,0.1]
	\end{split}\label{eq:LinearAdvDif}
	\end{align}
	on the domain $\Omega = [0,1]$. For a fixed $N\in\mathbb N$ we approximate the diffusive part of the differential equation with second-order central differences on an equidistant grid with grid size $h = \frac{1}{N-1}$ and grid points $x_k = kh$, $k=0\dots,N-1$
	\[\partial_{xx}u(x_k) = \frac{u(x_{k+1}) - 2u(x_k) + u(x_{k-1})}{{h}^2} + \mathcal{O}({h}^2).\]
	In order to avoid numerical instabilities we discretize the advective part with forward differences, similar to the upwind scheme
	\[\partial_{x}u(x_k) = \frac{u(x_{k+1}) - u(x_k)}{h} + \mathcal{O}(h).\]
	The resulting system of ordinary differential equation is given by
	\begin{align*}
	\partial_tu &= Au.
	\end{align*}
	In order to measure the relative strength of advection compared to diffusion we employ the P\'eclet number $\operatorname{Pe} = \frac{b}{a}$.
	\begin{figure}[ht]
		\newcommand{\precision}{single}
		\newcommand{\Pe}{Pe=10.0}
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
		\caption{Approximation of $e^{0.1A}u_0$ using single precision \texttt{expleja} for a fixed interpolation degree $m=100$ and varying number of substeps $s$. The relative error is measured in the Eucledian norm. The reference solution was computed using the double precision \texttt{expleja} algorithm.}
		\label{fig:Experiment1}
	\end{figure} 
	\noindent The solution of the differential equation is given by $e^{0.1A}u_0$, which can be approximated using the Leja method, as shown in Figure \ref{fig:Experiment1}. For the matrix-free case we need to compute the spectral radius of $A$, which can be done using the power method.
	
	
	\subsection{Analysis of the power method}\label{poweritanalysis}
	\begin{figure}[t]
		\newcommand{\boundary}{periodic}
		\centering
		\includegraphics[width=1.\columnwidth]{{../Figures/Spectrum/\boundary}.pdf}
		\caption{The spectrum of $A$. We assume \boundary\ boundary conditions.}
		\label{fig:spectrum}
	\end{figure}
	%All eigenvalues of the discretized one-dimensional using the upwind scheme are -1/h.   
	%	The eigenvalues of discretized one-dimensional Laplace operator $A_{Dif}\in\mathbb{R}^{N\times N}$ on the interval $[0,1]$ with periodic boundary conditions are given by
	%\[
	%\lambda_j =
	%\begin{cases*}
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi (j-1)}{2(N+1)}\right),\quad\text{if j is odd}\\
	%-\frac{4}{h^2} \sin^2\left(\frac{\pi j}{2(N+1)}\right),\quad\text{if j is even}
	%\end{cases*} 
	%,\quad j=1,\dots, N.
	%\]
	We investigate the rate of convergence of the power method to the largest eigenvalue (with respect to the modulus) $\lambda_{max}$ of $A$.
	For our analysis we assume periodic boundary conditions
	\[ A = \frac{a}{h^2}
	\begin{bmatrix}
	-2     & 1  & 0      & \cdots & 0  & 1      \\
	1      & -2 & 1      &        &    & 0      \\
	0      & 1  & \ddots & \ddots &    & \vdots \\
	\vdots &    & \ddots & \ddots & 1  & 0      \\
	0      &    &        & 1      & -2 & 1      \\
	1      & 0  & \cdots & 0      & 1  & -2
	\end{bmatrix}
	+\frac{b}{h}
	\begin{bmatrix}
	-1     & 1  & 0      & \cdots & 0  & 1      \\
	0      & -1 & 1      &        &    & 0      \\
	\vdots & 0  & \ddots & \ddots &    & \vdots \\
	\vdots &    & \ddots & \ddots & 1  & 0      \\
	0      &    &        & 0      & -1 & 1      \\
	1      & 0  & \cdots & \cdots & 0  & -1
	\end{bmatrix}.
	\]
	Consider the discrete Fourier basis
	\[
	v_j = \frac{1}{\sqrt N}\colvec{4}{e^{i\frac{2\pi}{N}j 0}}{e^{i\frac{2\pi}{N}j 1}}{\vdots}{e^{i\frac{2\pi}{N}j(N-1)}}, \quad j\in{0\dots N-1}.
	\]
	Each $v_j$ is an eigenvector of $A$
	\[
	Av_j = \lambda_jv_j, 
	\]
	where the eigenvalues $\lambda_j$ are given by
	\begin{align*}
	\lambda_j &= \frac{a}{h^2}\left(e^{i\frac{2\pi}{N}j} - 2 + e^{-i\frac{2\pi}{N}j}\right)-\frac{b}{h}\left(e^{i\frac{2\pi}{N}j}-1\right) \\ 
	&=-\left(\frac{4a}{h^2}-\frac{2b}{h}\right) \sin^2\!\left(\frac{\pi j}{N}\right) + i\frac{b}{h}\sin\!\left(\frac{2\pi j}{N}\right),
	\end{align*}
	see Figure \ref{fig:spectrum}.
	From now on we study the behaviour of $v = \frac{1}{N}\sum_{j=0}^{N-1} v_j$, the normalized sum of all eigenvectors. Let $n\in\mathbb{N}$ be the number of power iterations. We begin our analysis with the following auxiliary calculation
	\begin{align*}
	\norm{A^nv}_2
	&= \sqrt{\frac{1}{N}\sum_{j=0}^{N-1}{\abs{\lambda_j}^{2n}}}
	= \frac{(4a)^n}{h^{2n}}\sqrt{I_{N,n}} ,
	\end{align*}
	where
	\begin{align*}
	I_{N,n} &= \frac{1}{N}\sum_{j=0}^{N-1}\left(\left(1-\frac{hb}{2a}\right)^2 \sin^4\!\left(\frac{\pi j}{N}\right) + \frac{hb}{4a}\sin^2\!\left(\frac{2\pi j}{N}\right)\right)^n.
	\end{align*}
	%&\xrightarrow[]{N\to\infty} (4a)^{2n} \int_{0}^{1} \sin^{4n}(\pi x)dx = \\
	%&= (4a)^{2n} \frac{2}{\pi} \int_{0}^{\frac\pi 2} \sin^{4n}(x)dx \\
	%&= (4a)^{2n} \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5),
	%where $\operatorname{B}$ is the beta function. 
	The first equality holds since all eigenvectors are orthogonal. %When the limit is taken all terms depending on $h$ vanish. The remaining sum is a left Riemann sum of the integrable function $x\to (4a)^{2n}\sin^{4n}(\pi x)$. 
	We underestimate the largest eigenvalue $\lambda_{max}$ by a factor of
	\begin{align*}
	\frac{\norm{A^{n+1}v}_2}{\norm{A^nv}_2}\frac{1}{\abs{\lambda_{max}}}
	= \frac{\frac{h^{2n+2}}{(4a)^{n+1}}\norm{A^{n+1}v}_2}{\frac{h^{2n}}{(4a)^n}\norm{A^nv}_2}\frac{4a}{\abs{h^2\lambda_{max}}} = \sqrt{\frac{I_{N,n+1}}{I_{N,n}}}\frac{4a}{h^2\abs{\lambda_{max}}}.
	\end{align*}
	using the power method. For large $N$ we have $\frac{4a}{\abs{h^2\lambda_{max}}}\approx 1$.  
	% $\alpha_n$, see Table \ref{table:alphan}, where $\alpha_n$ is given by
	In order to continue our analysis and get some asymptotic bounds we interpret the $n$-th root the summands of $I_{N,n}$ as a function of $x$
	\[
	S_{N}(x) = \left(1-\frac{hb}{2a}\right)^{2} \sin^4\!\left(\pi x\right) + \frac{hb}{4a}\sin^2\!\left(2\pi x\right).
	\]
	In the limit $S_N(\cdot)$ converges to $\sin^4(\pi\cdot)$. Furthermore we have uniform convergence since for all for $x\in[0,1]$ 
	\begin{align*}
	\abs{S_N(x) - \sin^4(\pi x)} =
	\left\lvert\frac{hb}{2a}\left(-2 + \frac{hb}{2a}\right) \sin^4\!\left(\pi x\right) + \frac{hb}{4a}\sin^2\!\left(2\pi x\right)\right\rvert \le \frac{hb}{a}\sin^4\!\left(\pi x\right) \le \frac{hb}{a}, 
	\end{align*}
	if $N$ is sufficiently large. Therefore, if we keep $n$ fixed, we can take the limit
	\[
	\lim_{N\to\infty} I_{N,n} = \lim_{N\to\infty} \frac{1}{N}\sum_{j=0}^{N-1}\left(S_N(x_j)\right)^n = \int_{0}^{1} \sin^{4n}(\pi x)dx = \frac{1}{\pi} \operatorname{B}(2n+0.5,0.5),
	\]
	\noindent where $\operatorname{B}$ is the beta function. Finally we can establish an asymptotic bound
	\begin{align*}
	\lim_{N\to\infty} \frac{I_{N,n+1}}{I_{N,n}} &=   
	\frac{\operatorname{B}(2n+2.5,0.5)}{\operatorname{B}(2n+0.5,0.5)} \\&= 
	\frac{\Gamma(2n + 1)\Gamma(2n + 2.5)}{\Gamma(2n + 3)\Gamma(2n + 0.5)} \\&= 
	\frac{(2n)!}{(2n+2)!}
	\frac{\Gamma(2n + 2.5)}{\Gamma(2n + 0.5)}\\&=
	\frac{(2n)!}{(2n+2)!}
	\frac{2^{4n}\sqrt\pi}{2^{4n+4}\sqrt\pi}
	\frac{(4n+4)!}{(4n)!}
	\frac{(2n)!}{(2n+2)!} \\&=
	\frac{(4n+4)(4n+3)(4n+2)(4n+1)}{16(2n+2)^2(2n+1)^2} \\&=
	\frac{(4n+3)(4n+1)}{(4n+4)(4n+2)}\\&=
	\left(1-\frac{1}{4n+4}\right)\left(1-\frac{1}{4n+2}\right)
	\end{align*}
	For the fourth equality we applied the duplication formula for the gamma function. All in all we underestimate the largest eigenvalue $\lambda_{max}$ by a factor of 
	\[
	\lim_{N\to\infty}\frac{\norm{A^{n+1}v}_2}{\norm{A^{n}v}_2}\frac{1}{\abs{\lambda_{max}}} =
	\sqrt{\left(1+\frac{1}{4n+4}\right)\left(1-\frac{3}{4n+2}\right)} \approx 1-\frac{1}{4n+3}
	\]
	at the limit $N\to\infty$. %For some sample values of $\alpha_n$ consider Table \ref{table:alphan}. We observe that the relative increase of $\alpha_n$ is  
	%\begin{table}
	%	\centering
	%	\def\arraystretch{1.5}
	%	\begin{tabular}{c|ccccccccc}	
	%		n & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\ 
	%		\hline 
	%		$\alpha_n$ & 0.676 & 0.727 & 0.906 & 0.931 & 0.946 & 0.955 & 0.962 & 0.967 & 0.971\\ 
	%	\end{tabular} 
	%	\caption{Number of power iterations $n$ versus $\alpha_n$. In the limit $N\to\infty$ the power method underestimates the absolutely largest eigenvalue by the factor $\alpha_n$ for the sum of all normalized eigenvectors of $A$.}
	%	\label{table:alphan}
	%\end{table}
	\subsection{Experiments for the power method}
	As discussed in section \ref{matrixfreeimplementation} we need to estimate the largest eigenvalue $\lambda_{max}$ of $A$ for the matrix-free \texttt{expleja} algorihm. However, we can only guarantee convergence if we overestimate $\lambda_{max}$. Therefore we have to include a safety factor $\text{sf}$ by which we multiply the output of the power method. 
	%The question naturally arises, how many power iterations $n$ and which safety factor $\text{sf}\ge 1$ is necessary for convergence.
	%The power method analysis in section \ref{poweritanalysis} partially answers this question. For large $N$ and for the vector $v=\sum_{j=0}^{N-1}v_j$ the choice 
	%\[ \text{sf} = \frac{1}{\alpha_n}\]
	%is optimal. 
	
	%In the matrix-free case the linear operator $A$ is not explicitly given. In order to compute the operator norm ${\norm A}_2$ we use power iterations to estimate the absolutely largest eigenvalue of $A$. A priory it is not clear how many power iterations $it$ are necessary for a good approximation. 
	\begin{figure}[h]
		\newcommand{\Pe}{Pe=1.0}
		\newcommand{\precision}{single}
		\newcommand{\safe}{sf=1.1}
		
		\centering
		\includegraphics[width=0.9\columnwidth]{../Figures/Experiment1LinOp/1, \precision, \Pe, \safe.pdf}
		\caption{Space dimension $N$ vs costs \texttt{mv} per timestep $s$ for matrix-free single precision \texttt{expleja}. The interpolation degree $m$ is fixed to 100. The Newton interpolation can still terminate early resulting in smaller numbers of $\texttt{mv}/s$. The number of power iterations are denoted by $n$. For this experiment we chose $\operatorname{Pe}=1.0$ and $\text{sf}=1.1$. Results are only shown if they achieve \precision\ precision.} \label{fig:Poweriterations}
	\end{figure}
	We solve the initial value problem \ref{eq:LinearAdvDif} for a fixed number of substeps $s$ with fixed interpolation degree $m=100$ per substep, while still allowing for an early termination of the interpolation if the error is small enough. See Figure \ref{fig:Poweriterations}. While the algorithm did not converge for all combinations of $N$, $\text{sf}$ and P\'eclet numbers we tried, we observe that $n=4$ and $\text{sf}=1.1$ is a robust choice independent of $N$ for our initial vector $u_0$.\\
	For all future experiments we choose $\text{sf}=1.1$ and $n=4$ for the matrix-free \texttt{expleja} algorithm. As an additional optimization we save the approximated eigenvector corresponding to $\lambda_{max}$ when integrating over multiple time steps. We use it as the initial vector for the power method for the next time step. Furthermore we terminate the power method early if the relative change of the approximated eigenvalue is smaller than $0.01$. 
	%	We investigate the rate of convergence for the power method given $A$ and an initial vector $v$. Consider $v=\frac{1}{N}\sum_{j=1}^N v_{j}$, where each $v_j$ is the normalized eigenvector corresponding to the eigenvalue $\lambda_j$. Let $N$ be even.
	%	After $n$ power iteration we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of
	%	\begin{align*}
	%	\frac{\norm{A^{n-1}v}_2}{\norm{A^nv}_2}\abs{\lambda_N} &= 
	%	\sqrt{\frac{\sum_{j=1}^{N}{\lambda_j^{2(n-1)}}}{\sum_{j=1}^{N}{\lambda_j^{2n}}}}\abs{\lambda_N} \\
	%	&= \sqrt{\frac{\sum_{j=1}^{N/2}\sin^{4(n-1)}
	%			\left(\frac{\pi j}{N+1}\right)}{\sum_{j=1}^{N/2}\sin^{4n}
	%			\left(\frac{\pi j}{N+1}\right)}}\sin^2\left(\frac{\pi N}{2(N+1)}\right).
	%	\end{align*}
	%	The first equality holds since all eigenvectors are orthogonal. In order to continue our analysis and get some asymptotic bounds we interpret the sum of sine functions as an integral approximated by the trapezoidal rule. We use the nodes $j/(N+1)$ for $j=0,\dots,{N+1}$.
	%	\[
	%	\int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} =
	%	\frac{1}{(N+1)}\left(2\sum_{j=1}^{N}\sin^{4n}\left(\frac{\pi j}{(N+1)}\right) + \frac{1}{2}\right)
	%	+ \mathcal{O}\left(\frac{1}{12(N+1)^2}\right)
	%	\]
	%	Note that the error of the approximation is strictly positive since the second derivative  math.stackexchange\footnote{\url{https://math.stackexchange.com/questions/50447/integration-of-powers-of-the-sin-x}} we can blissfully accept the identity
	%	\[I_n \defneq \int_{0}^{1}{\sin^{4n}\left(\frac{\pi x}{2}\right)} = \frac{\Gamma(2n+0.5)}{\sqrt{\pi}\ \Gamma(2n+1)}. \]
	%	In order to simplify our calculations we take the limit of $N$
	%	\[ 
	%	\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} \xrightarrow[]{N\to\infty}
	%	\sqrt{\frac{I_n}{I_{n-1}}}, 
	%	\]
	%	where
	%	
	%	\begin{align*}
	%	\frac{I_n}{I_{n-1}} &= 
	%	\frac{\Gamma(2n - 1)\Gamma(2n + 0.5)}{\Gamma(2n + 1)\Gamma(2n - 1.5)} \\&=   
	%	\frac{(2n - 2)!}{(2n)!}
	%	\frac{\Gamma(2n + 0.5)}{\Gamma(2n - 1.5)}
	%	\frac{\Gamma(2n)}{\Gamma(2n)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n-2)} \\&=
	%	\frac{1}{2n(2n-1)} 
	%	\frac{2^{1-4n}\sqrt\pi}{2^{5-4n}\sqrt\pi}
	%	\frac{\Gamma(4n)}{\Gamma(4n-4)}
	%	\frac{\Gamma(2n-2)}{\Gamma(2n)} \\&=
	%	\frac{1}{32n(2n-1)} 
	%	\frac{(4n-1)!}{(4n-5)!}
	%	\frac{(2n-3)!}{(2n-1)!} \\&=
	%	\frac{(4n-1)(4n-2)(4n-3)(4n-4)}{32n(2n-1)^2(2n-2)} \\&=
	%	\frac{(4n-1)(4n-3)}{8n(2n-1)} \\&=
	%	\frac{4n-1}{4n}\frac{4n-3}{4n-2} \\&=
	%	\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)
	%	%%\\&=1 - \frac{1}{2n}\frac{8n-3}{8n-4} 
	%	\end{align*}
	%	For the third equality we applied the duplication formula for the gamma function. All in all we underestimate the absolutely largest eigenvalue $\lambda_N$ by a factor of 
	%	\[
	%	\lim_{N\to\infty}\frac{\norm{A^{n-1}v}}{\norm{A^{n}v}}\abs{\lambda_N} =
	%	\sqrt{\left(1-\frac{1}{4n}\right)\left(1-\frac{1}{4n-2}\right)} \approx
	%	1-\frac{1}{4n-1}
	%	\]
	%	at the limit $N\to\infty$. 
	
	\section{Matrix-free Leja based exponential integrators} \label{expint}
	Exponential integrators are a class of numerical integrators which excel at solving stiff differential equations. Unlike most numerical ordinary differential equation (ODE) solvers their construction is based on the variation-of-constants formula. Consider the semilinear initial value problem
	\begin{align}
	\begin{split}
	\partial_tu &= F(u) = Au + g(u) \\ 
	u(0) &= u_0
	\end{split}\label{semilinear}
	\end{align}
	where $A = \partial_uF$ and $g(u) = F(u)-Au$ is the linear and nonlinear part of $F$ respectively. The solution of the ODE is given by the variation-of-constants formula
	\[
	u(t) = e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u(\tau))d\tau.
	\]
	Similar to Runge-Kutta methods we replace the integrand with a polynomial approximation. Unlike Runge-Kutta methods we leave the matrix exponential untouched and only replace $g$. The most well-known Rosenbrock-type exponential integrator, the exponential Rosenbrock-Euler method, can be obtained by using the left hand rule. By replacing $g$ with $g(u_0)$ we get
	\[
	u(t) \approx e^{At}u_0 + \int_{0}^{t}e^{(t-\tau)A}g(u_0)d\tau = e^{At}u_0 + \varphi_1(tA)g(u_0),
	\]
	where $\varphi_1(z) = \frac{e^z-1}z$. The exponential Rosenbrock-Euler method is of order 2 and is exact for linear problems, i.e. if g(u)=0. We will refer to it as \texttt{exprb2} in section \ref{sec:NE}.
	\subsection{Higher order Rosenbrock methods}
	Exponential Rosenbrock methods are a special class of exponential integrators which efficiently solve semi-linear problems \eqref{semilinear}. For a given time step size $\tau$ the numerical solution $u_1$ is given by
	\begin{align}
	\begin{split}
	U_{i} &= e^{c_i \tau A}u_0 + \tau\sum_{j=1}^{i-1}a_{ij}(\tau A)g(U_{j}), \quad \\
	u_{1} &= e^{    \tau A}u_0 + \tau\sum_{i=1}^{s}b_i(\tau A)g(U_{i}),
	\end{split}\label{exprbscheme}
	\end{align}
	where $s\in\mathbb{N}$ and $a_{ij}$, $b_{i}$ are matrix functions. The numerical scheme can be represented as a Butcher tableau
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc}
			$c_1$ &  &  &  & \\
			$c_2$ & $a_{21}(\tau A)$ &  &  & \\
			$\vdots$ & $\vdots$ &  $\ddots$  &  & \\
			$c_s$ & $a_{s1}(\tau A)$ & $\ldots$ & $a_{s,s-1}(\tau A)$  & \\
			\hline
			&$b_1(\tau A)$ & $\ldots$ & $b_{s-1}(\tau A)$ & $b_s(\tau A)$
		\end{tabular}
		.
	\end{table} \noindent The functions $a_{ij}$ and $b_{i}$ are typically given as linear combinations of the $\varphi_k$-functions, which in turn are recursively defined as 
	\[\varphi_{k+1}(z) = \frac{\varphi_k(z)-1}z, \quad \varphi_0(z) = e^z, \quad k\in\mathbb{N}.\]
	For example consider the embedded method
	\begin{table}[H]
		\vspace{-1em}
		\centering
		\renewcommand\arraystretch{1.2}
		\[
		\begin{array}
		{c|ccc}
		0\\
		\frac{1}{2} & \frac{1}{2}\varphi_1(\frac{1}{2}\cdot)\\
		1& 0& \varphi_1\\
		\hline
		\texttt{exprb3} & \varphi_1 - 14\varphi_3 & 16\varphi_3 & -2\varphi_3  \\
		\texttt{exprb4} & \varphi_1 - 14\varphi_3 + 36\varphi_4 & 16\varphi_3 -48\varphi_4 & -2\varphi_3 + 12\varphi_4 
		\end{array},
		\]
		\vspace{-2em}
	\end{table}
	\noindent i.e. 
	\begin{alignat*}{2}
	U_{1} &= u_0,\\
	U_{2} &= e^{\frac{\tau}{2} A}u_0 &&+ \tau\varphi_1\left(\frac{\tau}{2} A\right)g(U_1) = u_0 + \frac{\tau}{2}\varphi_1\left(\frac{\tau}{2}A\right)F(u_0),\\
	U_{3} &= e^{\tau A}u_0 &&+ \tau\varphi_1(\tau A)g(U_2) = e^{\tau A}F(U_2),\\
	\tilde{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3)(\tau A)g(u_0) + 16\varphi_3(\tau A)g(U_2) - 2\varphi_3(\tau A)g(U_3),\\
	\hat{u}_1 &= e^{\tau A}u_0 &&+ (\varphi_1 - 14\varphi_3 + 36\varphi_4)(\tau A)g(u_0) + (16\varphi_3 -48\varphi_4)(\tau A)g(U_2) \\
	& &&+ (-2\varphi_3 + 12\varphi_4)(\tau A)g(U_3),
	\end{alignat*}
	where $\tilde{u}_1$ and $\hat{u}_1$ is the numerical solution given by \texttt{exprb3} and \texttt{exprb4} respectively.
	\noindent This scheme is known as $\texttt{exprb43}$ \cite[Example 2.24]{bible}. It uses \texttt{exprb3} as a third-order estimator for its fourth-order method \texttt{exprb4}. Both integrators are well suited for numerical computations since all internal stages can be cheaply computed using the exponential Euler method. \\
	Under the simplifying assumptions
	\[
	\sum_{j=1}^s b_j = \varphi_1, \quad  \sum_{j=1}^s a_{ij} = c_i\varphi_1(c_i\cdot) 
	\]
	for $1\le i\le s$ the scheme \eqref{exprbscheme} can be expressed as 
	\begin{align}
	\begin{split}
	U_i &= u_0 + c_i\tau\varphi_1(c_i \tau A)F(u_0) + \tau\sum_{j=2}^{i-1}a_{ij}(\tau A)D_j, \\
	D_j &= g(U_j) - g(u_0), \quad 2\le j\le s, \\
	u_1 &= u_0 +    \tau\varphi_1(    \tau A)F(u_0) + \tau\sum_{i=2}^{s}     b_i(\tau A)D_i.
	\end{split}\label{Djscheme}
	\end{align}
	The main advantage of this reformulation lies in the fact that the norm of all $D_j$ is expected to be small. This can be exploited by the Leja method by allowing an early termination of the Newton interpolation. \\
	For an efficient implementation of exponential Rosenbrock integrators it is crucial to compute only a single action of a matrix function per stage $U_i$ and for solution $u_1$. Since the most frequently employed methods depend on linear combinations of $\varphi_k$-functions this can be done using the matrix exponential function.
	
	\subsection{Computing the action of the $\varphi$-functions}
	Exponential integrators rely on the efficient computation of $\varphi_k$-functions. In the matrix case $A\in\mathbb{C}^{N\times N}$ this can be done by slightly expanding $A$, see \cite[Theorem 2.1]{action}.\\
	Let $V = [V_p\dots V_2, V_1]\in\mathbb{C}^{N\times p}$, $u\in\mathbb{C}^{N\times 1}$, $\tau\in\mathbb{C}$ and
	
	\begin{align*}
	\tilde{A} = 
	\left[ \begin{array}
	{cc}A& V \\0 & J\\
	\end{array}\right],  \quad
	J = 
	\left[ \begin{array}
	{cc}0& I_{p-1} \\0 & 0\\
	\end{array}\right],
	\end{align*}
	where $I_{n}$ is the $n\times n$ identity matrix. Let $e_n$ denote the $n$-th $p\times 1$ unity vector. Then
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_j} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{j}\tau^k\varphi_k(\tau A)V_{p-j+k}, 
	\quad j\in\{1,\dots,p\}. 
	\end{align*}
	In particular for $j=p$ we have
	\begin{align*}
	\begin{bmatrix}I_N & 0\end{bmatrix} e^{\tau\tilde{A}}\colvec{2}{u}{e_p} =
	e^{\tau A}u +
	\displaystyle\sum_{k=1}^{p}\tau^k\varphi_k(\tau A)V_{k}.
	\hphantom{V_{-1}, \quad j\in\{1,\dots,p\}.}
	\end{align*}
	This formulation can be directly applied to each stage in \eqref{Djscheme} assuming $a_{ij}$ and $b_j$ are linear combinations of $\varphi_k$-functions. Therefore for each stage only a single action of an expanded matrix exponential has to be evaluated. In total this has to be done $s$ times for an exponential Rosenbrock method with $s$ stages. \\
	%Many exponential Rosenbrock methods can use this relation to reduce computational costs. For most practical integrators each stage only requires a single action of an expanded matrix exponential has to be evaluated. This is in particular true for the exponential Rosenbrock-Euler methods which can be solved in a single step.
	For a matrix-free implementation of $\tilde A$ given an operator $A$ we can simply compute the action of $\tilde{A}$ as follows
	\[
	\tilde{A}\colvec{2}{v}{w} = \colvec{2}{Av}{0} + \colvec{2}{Vw}{Jw}, \quad v\in\mathbb{C}^{N\times 1}, w\in\mathbb{C}^{p\times 1}.
	\]
	The Leja method only relies on matrix-vector multiplications with $\tilde{A}$ and therefore the explicit knowledge of $A$ is not required. To summarize, an efficient matrix-free implementation of exponential Rosenbrock-methods can be achieved using the Leja method. In particular \texttt{exprb3} and \texttt{exprb4} can be evaluated by computing three actions of matrix exponentials. 
	
	
	\section{Nonlinear Advection-Diffusion-Reaction Equation}
	Consider the one-dimensional advection-diffusion-reaction equation
	\begin{align*}
	\partial_tu &= \alpha\partial_x(u+\partial_xu) + \beta\partial_x(u^2) + u(u-0.5) \quad \alpha,\beta,\ge 0\\
	u_0(t) &= e^{-80\cdot(t-0,45)^2}, \quad t\in[0,0.1]
	\end{align*}
	\begin{align*}
	\partial_tu &= \alpha\nabla(u+\nabla u) + \beta(\partial_x + \partial_y)(u^2) + u(u-0.5) ~~\text{and}\\
	u_0(t) &= e^{-80\cdot(t-0,45)^2} ~~\text{with}~~ \alpha,\beta,\ge 0  ~~\text{and}~~ t\in[0,0.1]
	\end{align*}
	on the domain $\Omega = [0,1]$.
	
	\section{Numerical experiments}\label{sec:NE}
	In this section we will apply matrix-free exponential Rosenbrock integrators to multiple advection-diffusion-reaction equations. We compare the behavior of these methods with other matrix-free ODE solvers.
	
	\paragraph{Crank-Nicolson method:}
	We refer to the Crank-Nicolson method of order 2 as \texttt{cn2}. 
	In our implementation of \texttt{cn2}, we used the SciPy package \texttt{scipy.sparse.linalg.gmres} to solve linear equations. We set the relative tolerance to $\texttt{tol}/s$, where $s$ is the total number of substeps taken for solving the ODE. This choice guarantees that the sum of errors made by \texttt{gmres} is always lower than our specified tolerance \texttt{tol}, since we have to solve exactly one linear equation per substep. No preconditioner was used for \texttt{gmres}. The Crank-Nicolson method is unconditionally stable and therefore does not have to satisfy the Courant-Friedrichs-Lewy (CFL) conditions for the differential equations we investigate in our experiments.
	
	\paragraph{Explicit Runge-Kutta method:}
	We refer to the explicit midpoint method of order 2 as \texttt{rk2} and refer to the classical Runge-Kutta method of order 4 as \texttt{rk4}. No explicit Runge-Kutta method is A-stable. Therefore small time step sizes have to be chosen when solving for stiff differential equations. In our experiments this is the case when the considered differential equation is diffusion-dominated.
	
	\paragraph{Exponential Rosenbrock methods:}
	We refer to the exponential Rosenbrock methods of order 2, 3 and 4 discussed in \ref{expint} as \texttt{exprb2}, \texttt{exprb3} and \texttt{exprb4} respectively.
	The action of the matrix exponential is approximated with the matrix-free \texttt{expleja} algorithm. At each time step we compute the optimal interpolation degree and substep parameter according to \eqref{eq:msmatrixfree}.
	The maximal interpolation degree is set to 100. Note that the total number of matrix-vector multiplication per time step can still exceed 100 since we have to compute the spectral radius. This typically happens for $s=1$. \\
	
	All experiments are conducted in Python 3.7 \cite{python} with NumPy 1.18.1 \cite{numpy} and SciPy 1.4.1 \cite{numpy}.
	
	\subsection{Experiment 1: Linear advection diffusion equation}
	Consider a nonlinear variant of the advection-diffusion equation in section \ref{sec:LinearADe}. Let $\alpha,\beta,\ge 0$. Similar to uor
	\begin{align*}
	\partial_tu &= \alpha\partial_x(u+\partial_xu) + \beta\partial_x(u^2) + u(u-0.5) \\
	u_0(t) &= e^{-80\cdot(t-0,45)^2}, \quad t\in[0,0.1].
	\end{align*}
	
	In this section
	For the first experiments we will discretize multiple advection-diffusion-reaction equations We will always choose an equidistant grid with grid size $h = \frac{1}{N}$, $N\in\mathbb{N}$ and grid points $x_i = ih$ for $i=0\dots,N$ on the domain $\Omega = [0,1]$. The resulting ordinary differential equations (ODEs) will be solved with four different integrators. Our goal is to investigate the respective computational costs of these methods while achieving a prescribed relative tolerance \texttt{tol}.
	
	
	\noindent For our experiments we will often fix one of two different P\'eclet numbers
	\[\texttt{Pe} = \frac{b}{a}, \quad \texttt{pe} = \frac{hb}{2a},\]
	The P\'eclet numbers are dimensionless quantities representing the ratio of the advective velocity $b$ to the diffusive velocity $a$. While \texttt{Pe} characterizes the original partial differential equation, the grid P\'eclet number \texttt{pe} is the dimensionless quantity for the resulting ODE after discretization. Note that by fixing \texttt{pe} for varying grid sizes, we have to change the original partial differential equantion. Unless otherwise noted we accomplish that by replacing $b$ with $2b$ and $a$ with $ah$.
	
	
	\subsection{Experiment 1: Linear advection diffusion equation}
	Consider the one-dimensional advection-diffusion equation
	\begin{align*}
	\partial_tu &= a\partial_{xx}u + b\partial_xu \quad a,b\ge 0\\
	u_0(t) &= e^{-80\cdot(t-0,45)^2} \quad t\in[0,0.1]
	\end{align*}
	with homogeneous Dirichlet boundary conditions on the domain $\Omega = [0,1]$. 
	For a fixed $N\in\mathbb N$ we approximate the diffusive part with second-order central differences on an equidistant grid with grid size $h = \frac{1}{N}$ and grid points $x_i = ih$, $i=0\dots,N$.
	\[\partial_{xx}u(x_i) = \frac{u(x_{i+1}) - 2u(x_i) + u(x_{i-1})}{{h}^2} + \mathcal{O}({h}^2)\]
	In order to limit numerical instabilities we discretize the advective part with forward differences, similar to the upwind scheme.\footnote{Maybe create a seperate section on hybrid difference schemes? There we can also analyze the resulting matrix $A$ itself and plot the eigenvalues. I need sources for that though.}
	\[\partial_{x}u(x_i) = \frac{u(x_{i+1}) - u(x_i)}{h} + \mathcal{O}(h)\]
	The resulting system of ordinary differential equation is given by
	\begin{align*}
	\partial_tu &= Au.
	\end{align*} 
	Some eigenvalues of $A$ can have an extremely large negative real part. Therefore, since no explicit Runge-Kutta method is A-stable, this imposes very stingend conditions on the time step size $\tau$ for $\operatorname{rk2}$ and $\operatorname{rk4}$. We will refer to the Courant-Friedrich-Lewy (CFL) conditions imposed by the advective and diffusive part of $A$ respectively by $C_{adv}$ and $C_{dif}$.  
	\[ C_{adv} = \frac{b\tau}{h} \le 1, \quad C_{dif} = \frac{a\tau}{h^2} \le \frac{1}{2}\] 
	
	
	
	In our case the problem is fully linear and therefore $\texttt{exprb2}$ simplifies to the computation of the action of the matrix exponential function with the Leja method. We write $\texttt{expleja}$ for the single precision Leja method approximation. The reference solution was computed with double precision and therefore uses different nodes.
	
	In order to keep the solution from vanishing, we fix $b = 1$ and only consider coefficients $a\in[0,1]$. The advection-diffusion ratio scaled by the grid size $h$ is represented by the grid P\'eclet number
	
	
	\subsection{Experiment 2: 1D Nonlinear advection diffusion equation}
	\[ \partial_tu = \alpha\partial_{x}((u+1)\partial_{x}u) + \partial_{x}(u^2) + u(u-0.5) \]
	
	We discretize, solve again with rk2, rk4, cn2 and exprb2.
	
	\section{Appendix}
	Matrix analysis, Horn and Johnson, Lemma 5.6.10
	\[\rho(A) = \inf\{\norm{A} : \norm{\cdot} \text{ is an induced operator norm} \} \]
	
	%
	%\subsection{CFL Condition} \label{CFL}
	%We conduct a Von Neumann stability analysis for the diffusion equation
	%\[ \partial_tu = \partial_{xx}u. \]
	%The eigenfunctions of $\partial_{xx}$ are given by
	%\[ u_k(x) = e^{ikx} \]
	%with eigenvalues $-k^2$.
	
	\subsection{Experiment Linear}
	\newcommand{\Pe}{Pe=10.0}
	\newcommand{\precision}{single}
	\newcommand{\safe}{sf=1.1}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/1, \precision, \Pe.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/2, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/3, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/5, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment1/4, \precision, \Pe.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	
	\newpage
	\subsection{Experiment Nonlinear 1D}
	\newcommand{\adv}{\detokenize{α}=0.01}
	\newcommand{\dif}{\detokenize{β}=0.1}
	\renewcommand{\precision}{half}
	{1, \precision, \adv, \dif}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/1, \precision, \adv, \dif.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/2, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/3, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment2/4, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\subsection{Experiment Nonlinear 2D}
	{1, \precision, \adv, \dif}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/1, \precision, \adv, \dif.pdf}
		%	\caption{Remark: In this case $N$ is equal to the grid P\'eclet number $\operatorname{pe}$.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/2, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/3, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\columnwidth]{../Figures/Experiment_2D/4, \precision, \adv, \dif.pdf}
		%	\caption{A picture of a gull.}
	\end{figure}
	
	
	%DESCRIPTION OF THE EXPERIMENT, NUMBER OF POWERITS, SAFETYFACTOR, HOW STABLE IS THE COMPUTATION???
	%First that the matrix $A = A(t)$ changes at every time step and therefore
	
	
	
	
	\clearpage
	\begin{thebibliography}{13}
		\bibitem{rosenbr} M. Caliari, A. Ostermann. Implementation of exponential Rosenbrock-type integrators, Applied Numerical Mathematics 59 (2009), 568-581.
		\bibitem{action} A. Al-Mohy, N. Higham. Computing the action of the matrix exponential, with an application to exponential integrators, SIAM Journal on Scientific Computing 33 (2011), 488-511.
		\bibitem{newt} L. Reichel. Newton interpolation at Leja points, BIT Numerical Mathematics 30 (1990), 332-346.
		\bibitem{advdif} M. Caliari, M. Vianello, L. Bergamaschi. Interpolating discrete advection-diffusion propagators at Leja sequences, Journal of Computational and Applied Mathematics 172 (2004), 79-99.
		\bibitem{lejarev} M. Caliari, P. Kandolf, A. Ostermann, S. Rainer. The Leja method revisited: backward error analysis for the matrix exponential, SIAM Journal on Scientific Computation, Accepted for publication (2016). arXiv:1506.08665.
		\bibitem{bible} M. Hochbruck, A. Ostermann. Exponential integrators, Acta Numerica 19 (2010), 209-286
		\bibitem{polynomialmethods} P. Novati, Polynomial methods for the computation of functions of large unsymmetric matrices, Ph.D. Thesis in Computational Mathematics, University of Trieste, advisor I. Moret (2000).
		\bibitem{newtoninterpolation} L. Reichel, Newton interpolation at Leja points, BIT 30 (2) (1990), 332–346.
		\bibitem{matrixanalysis} R. Horn, C. Johnson, Matrix Analysis, Cambridge University Press (2012).
		
		\bibitem{python} Python Software Foundation. Python Language Reference, version 3.7. Available at https://www.python.org. Manual at https://docs.python.org/3/. [Online; accessed 2020-02-19]
		\bibitem{numpy} S. v. d. Walt, C. Colbert, G Varoquaux. The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science \& Engineering, 13 (2011), 22-30.
		\bibitem{numpyguide} T. Oliphant. A guide to NumPy, USA: Trelgol Publishing, (2006).
		\bibitem{scipy} P. Virtanen, R. Gommers, T. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. v. d. Walt, M. Brett, J. Wilson, J. Millman, N. Mayorov, A. Nelson, E. Jones, R. Kern, E. Larson, C. Carey, İ. Polat, Y. Feng, E. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. Quintero, C. Harris, A. Archibald, A. Ribeiro, F. Pedregosa, P. v. Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, in press.
		\bibitem{matplotlib} J. Hunter. Matplotlib: A 2D Graphics Environment, Computing in Science \& Engineering, 9, 90-95 (2007).
		\bibitem{pandas} W. McKinney. Data Structures for Statistical Computing in Python, Proceedings of the 9th Python in Science Conference, 51-56 (2010).
	\end{thebibliography}
	
\end{document}