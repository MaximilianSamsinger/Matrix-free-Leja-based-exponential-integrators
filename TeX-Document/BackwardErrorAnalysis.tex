\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[ngerman,english]{babel}
\usepackage{blindtext}
\usepackage{color}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{subcaption}
\usepackage{upgreek}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\begin{document}
\section{Backward error analysis}
See \footnote{The Leja method revisited: backward error analysis for the matrix exponential. Section 3 \url{https://arxiv.org/pdf/1506.08665.pdf}} for the notation and background.
\subsection{Bounding the relative error of the Leja method}
For
\[
	L_{m,c}(s^{-1}A)^s = e^{A+sh_{m+1,c}(s^{-1}A)} \eqdefn e^{A + \Delta A} 
\]
we bound the backward error for a given tolerance $\operatorname{tol}$ such that
\[\norm{\Delta A} \le \operatorname{tol}\norm{A}.\]
We have
\[
	L_{m,c}(s^{-1}A)^sv = e^{A + \Delta A}v = e^{\Delta A}e^{A}v
\]
since $\Delta A$ and $A$ commute. Now we can establish an upper bound for the error
\begin{align*}
	\norm{L_{m,c}(s^{-1}A)^sv - e^Av} &= \norm{(e^{\Delta A}-I) e^{A}v} \\
	&\le
	\norm{e^{\Delta A}-I} \norm{e^{A}v} \\
	&\le (e^{\norm{\Delta A}}-1) \norm{e^{A}v} \\ 
	&\le (e^{\operatorname{tol}\norm{A}}-1) \norm{e^{A}v}.
\end{align*}

\paragraph{Conclusion:} The relative error $\operatorname{relerror}$ of Leja interpolation might be too large if\\ $e^{\operatorname{tol}\norm{A}}-1 > \operatorname{relerror}$. Therefore we should choose
\[\operatorname{tol} = \frac{\log(\operatorname{relerror} + 1)}{\norm{A}} \]
in order to bound the relative error of the interpolation.

\subsection{Choosing an beneficial norm}
For the backward error analysis the matrix norm was not specified. Furthermore we can minimize the costs of the interpolation by choosing a norm such that $\norm{A}$ is as small as possible.

A well know result from matrix analysis states that for every $A$ and for every $\varepsilon>0$ exists a induced matrixnorm $\norm{\cdot}_{A,\varepsilon}$, such that
\[\rho(A) \le \norm{A}_{A,\varepsilon} \le \rho(A) + \varepsilon. \]
The first inequality holds true for every matrix norm. We will choose $\varepsilon$ small enough, such that
\[\hat\theta_m \defneq \underset{\begin{subarray}{c}m=0,\dots,100\\\rho(A) < \theta_m\end{subarray}}{\min}\theta_m\]
\[\norm{A}_{A,\varepsilon} \le \hat\theta_m\]
Using this construction the explicit knowledge of $\norm{\cdot}_{A,\varepsilon}$ is no longer needed. $\rho(A)$ can be cheaply approximated using power iterations. However, this procedure underestimates the largest eigenvalue and therefore we have to compensate for that by multiplying the estimate with a safety factor. 

%The cost of the computation is given by
%\[C_m(A) = ms = m\displaystyle\lceil{\frac{\rho(A)}{\theta_m}}\displaystyle\rceil = m\cdot1 \]
%However, since $\rho(A) \gg \underset{m=1,\dots,100}{\max}\theta_m$, we have to choose $s$ large enough such that 
%\[\rho(s^{-1}A) \le \underset{m=1,\dots,100}{\max}\theta_m\]
\paragraph{Conclusion:} We can replace the norm of $A$ in the backward error analysis by a slight overestimate of the spectral norm. This improves the speed of the computation and might increase the accuracy of the computation. (See the conclusion of section 1.1 of this document) Furthermore a hump reduction procedure is no longer necessary.



\end{document}